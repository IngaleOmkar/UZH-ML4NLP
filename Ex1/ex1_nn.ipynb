{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/exercises/ex1/ex1_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Q-2GcUhgB0S7"},"source":["# ML4NLP1\n","\n","## Starting Point for Exercise 1, part II\n","\n","\n","\n","This notebook is supposed to serve as a starting point and/or inspiration when starting exercise 1, part II.\n","\n","\n","\n","One of the goals of this exercise is o make you acquainted with **skorch**. You will probably need to consult the [documentation](https://skorch.readthedocs.io/en/stable/)."]},{"cell_type":"markdown","metadata":{"id":"V920LTuiq40d"},"source":["# Installing skorch and loading libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:32:03.130025Z","iopub.status.busy":"2024-10-11T23:32:03.129637Z","iopub.status.idle":"2024-10-11T23:32:03.136049Z","shell.execute_reply":"2024-10-11T23:32:03.135148Z","shell.execute_reply.started":"2024-10-11T23:32:03.129989Z"},"id":"utYcb97jq40t","trusted":true},"outputs":[],"source":["import subprocess\n","\n","\n","\n","# Installation on Google Colab\n","\n","try:\n","\n","    import google.colab\n","\n","    subprocess.run(['python', '-m', 'pip', 'install', 'skorch'])\n","\n","except ImportError:\n","\n","    pass"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:32:03.138221Z","iopub.status.busy":"2024-10-11T23:32:03.137878Z","iopub.status.idle":"2024-10-11T23:32:28.686837Z","shell.execute_reply":"2024-10-11T23:32:28.685576Z","shell.execute_reply.started":"2024-10-11T23:32:03.138187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting skorch\n","  Downloading skorch-1.0.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from skorch) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from skorch) (1.2.2)\n","Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from skorch) (1.14.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from skorch) (0.9.0)\n","Requirement already satisfied: tqdm>=4.14.0 in /opt/conda/lib/python3.10/site-packages (from skorch) (4.66.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.0->skorch) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.0->skorch) (3.5.0)\n","Downloading skorch-1.0.0-py3-none-any.whl (239 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: skorch\n","Successfully installed skorch-1.0.0\n","Collecting gdown\n","  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n","Installing collected packages: gdown\n","Successfully installed gdown-5.2.0\n"]}],"source":["!pip install skorch\n","\n","!pip install gdown"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:32:28.689146Z","iopub.status.busy":"2024-10-11T23:32:28.688801Z","iopub.status.idle":"2024-10-11T23:32:32.942507Z","shell.execute_reply":"2024-10-11T23:32:32.941686Z","shell.execute_reply.started":"2024-10-11T23:32:28.689112Z"},"id":"WZ3Y_KHvq40x","trusted":true},"outputs":[],"source":["import torch\n","\n","from torch import nn\n","\n","import torch.nn.functional as F\n","\n","from skorch import NeuralNetClassifier\n","\n","\n","\n","import pandas as pd\n","\n","import numpy as np\n","\n","import csv\n","\n","import re\n","\n","import string\n","\n","from collections import defaultdict\n","\n","\n","\n","# Set seed for reproducibility\n","\n","seed = 42\n","\n","np.random.seed(seed)\n","\n","torch.manual_seed(seed)\n","\n","torch.cuda.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"dAnY8yaDq400"},"source":["## Training a classifier and making predictions"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:32:32.944269Z","iopub.status.busy":"2024-10-11T23:32:32.943731Z","iopub.status.idle":"2024-10-11T23:32:58.544366Z","shell.execute_reply":"2024-10-11T23:32:58.543223Z","shell.execute_reply.started":"2024-10-11T23:32:32.944224Z"},"id":"zWjt9xGoswAC","outputId":"5e9f1bd0-c578-4591-bb31-a31a6626c971","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs\n","To: /kaggle/working/x_train.txt\n","100%|██████████████████████████████████████| 64.1M/64.1M [00:01<00:00, 40.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6\n","To: /kaggle/working/x_test.txt\n","100%|██████████████████████████████████████| 65.2M/65.2M [00:00<00:00, 70.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl\n","To: /kaggle/working/y_train.txt\n","100%|█████████████████████████████████████████| 480k/480k [00:00<00:00, 108MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X\n","To: /kaggle/working/y_test.txt\n","100%|████████████████████████████████████████| 480k/480k [00:00<00:00, 98.2MB/s]\n"]}],"source":["# Download dataset\n","\n","!gdown 1QP6YuwdKFNUPpvhOaAcvv2Pcp4JMbIRs # x_train\n","\n","!gdown 1QVo7PZAdiZKzifK8kwhEr_umosiDCUx6 # x_test\n","\n","!gdown 1QbBeKcmG2ZyAEFB3AKGTgSWQ1YEMn2jl # y_train\n","\n","!gdown 1QaZj6bI7_78ymnN8IpSk4gVvg-C9fA6X # y_test"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:32:58.548083Z","iopub.status.busy":"2024-10-11T23:32:58.547630Z","iopub.status.idle":"2024-10-11T23:33:00.159486Z","shell.execute_reply":"2024-10-11T23:33:00.158709Z","shell.execute_reply.started":"2024-10-11T23:32:58.548029Z"},"id":"-M6DgXdjtJyH","trusted":true},"outputs":[],"source":["with open(f'x_train.txt') as f:\n","\n","    x_train = f.read().splitlines()\n","\n","with open(f'y_train.txt') as f:\n","\n","    y_train = f.read().splitlines()\n","\n","with open(f'x_test.txt') as f:\n","\n","    x_test = f.read().splitlines()\n","\n","with open(f'y_test.txt') as f:\n","\n","    y_test = f.read().splitlines()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"execution":{"iopub.execute_input":"2024-10-11T23:33:00.160925Z","iopub.status.busy":"2024-10-11T23:33:00.160592Z","iopub.status.idle":"2024-10-11T23:33:02.342288Z","shell.execute_reply":"2024-10-11T23:33:02.341274Z","shell.execute_reply.started":"2024-10-11T23:33:00.160892Z"},"id":"oRqfDA9FuoX1","outputId":"23c71978-d23f-48e5-88ad-f36308461277","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n","      <td>est</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sebes, Joseph; Pereira Thomas (1961) (på eng)....</td>\n","      <td>swe</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...</td>\n","      <td>mai</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Après lo cort periòde d'establiment a Basilèa,...</td>\n","      <td>oci</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...</td>\n","      <td>tha</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text label\n","0  Klement Gottwaldi surnukeha palsameeriti ning ...   est\n","1  Sebes, Joseph; Pereira Thomas (1961) (på eng)....   swe\n","2  भारतीय स्वातन्त्र्य आन्दोलन राष्ट्रीय एवम क्षे...   mai\n","3  Après lo cort periòde d'establiment a Basilèa,...   oci\n","4  ถนนเจริญกรุง (อักษรโรมัน: Thanon Charoen Krung...   tha"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Combine x_train and y_train into one dataframe\n","\n","train_df = pd.DataFrame({'text': x_train, 'label': y_train})\n","\n","# Write train_df to csv with tab as separator\n","\n","train_df.to_csv('train_df.csv', index=False, sep='\\t')\n","\n","# Comibne x_test and y_test into one dataframe\n","\n","test_df = pd.DataFrame({'text': x_test, 'label': y_test})\n","\n","# Inspect the first 5 items in the train split\n","\n","train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"-s_6f3E7Kt0s"},"source":["### Data preparation\n","\n","\n","\n","Prepare your dataset for this experiment using the same method as you did in part 1.\n","\n","\n","\n","Get a subset of the train/test data that includes 20 languages. Include English, German, Dutch, Danish, Swedish, Norwegian, and Japanese, plus 13 additional languages of your choice based on the items in the list of labels.\n","\n","\n","\n","Don't forget to encode your labels using the adjusted code snippet from part 1!\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:33:02.344357Z","iopub.status.busy":"2024-10-11T23:33:02.343904Z","iopub.status.idle":"2024-10-11T23:33:02.753336Z","shell.execute_reply":"2024-10-11T23:33:02.752268Z","shell.execute_reply.started":"2024-10-11T23:33:02.344307Z"},"id":"PXpIOpjRxzTl","outputId":"f1c4f2eb-8422-465b-e988-3d02c9bc1f1f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['銀行券は帝国国庫及びドイツ帝国銀行(Reichsbank)から発行され、帝国のいくつかの構成国の銀行からも発行された。帝国国庫発行の帝国紙幣(Reichskassenschein)は5、10、20、50マルクが発行された一方、ドイツ帝国銀行券(Reichsbanknote)は20、50、100、1000マルクが発行された。1914年以降に発行されたこれらの銀行券はパピエルマルクと呼ばれる。', 'في عام 2007، كرئيس أساقفة و كاردينال بوينس آيرس، قدم بيرجوليو النسخة النهائية من البيان المشترك الصادر عن أساقفة أمريكا اللاتينية المسمى \"وثيقة أباريسيدا\" بعد إقراره من قبل البابا بندكت السادس عشر. نصت الوثيقة على ضرورة الامتثال و قبول تعاليم الكنيسة ضد \"جرائم نكراء\" مثل الإجهاض والقتل الرحيم: \"نأمل أن المشرعين ورؤساء الحكومات، والعاملين في مجال الصحة، سيدركون كرامة الحياة الإنسانية وأهمية العائلة في شعوبنا، و سيدافعون عن حمايتها من جرائم نكراء مثل الإجهاض والقتل الرحيم، وهذه هي مسؤوليتهم. ونحن نلزم أنفسنا \"تماسك إفخارستي\"، بما معناه، يجب أن نكون واعين بأن الناس لا يستطيعون الحصول على القربان المقدس وفي الوقت نفسه هم يعملون ضد الوصايا، ولا سيما عندما يوافقون على الإجهاض والقتل الرحيم، وغيرها من الجرائم الخطيرة ضد الحياة والعائلة، وهو ينطبق بشكل خاص على مسؤولية المشرعين والحكام، والعاملين في مجال الصحة\". وقد وصف الحركة المؤيدة للإجهاض باعتباره \"ثقافة الموت\"، و كان يعارض توزيع وسائل منع الحمل مجانًا في الأرجنتين.', 'Luis Alberto Suárez Díaz (s. 24. tammikuuta 1987 Salto, Uruguay) on uruguaylainen jalkapalloilija, joka pelaa hyökkääjänä tai oikeana laitalinkkinä Barcelonassa ja edustaa myös Uruguayn maajoukkuetta. Suárez tunnetaan lempinimellä El Pistolero (suom. revolverisankari).', \"Bij installatie zal een instantienaam en een aantal poortnummers worden bepaald. De instantienaam bestaat uit twee tekens en moet beginnen met een letter. Deze code zal gebruikt worden om te berekenen op welke poorten de Ingres Servers zal luisteren. Standaard zal er gebruikgemaakt worden van de de code 'll', welke zal luisteren naar de poorten 21064 tot 21071.\", '1951年5月14日，联合国大会额外措施委员会通过了美国提出的对中华人民共和国和朝鲜民主主义人民共和国实行禁运的提案。5月18日该提案被联合国大会表决通过为联合国大会500号决议“ADDITIONAL MEASURES TO BE EMPLOYED TO MEET THE AGRESSION IN KOREA”。5月22日，中国政府外交部就联大500号决议发表声明，指出这是联大“又一破坏联合国宪章，侵越安全理事会权限并蓄意扩大侵略战争的非法行动”。43个国家接受联大500号决议并加以实行。中国从西方国家的进口额，1952年比1951年下跌了四成。1951年5月，政务院发布允许对外资企业征用或征购。']\n","['jpn', 'ara', 'fin', 'nld', 'zho']\n"]}],"source":["# TODO: Create your train/test subsets of languages\n","\n","# Note, make sure these are the same as what you used in Part 1!\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","# TODO: Create your train/test subsets of languages\n","\n","language_filter = ['eng','deu','nld','dan','swe','nob','jpn', #basics\n","\n","                   'fra', 'spa', 'rus', 'por', 'ita', 'kor', 'ara', 'zho', 'hin', 'tam', 'tha', 'vie', 'fin' #additionals\n","\n","                   ]\n","\n","# Filter x and y based on the language filter\n","\n","filtered_x = [text for text,label in zip(x_train + x_test,y_train + y_test) if label in language_filter]\n","\n","filtered_y = [label for label in y_train + y_test if label in language_filter]\n","\n","\n","\n","# Split the train/test data into 8:2\n","\n","x_train,x_test,y_train,y_test = train_test_split(filtered_x,filtered_y,test_size = 0.2,random_state=42)\n","\n","\n","\n","#display\n","\n","print(x_train[:5])\n","\n","print(y_train[:5])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:33:02.755352Z","iopub.status.busy":"2024-10-11T23:33:02.754920Z","iopub.status.idle":"2024-10-11T23:33:02.794038Z","shell.execute_reply":"2024-10-11T23:33:02.792907Z","shell.execute_reply.started":"2024-10-11T23:33:02.755303Z"},"id":"vMp0gji4MCDl","outputId":"11406782-1d32-41ab-fc28-605994b5bdd9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['ara' 'dan' 'deu' 'eng' 'fin' 'fra' 'hin' 'ita' 'jpn' 'kor' 'nld' 'nob'\n"," 'por' 'rus' 'spa' 'swe' 'tam' 'tha' 'vie' 'zho']\n","[ 8  0  4 ... 19 18  1]\n","[ 7 15 12 ...  1  1  0]\n"]}],"source":["# TODO: Use your adjusted code from part 1 to encode the labels again\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","\n","label_encoder = LabelEncoder().fit(y_train)\n","\n","y_train, y_test = label_encoder.transform(y_train), label_encoder.transform(y_test)\n","\n","print(label_encoder.classes_)\n","\n","print(y_train)\n","\n","print(y_test)"]},{"cell_type":"markdown","metadata":{"id":"iGBLxHU-LcVL"},"source":["### Feature Extraction"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:02.795812Z","iopub.status.busy":"2024-10-11T23:33:02.795447Z","iopub.status.idle":"2024-10-11T23:33:07.691198Z","shell.execute_reply":"2024-10-11T23:33:07.690424Z","shell.execute_reply.started":"2024-10-11T23:33:02.795759Z"},"id":"2-Ls0e0GQgMF","trusted":true},"outputs":[],"source":["# First, we extract some simple features as input for the neural network\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","\n","vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2), max_features=100, binary=True)\n","\n","X = vectorizer.fit_transform(x_train)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:07.692620Z","iopub.status.busy":"2024-10-11T23:33:07.692318Z","iopub.status.idle":"2024-10-11T23:33:07.720626Z","shell.execute_reply":"2024-10-11T23:33:07.719909Z","shell.execute_reply.started":"2024-10-11T23:33:07.692589Z"},"id":"9EiRal_1Q0iJ","trusted":true},"outputs":[],"source":["# We need to change the datatype to make it play nice with pytorch\n","\n","X = X.astype(np.float32)\n","\n","y = y_train.astype(np.int64)"]},{"cell_type":"markdown","metadata":{"id":"oMFoiitJq407"},"source":["In the following, we define a vanilla neural network with two hidden layers. The output layer should have as many outputs as there are classes. In addition, it should have a nonlinearity function."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:07.723792Z","iopub.status.busy":"2024-10-11T23:33:07.723442Z","iopub.status.idle":"2024-10-11T23:33:07.731471Z","shell.execute_reply":"2024-10-11T23:33:07.730606Z","shell.execute_reply.started":"2024-10-11T23:33:07.723742Z"},"id":"7Q5EDIGQUUBy","trusted":true},"outputs":[],"source":["# TODO: In the following, you can find a small (almost) working example of a neural network.\n","\n","# Unfortunately, again, the cat messed up some of the code. Please fix the code such that it is executable. (Hint: the input and output sizes look a bit weird...)\n","\n","\n","\n","class ClassifierModule(nn.Module):\n","\n","    def __init__(\n","\n","        self,\n","\n","        num_units=200,\n","\n","        nonlin=F.relu,\n","\n","        num_classes=20,\n","\n","        input_size=100,\n","\n","    ):\n","\n","        super(ClassifierModule, self).__init__()\n","\n","        self.num_units = num_units\n","\n","        self.nonlin = nonlin\n","\n","\n","\n","        self.dense0 = nn.Linear(input_size, num_units)\n","\n","        self.nonlin = nonlin\n","\n","        self.dense1 = nn.Linear(num_units, 50)\n","\n","        self.output = nn.Linear(50, num_classes)\n","\n","\n","\n","    def forward(self, X, **kwargs):\n","\n","        X = self.nonlin(self.dense0(X))\n","\n","        X = F.relu(self.dense1(X))\n","\n","        X = self.output(X)\n","\n","        return X.squeeze(dim=1)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:07.733147Z","iopub.status.busy":"2024-10-11T23:33:07.732743Z","iopub.status.idle":"2024-10-11T23:33:07.772288Z","shell.execute_reply":"2024-10-11T23:33:07.771541Z","shell.execute_reply.started":"2024-10-11T23:33:07.733103Z"},"id":"wKnJECeQGpyI","trusted":true},"outputs":[],"source":["# Initalise the neural net classifier.\n","\n","net = NeuralNetClassifier(\n","\n","    ClassifierModule(\n","\n","        input_size=X.shape[1],\n","\n","        num_units=200,\n","\n","        num_classes=len(label_encoder.classes_),\n","\n","        nonlin=F.relu,\n","\n","    ),\n","\n","    max_epochs=20,\n","\n","    criterion=nn.CrossEntropyLoss(),\n","\n","    lr=0.1,\n","\n","    device='cuda',  # comment this to train with CPU\n","\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:33:07.773741Z","iopub.status.busy":"2024-10-11T23:33:07.773402Z","iopub.status.idle":"2024-10-11T23:33:44.094034Z","shell.execute_reply":"2024-10-11T23:33:44.093089Z","shell.execute_reply.started":"2024-10-11T23:33:07.773708Z"},"id":"QcNOd9yBSxys","outputId":"53b79f0e-c2fe-4bb0-e230-ee63200e3281","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7376\u001b[0m       \u001b[32m0.2303\u001b[0m        \u001b[35m2.4827\u001b[0m  1.9744\n","      2        \u001b[36m2.1024\u001b[0m       \u001b[32m0.4122\u001b[0m        \u001b[35m1.7188\u001b[0m  1.7499\n","      3        \u001b[36m1.5182\u001b[0m       \u001b[32m0.5319\u001b[0m        \u001b[35m1.3270\u001b[0m  1.7271\n","      4        \u001b[36m1.2503\u001b[0m       \u001b[32m0.6306\u001b[0m        \u001b[35m1.1424\u001b[0m  1.7992\n","      5        \u001b[36m1.0864\u001b[0m       \u001b[32m0.6506\u001b[0m        \u001b[35m1.0249\u001b[0m  1.6949\n","      6        \u001b[36m0.9856\u001b[0m       \u001b[32m0.6606\u001b[0m        \u001b[35m0.9579\u001b[0m  1.6832\n","      7        \u001b[36m0.9264\u001b[0m       \u001b[32m0.6700\u001b[0m        \u001b[35m0.9192\u001b[0m  1.7867\n","      8        \u001b[36m0.8891\u001b[0m       \u001b[32m0.6753\u001b[0m        \u001b[35m0.8941\u001b[0m  1.7717\n","      9        \u001b[36m0.8632\u001b[0m       \u001b[32m0.6787\u001b[0m        \u001b[35m0.8764\u001b[0m  1.7162\n","     10        \u001b[36m0.8434\u001b[0m       \u001b[32m0.6822\u001b[0m        \u001b[35m0.8627\u001b[0m  1.7720\n","     11        \u001b[36m0.8275\u001b[0m       \u001b[32m0.6837\u001b[0m        \u001b[35m0.8523\u001b[0m  1.7142\n","     12        \u001b[36m0.8141\u001b[0m       \u001b[32m0.6875\u001b[0m        \u001b[35m0.8438\u001b[0m  1.7065\n","     13        \u001b[36m0.8027\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.8368\u001b[0m  1.6811\n","     14        \u001b[36m0.7928\u001b[0m       \u001b[32m0.6925\u001b[0m        \u001b[35m0.8307\u001b[0m  1.7162\n","     15        \u001b[36m0.7841\u001b[0m       \u001b[32m0.6937\u001b[0m        \u001b[35m0.8263\u001b[0m  1.7428\n","     16        \u001b[36m0.7762\u001b[0m       \u001b[32m0.6953\u001b[0m        \u001b[35m0.8223\u001b[0m  1.6988\n","     17        \u001b[36m0.7692\u001b[0m       0.6953        \u001b[35m0.8198\u001b[0m  1.6831\n","     18        \u001b[36m0.7628\u001b[0m       0.6941        \u001b[35m0.8176\u001b[0m  1.7301\n","     19        \u001b[36m0.7569\u001b[0m       \u001b[32m0.6959\u001b[0m        \u001b[35m0.8157\u001b[0m  1.7779\n","     20        \u001b[36m0.7515\u001b[0m       \u001b[32m0.6969\u001b[0m        \u001b[35m0.8143\u001b[0m  1.8246\n"]},{"data":{"text/plain":["<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n","  module_=ClassifierModule(\n","    (dense0): Linear(in_features=100, out_features=200, bias=True)\n","    (dense1): Linear(in_features=200, out_features=50, bias=True)\n","    (output): Linear(in_features=50, out_features=20, bias=True)\n","  ),\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Train the classifier\n","\n","net.fit(X, y)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:33:44.095948Z","iopub.status.busy":"2024-10-11T23:33:44.095363Z","iopub.status.idle":"2024-10-11T23:33:45.765386Z","shell.execute_reply":"2024-10-11T23:33:45.764226Z","shell.execute_reply.started":"2024-10-11T23:33:44.095903Z"},"id":"7SWtTc1zjie8","outputId":"2d98581e-cbd7-4af8-ccd4-1320e41d8b48","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.682\n"]}],"source":["X_test = vectorizer.transform(x_test)\n","\n","X_test = X_test.astype(np.float32)\n","\n","y_test_np = np.array(y_test, dtype=np.int64)\n","\n","\n","\n","y_pred = net.predict(X_test)\n","\n","test_accuracy = np.mean(y_pred == y_test_np)\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Experimenting with a better count vectorizer"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:45.767309Z","iopub.status.busy":"2024-10-11T23:33:45.766871Z","iopub.status.idle":"2024-10-11T23:33:50.875490Z","shell.execute_reply":"2024-10-11T23:33:50.874693Z","shell.execute_reply.started":"2024-10-11T23:33:45.767257Z"},"trusted":true},"outputs":[],"source":["vectorizer_updated = CountVectorizer(analyzer='char', ngram_range=(2, 2), max_features=5000, binary=True)\n","\n","X_cv_updated = vectorizer_updated.fit_transform(x_train)\n","\n","X_cv_updated = X_cv_updated.astype(np.float32)\n","\n","y_cv_updted = y_train.astype(np.int64)\n","\n","\n","\n","net_cv_updated = NeuralNetClassifier(\n","\n","    ClassifierModule(\n","\n","        input_size=X_cv_updated.shape[1],\n","\n","        num_units=200,\n","\n","        num_classes=len(label_encoder.classes_),\n","\n","        nonlin=F.relu,\n","\n","    ),\n","\n","    max_epochs=20,\n","\n","    criterion=nn.CrossEntropyLoss(),\n","\n","    lr=0.1,\n","\n","    device='cuda',  # comment this to train with CPU\n","\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:33:50.877006Z","iopub.status.busy":"2024-10-11T23:33:50.876673Z","iopub.status.idle":"2024-10-11T23:34:29.519769Z","shell.execute_reply":"2024-10-11T23:34:29.518843Z","shell.execute_reply.started":"2024-10-11T23:33:50.876972Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.3122\u001b[0m       \u001b[32m0.6966\u001b[0m        \u001b[35m1.2376\u001b[0m  1.9263\n","      2        \u001b[36m0.7029\u001b[0m       \u001b[32m0.8925\u001b[0m        \u001b[35m0.3870\u001b[0m  1.8889\n","      3        \u001b[36m0.2740\u001b[0m       \u001b[32m0.9647\u001b[0m        \u001b[35m0.2131\u001b[0m  1.9559\n","      4        \u001b[36m0.1582\u001b[0m       \u001b[32m0.9731\u001b[0m        \u001b[35m0.1500\u001b[0m  1.9326\n","      5        \u001b[36m0.1123\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1245\u001b[0m  1.9341\n","      6        \u001b[36m0.0886\u001b[0m       \u001b[32m0.9781\u001b[0m        \u001b[35m0.1110\u001b[0m  1.9176\n","      7        \u001b[36m0.0732\u001b[0m       \u001b[32m0.9788\u001b[0m        \u001b[35m0.1026\u001b[0m  1.9376\n","      8        \u001b[36m0.0620\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.0969\u001b[0m  1.9674\n","      9        \u001b[36m0.0534\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.0927\u001b[0m  1.9149\n","     10        \u001b[36m0.0464\u001b[0m       \u001b[32m0.9809\u001b[0m        \u001b[35m0.0898\u001b[0m  1.9440\n","     11        \u001b[36m0.0406\u001b[0m       \u001b[32m0.9816\u001b[0m        \u001b[35m0.0876\u001b[0m  1.9457\n","     12        \u001b[36m0.0358\u001b[0m       0.9816        \u001b[35m0.0861\u001b[0m  1.9044\n","     13        \u001b[36m0.0316\u001b[0m       0.9812        \u001b[35m0.0850\u001b[0m  1.9995\n","     14        \u001b[36m0.0281\u001b[0m       0.9816        \u001b[35m0.0842\u001b[0m  1.9498\n","     15        \u001b[36m0.0250\u001b[0m       0.9816        \u001b[35m0.0836\u001b[0m  1.8980\n","     16        \u001b[36m0.0223\u001b[0m       \u001b[32m0.9819\u001b[0m        \u001b[35m0.0833\u001b[0m  1.8908\n","     17        \u001b[36m0.0199\u001b[0m       \u001b[32m0.9831\u001b[0m        \u001b[35m0.0831\u001b[0m  1.8921\n","     18        \u001b[36m0.0179\u001b[0m       0.9828        \u001b[35m0.0830\u001b[0m  1.9313\n","     19        \u001b[36m0.0161\u001b[0m       0.9825        0.0830  1.8906\n","     20        \u001b[36m0.0146\u001b[0m       0.9825        0.0831  1.9131\n"]},{"data":{"text/plain":["<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n","  module_=ClassifierModule(\n","    (dense0): Linear(in_features=5000, out_features=200, bias=True)\n","    (dense1): Linear(in_features=200, out_features=50, bias=True)\n","    (output): Linear(in_features=50, out_features=20, bias=True)\n","  ),\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["net_cv_updated.fit(X_cv_updated, y_cv_updted)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:34:29.521331Z","iopub.status.busy":"2024-10-11T23:34:29.521002Z","iopub.status.idle":"2024-10-11T23:34:31.418733Z","shell.execute_reply":"2024-10-11T23:34:31.417551Z","shell.execute_reply.started":"2024-10-11T23:34:29.521297Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.98075\n"]}],"source":["X_cv_updated_test = vectorizer_updated.transform(x_test)\n","\n","X_cv_updated_test = X_cv_updated_test.astype(np.float32)\n","\n","y_test_np = np.array(y_test, dtype=np.int64)\n","\n","\n","\n","y_cv_updated_pred = net_cv_updated.predict(X_cv_updated_test)\n","\n","test_cv_updated_accuracy = np.mean(y_cv_updated_pred == y_test_np)\n","\n","print(f\"Test Accuracy: {test_cv_updated_accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"JiovRi0liZ9c"},"source":["### Experimenting with TF-IDF vectorizer instead of count vectorizer"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:34:31.420925Z","iopub.status.busy":"2024-10-11T23:34:31.420362Z","iopub.status.idle":"2024-10-11T23:35:14.631714Z","shell.execute_reply":"2024-10-11T23:35:14.630674Z","shell.execute_reply.started":"2024-10-11T23:34:31.420874Z"},"id":"ojozp-muhd34","outputId":"074ca18d-4853-40f4-98f9-e7a1989ae56f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9956\u001b[0m       \u001b[32m0.0509\u001b[0m        \u001b[35m2.9912\u001b[0m  1.9132\n","      2        \u001b[36m2.9870\u001b[0m       \u001b[32m0.1506\u001b[0m        \u001b[35m2.9810\u001b[0m  1.8943\n","      3        \u001b[36m2.9729\u001b[0m       \u001b[32m0.2853\u001b[0m        \u001b[35m2.9608\u001b[0m  1.8978\n","      4        \u001b[36m2.9367\u001b[0m       \u001b[32m0.4203\u001b[0m        \u001b[35m2.8967\u001b[0m  1.8823\n","      5        \u001b[36m2.7900\u001b[0m       0.2347        \u001b[35m2.6222\u001b[0m  1.9340\n","      6        \u001b[36m2.4308\u001b[0m       0.3503        \u001b[35m2.2732\u001b[0m  1.9807\n","      7        \u001b[36m2.1525\u001b[0m       \u001b[32m0.6381\u001b[0m        \u001b[35m2.0182\u001b[0m  1.9009\n","      8        \u001b[36m1.8529\u001b[0m       \u001b[32m0.7141\u001b[0m        \u001b[35m1.6706\u001b[0m  1.8926\n","      9        \u001b[36m1.4840\u001b[0m       \u001b[32m0.7234\u001b[0m        \u001b[35m1.3078\u001b[0m  1.8834\n","     10        \u001b[36m1.1609\u001b[0m       \u001b[32m0.7422\u001b[0m        \u001b[35m1.0385\u001b[0m  1.9115\n","     11        \u001b[36m0.9309\u001b[0m       \u001b[32m0.7953\u001b[0m        \u001b[35m0.8462\u001b[0m  1.8606\n","     12        \u001b[36m0.7571\u001b[0m       \u001b[32m0.8653\u001b[0m        \u001b[35m0.6860\u001b[0m  1.8747\n","     13        \u001b[36m0.6040\u001b[0m       \u001b[32m0.9156\u001b[0m        \u001b[35m0.5400\u001b[0m  1.8755\n","     14        \u001b[36m0.4709\u001b[0m       \u001b[32m0.9297\u001b[0m        \u001b[35m0.4230\u001b[0m  1.9304\n","     15        \u001b[36m0.3704\u001b[0m       \u001b[32m0.9400\u001b[0m        \u001b[35m0.3425\u001b[0m  1.8627\n","     16        \u001b[36m0.3023\u001b[0m       \u001b[32m0.9453\u001b[0m        \u001b[35m0.2903\u001b[0m  1.9076\n","     17        \u001b[36m0.2573\u001b[0m       \u001b[32m0.9484\u001b[0m        \u001b[35m0.2559\u001b[0m  1.8644\n","     18        \u001b[36m0.2264\u001b[0m       \u001b[32m0.9509\u001b[0m        \u001b[35m0.2321\u001b[0m  1.9210\n","     19        \u001b[36m0.2039\u001b[0m       \u001b[32m0.9556\u001b[0m        \u001b[35m0.2146\u001b[0m  1.8869\n","     20        \u001b[36m0.1867\u001b[0m       \u001b[32m0.9575\u001b[0m        \u001b[35m0.2011\u001b[0m  1.8922\n"]},{"data":{"text/plain":["<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n","  module_=ClassifierModule(\n","    (dense0): Linear(in_features=5000, out_features=200, bias=True)\n","    (dense1): Linear(in_features=200, out_features=50, bias=True)\n","    (output): Linear(in_features=50, out_features=20, bias=True)\n","  ),\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","\n","\n","# Use TF-IDF vectorizer for better feature representation\n","\n","vectorizer_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2,2), max_features=5000, use_idf=True) # Increased ngram range and max_features\n","\n","X_tfidf = vectorizer_tfidf.fit_transform(x_train)\n","\n","X_tfidf = X_tfidf.astype(np.float32)\n","\n","y_tfidf = y_train.astype(np.int64)\n","\n","\n","\n","\n","\n","# Initalise the neural net classifier.\n","\n","net_tfid = NeuralNetClassifier(\n","\n","    ClassifierModule(\n","\n","        input_size=X_tfidf.shape[1],\n","\n","        num_units=200,\n","\n","        num_classes=len(label_encoder.classes_),\n","\n","        nonlin=F.relu,\n","\n","    ),\n","\n","    max_epochs=20,\n","\n","    criterion=nn.CrossEntropyLoss(),\n","\n","    lr=0.1,\n","\n","    device='cuda',  # comment this to train with CPU\n","\n",")\n","\n","\n","\n","# Train the classifier\n","\n","net_tfid.fit(X_tfidf, y_tfidf)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:35:14.633547Z","iopub.status.busy":"2024-10-11T23:35:14.633096Z","iopub.status.idle":"2024-10-11T23:35:16.553952Z","shell.execute_reply":"2024-10-11T23:35:16.552856Z","shell.execute_reply.started":"2024-10-11T23:35:14.633487Z"},"id":"Q3jbEf64jtGR","outputId":"64623d2b-f5b3-4629-eb71-2b64feb8f115","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy with TF-IDF: 0.95775\n"]}],"source":["X_test_tfidf = vectorizer_tfidf.transform(x_test)\n","\n","X_test_tfidf = X_test_tfidf.astype(np.float32)\n","\n","y_test_np = np.array(y_test, dtype=np.int64)\n","\n","\n","\n","y_pred_tfidf = net_tfid.predict(X_test_tfidf)\n","\n","test_accuracy_tfidf = np.mean(y_pred_tfidf == y_test_np)\n","\n","print(f\"Test Accuracy with TF-IDF: {test_accuracy_tfidf}\")\n"]},{"cell_type":"markdown","metadata":{"id":"cviv9PS-NSCq"},"source":["Note, you can also use `GridSearchCV` with `skorch`, but be aware that training a neural network takes much more time.\n","\n","\n","\n","Play around with 5 different sets of hyperparameters. For example, consider some of the following:\n","\n","\n","\n","- layer sizes\n","\n","- activation functions\n","\n","- regularizers\n","\n","- early stopping\n","\n","- vectorizer parameters\n","\n","\n","\n","Report your best hyperparameter combination. \\\\\n","\n","📝❓ What is the effect of your modifcations on validation performance? Discuss potential reasons."]},{"cell_type":"markdown","metadata":{"id":"FbxuaEDPrZSu"},"source":["☝ Note, during model development, if you run into the infamous CUDA out-of-memory (OOM) error, try clearing the GPU memory either with `torch.cuda.empty_cache()` or restarting the runtime."]},{"cell_type":"markdown","metadata":{},"source":["## Grid Search for best hyper parameters with Count Vectorizer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:35:16.556299Z","iopub.status.busy":"2024-10-11T23:35:16.555880Z","iopub.status.idle":"2024-10-11T23:35:16.573585Z","shell.execute_reply":"2024-10-11T23:35:16.572663Z","shell.execute_reply.started":"2024-10-11T23:35:16.556252Z"},"id":"BCwaMlVkifDx","trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","from skorch.callbacks import EarlyStopping\n","\n","\n","\n","# Define the parameter grid for GridSearchCV\n","\n","param_grid = {\n","\n","    'module__num_units': [100, 200, 300],\n","\n","    'module__nonlin': [F.relu],\n","\n","    'module__input_size': [X_cv_updated.shape[1]],\n","\n","    'lr': [0.01, 0.1],\n","\n","    'max_epochs': [20, 30],\n","\n","    'callbacks': [[('EarlyStopping', EarlyStopping(patience=patience))] for patience in [5]]\n","\n","}\n","\n","\n","\n","net_cv_gs = NeuralNetClassifier(\n","\n","    ClassifierModule(\n","\n","        input_size=X_cv_updated.shape[1],\n","\n","        num_units=200,\n","\n","        num_classes=len(label_encoder.classes_),\n","\n","        nonlin=F.relu,\n","\n","    ),\n","\n","    max_epochs=20,\n","\n","    criterion=nn.CrossEntropyLoss(),\n","\n","    lr=0.1,\n","\n","    device='cuda',  # comment this to train with CPU\n","\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-11T23:35:16.575820Z","iopub.status.busy":"2024-10-11T23:35:16.574861Z","iopub.status.idle":"2024-10-11T23:54:42.703397Z","shell.execute_reply":"2024-10-11T23:54:42.702365Z","shell.execute_reply.started":"2024-10-11T23:35:16.575768Z"},"id":"uP_UIB_OnHZC","outputId":"8bf71086-f01e-4b54-cf91-a45a495e8914","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9801\u001b[0m       \u001b[32m0.1012\u001b[0m        \u001b[35m2.9618\u001b[0m  1.2616\n","      2        \u001b[36m2.9412\u001b[0m       \u001b[32m0.1598\u001b[0m        \u001b[35m2.9182\u001b[0m  1.3189\n","      3        \u001b[36m2.8886\u001b[0m       \u001b[32m0.2835\u001b[0m        \u001b[35m2.8548\u001b[0m  1.2777\n","      4        \u001b[36m2.8119\u001b[0m       \u001b[32m0.4025\u001b[0m        \u001b[35m2.7636\u001b[0m  1.2515\n","      5        \u001b[36m2.7086\u001b[0m       \u001b[32m0.5492\u001b[0m        \u001b[35m2.6499\u001b[0m  1.2733\n","      6        \u001b[36m2.5905\u001b[0m       \u001b[32m0.7216\u001b[0m        \u001b[35m2.5287\u001b[0m  1.2417\n","      7        \u001b[36m2.4651\u001b[0m       \u001b[32m0.7610\u001b[0m        \u001b[35m2.3966\u001b[0m  1.2796\n","      8        \u001b[36m2.3203\u001b[0m       \u001b[32m0.7784\u001b[0m        \u001b[35m2.2389\u001b[0m  1.2790\n","      9        \u001b[36m2.1493\u001b[0m       \u001b[32m0.7929\u001b[0m        \u001b[35m2.0583\u001b[0m  1.2683\n","     10        \u001b[36m1.9621\u001b[0m       \u001b[32m0.7990\u001b[0m        \u001b[35m1.8685\u001b[0m  1.2782\n","     11        \u001b[36m1.7709\u001b[0m       \u001b[32m0.8140\u001b[0m        \u001b[35m1.6792\u001b[0m  1.2573\n","     12        \u001b[36m1.5869\u001b[0m       \u001b[32m0.8318\u001b[0m        \u001b[35m1.5034\u001b[0m  1.2456\n","     13        \u001b[36m1.4215\u001b[0m       \u001b[32m0.8477\u001b[0m        \u001b[35m1.3489\u001b[0m  1.2542\n","     14        \u001b[36m1.2771\u001b[0m       \u001b[32m0.8590\u001b[0m        \u001b[35m1.2139\u001b[0m  1.2554\n","     15        \u001b[36m1.1504\u001b[0m       \u001b[32m0.8716\u001b[0m        \u001b[35m1.0954\u001b[0m  1.2830\n","     16        \u001b[36m1.0390\u001b[0m       \u001b[32m0.8824\u001b[0m        \u001b[35m0.9916\u001b[0m  1.2584\n","     17        \u001b[36m0.9412\u001b[0m       \u001b[32m0.8988\u001b[0m        \u001b[35m0.9007\u001b[0m  1.2523\n","     18        \u001b[36m0.8550\u001b[0m       \u001b[32m0.9180\u001b[0m        \u001b[35m0.8204\u001b[0m  1.2666\n","     19        \u001b[36m0.7782\u001b[0m       \u001b[32m0.9316\u001b[0m        \u001b[35m0.7487\u001b[0m  1.2629\n","     20        \u001b[36m0.7096\u001b[0m       \u001b[32m0.9353\u001b[0m        \u001b[35m0.6845\u001b[0m  1.2562\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9850\u001b[0m       \u001b[32m0.0492\u001b[0m        \u001b[35m2.9670\u001b[0m  1.2765\n","      2        \u001b[36m2.9452\u001b[0m       \u001b[32m0.0501\u001b[0m        \u001b[35m2.9213\u001b[0m  1.2924\n","      3        \u001b[36m2.8919\u001b[0m       \u001b[32m0.0754\u001b[0m        \u001b[35m2.8601\u001b[0m  1.2587\n","      4        \u001b[36m2.8205\u001b[0m       \u001b[32m0.1331\u001b[0m        \u001b[35m2.7788\u001b[0m  1.2531\n","      5        \u001b[36m2.7296\u001b[0m       \u001b[32m0.2179\u001b[0m        \u001b[35m2.6798\u001b[0m  1.2626\n","      6        \u001b[36m2.6238\u001b[0m       \u001b[32m0.4269\u001b[0m        \u001b[35m2.5687\u001b[0m  1.2892\n","      7        \u001b[36m2.5051\u001b[0m       \u001b[32m0.5722\u001b[0m        \u001b[35m2.4431\u001b[0m  1.3099\n","      8        \u001b[36m2.3671\u001b[0m       \u001b[32m0.6781\u001b[0m        \u001b[35m2.2944\u001b[0m  1.2633\n","      9        \u001b[36m2.2052\u001b[0m       \u001b[32m0.7427\u001b[0m        \u001b[35m2.1241\u001b[0m  1.2431\n","     10        \u001b[36m2.0290\u001b[0m       \u001b[32m0.7732\u001b[0m        \u001b[35m1.9461\u001b[0m  1.2913\n","     11        \u001b[36m1.8509\u001b[0m       \u001b[32m0.7826\u001b[0m        \u001b[35m1.7702\u001b[0m  1.2477\n","     12        \u001b[36m1.6762\u001b[0m       \u001b[32m0.8154\u001b[0m        \u001b[35m1.6001\u001b[0m  1.2580\n","     13        \u001b[36m1.5085\u001b[0m       \u001b[32m0.8772\u001b[0m        \u001b[35m1.4401\u001b[0m  1.2607\n","     14        \u001b[36m1.3544\u001b[0m       \u001b[32m0.8866\u001b[0m        \u001b[35m1.2970\u001b[0m  1.2529\n","     15        \u001b[36m1.2182\u001b[0m       \u001b[32m0.8894\u001b[0m        \u001b[35m1.1714\u001b[0m  1.2549\n","     16        \u001b[36m1.0986\u001b[0m       0.8880        \u001b[35m1.0611\u001b[0m  1.2498\n","     17        \u001b[36m0.9934\u001b[0m       \u001b[32m0.8922\u001b[0m        \u001b[35m0.9643\u001b[0m  1.2538\n","     18        \u001b[36m0.9010\u001b[0m       \u001b[32m0.8988\u001b[0m        \u001b[35m0.8794\u001b[0m  1.2692\n","     19        \u001b[36m0.8196\u001b[0m       \u001b[32m0.9063\u001b[0m        \u001b[35m0.8045\u001b[0m  1.2617\n","     20        \u001b[36m0.7474\u001b[0m       \u001b[32m0.9222\u001b[0m        \u001b[35m0.7379\u001b[0m  1.2636\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9847\u001b[0m       \u001b[32m0.0492\u001b[0m        \u001b[35m2.9690\u001b[0m  1.2639\n","      2        \u001b[36m2.9531\u001b[0m       \u001b[32m0.0942\u001b[0m        \u001b[35m2.9351\u001b[0m  1.2800\n","      3        \u001b[36m2.9141\u001b[0m       \u001b[32m0.1462\u001b[0m        \u001b[35m2.8895\u001b[0m  1.2452\n","      4        \u001b[36m2.8594\u001b[0m       \u001b[32m0.1898\u001b[0m        \u001b[35m2.8245\u001b[0m  1.2515\n","      5        \u001b[36m2.7837\u001b[0m       \u001b[32m0.2549\u001b[0m        \u001b[35m2.7389\u001b[0m  1.2904\n","      6        \u001b[36m2.6903\u001b[0m       \u001b[32m0.3622\u001b[0m        \u001b[35m2.6400\u001b[0m  1.2799\n","      7        \u001b[36m2.5872\u001b[0m       \u001b[32m0.4311\u001b[0m        \u001b[35m2.5339\u001b[0m  1.2332\n","      8        \u001b[36m2.4737\u001b[0m       \u001b[32m0.4794\u001b[0m        \u001b[35m2.4134\u001b[0m  1.2533\n","      9        \u001b[36m2.3390\u001b[0m       \u001b[32m0.5825\u001b[0m        \u001b[35m2.2656\u001b[0m  1.2401\n","     10        \u001b[36m2.1727\u001b[0m       \u001b[32m0.6359\u001b[0m        \u001b[35m2.0844\u001b[0m  1.2468\n","     11        \u001b[36m1.9763\u001b[0m       \u001b[32m0.6645\u001b[0m        \u001b[35m1.8799\u001b[0m  1.3347\n","     12        \u001b[36m1.7693\u001b[0m       \u001b[32m0.7146\u001b[0m        \u001b[35m1.6789\u001b[0m  1.2618\n","     13        \u001b[36m1.5781\u001b[0m       \u001b[32m0.7573\u001b[0m        \u001b[35m1.5029\u001b[0m  1.2745\n","     14        \u001b[36m1.4135\u001b[0m       \u001b[32m0.7929\u001b[0m        \u001b[35m1.3533\u001b[0m  1.2583\n","     15        \u001b[36m1.2722\u001b[0m       \u001b[32m0.8191\u001b[0m        \u001b[35m1.2239\u001b[0m  1.2556\n","     16        \u001b[36m1.1480\u001b[0m       \u001b[32m0.8491\u001b[0m        \u001b[35m1.1091\u001b[0m  1.2581\n","     17        \u001b[36m1.0367\u001b[0m       \u001b[32m0.8636\u001b[0m        \u001b[35m1.0053\u001b[0m  1.2591\n","     18        \u001b[36m0.9359\u001b[0m       \u001b[32m0.8641\u001b[0m        \u001b[35m0.9114\u001b[0m  1.2480\n","     19        \u001b[36m0.8453\u001b[0m       \u001b[32m0.8707\u001b[0m        \u001b[35m0.8275\u001b[0m  1.2423\n","     20        \u001b[36m0.7650\u001b[0m       \u001b[32m0.8749\u001b[0m        \u001b[35m0.7534\u001b[0m  1.2499\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9858\u001b[0m       \u001b[32m0.0520\u001b[0m        \u001b[35m2.9725\u001b[0m  1.3015\n","      2        \u001b[36m2.9566\u001b[0m       \u001b[32m0.0989\u001b[0m        \u001b[35m2.9396\u001b[0m  1.2480\n","      3        \u001b[36m2.9176\u001b[0m       \u001b[32m0.1931\u001b[0m        \u001b[35m2.8934\u001b[0m  1.2594\n","      4        \u001b[36m2.8620\u001b[0m       \u001b[32m0.2737\u001b[0m        \u001b[35m2.8277\u001b[0m  1.2473\n","      5        \u001b[36m2.7861\u001b[0m       \u001b[32m0.3266\u001b[0m        \u001b[35m2.7418\u001b[0m  1.2451\n","      6        \u001b[36m2.6926\u001b[0m       \u001b[32m0.3814\u001b[0m        \u001b[35m2.6414\u001b[0m  1.2525\n","      7        \u001b[36m2.5871\u001b[0m       \u001b[32m0.5075\u001b[0m        \u001b[35m2.5309\u001b[0m  1.2648\n","      8        \u001b[36m2.4698\u001b[0m       \u001b[32m0.6396\u001b[0m        \u001b[35m2.4058\u001b[0m  1.2458\n","      9        \u001b[36m2.3331\u001b[0m       \u001b[32m0.7052\u001b[0m        \u001b[35m2.2590\u001b[0m  1.2920\n","     10        \u001b[36m2.1769\u001b[0m       \u001b[32m0.7451\u001b[0m        \u001b[35m2.0967\u001b[0m  1.2608\n","     11        \u001b[36m2.0122\u001b[0m       \u001b[32m0.7652\u001b[0m        \u001b[35m1.9307\u001b[0m  1.2625\n","     12        \u001b[36m1.8467\u001b[0m       \u001b[32m0.7798\u001b[0m        \u001b[35m1.7644\u001b[0m  1.2632\n","     13        \u001b[36m1.6785\u001b[0m       \u001b[32m0.8008\u001b[0m        \u001b[35m1.5936\u001b[0m  1.2536\n","     14        \u001b[36m1.5085\u001b[0m       \u001b[32m0.8411\u001b[0m        \u001b[35m1.4256\u001b[0m  1.2388\n","     15        \u001b[36m1.3464\u001b[0m       \u001b[32m0.8721\u001b[0m        \u001b[35m1.2715\u001b[0m  1.3110\n","     16        \u001b[36m1.2026\u001b[0m       \u001b[32m0.8894\u001b[0m        \u001b[35m1.1390\u001b[0m  1.2859\n","     17        \u001b[36m1.0803\u001b[0m       \u001b[32m0.8974\u001b[0m        \u001b[35m1.0278\u001b[0m  1.2243\n","     18        \u001b[36m0.9776\u001b[0m       \u001b[32m0.9030\u001b[0m        \u001b[35m0.9343\u001b[0m  1.2399\n","     19        \u001b[36m0.8904\u001b[0m       \u001b[32m0.9053\u001b[0m        \u001b[35m0.8542\u001b[0m  1.2536\n","     20        \u001b[36m0.8146\u001b[0m       \u001b[32m0.9058\u001b[0m        \u001b[35m0.7836\u001b[0m  1.2566\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9913\u001b[0m       \u001b[32m0.0909\u001b[0m        \u001b[35m2.9817\u001b[0m  1.2550\n","      2        \u001b[36m2.9701\u001b[0m       \u001b[32m0.1518\u001b[0m        \u001b[35m2.9579\u001b[0m  1.2438\n","      3        \u001b[36m2.9410\u001b[0m       \u001b[32m0.3580\u001b[0m        \u001b[35m2.9247\u001b[0m  1.2499\n","      4        \u001b[36m2.9011\u001b[0m       \u001b[32m0.5426\u001b[0m        \u001b[35m2.8787\u001b[0m  1.2666\n","      5        \u001b[36m2.8450\u001b[0m       0.5234        \u001b[35m2.8130\u001b[0m  1.2494\n","      6        \u001b[36m2.7641\u001b[0m       0.4799        \u001b[35m2.7173\u001b[0m  1.2472\n","      7        \u001b[36m2.6464\u001b[0m       0.4686        \u001b[35m2.5802\u001b[0m  1.2340\n","      8        \u001b[36m2.4878\u001b[0m       0.5220        \u001b[35m2.4065\u001b[0m  1.2550\n","      9        \u001b[36m2.3007\u001b[0m       \u001b[32m0.6078\u001b[0m        \u001b[35m2.2125\u001b[0m  1.2441\n","     10        \u001b[36m2.1018\u001b[0m       \u001b[32m0.6781\u001b[0m        \u001b[35m2.0143\u001b[0m  1.2480\n","     11        \u001b[36m1.9057\u001b[0m       \u001b[32m0.7563\u001b[0m        \u001b[35m1.8244\u001b[0m  1.2273\n","     12        \u001b[36m1.7249\u001b[0m       \u001b[32m0.7737\u001b[0m        \u001b[35m1.6535\u001b[0m  1.2663\n","     13        \u001b[36m1.5629\u001b[0m       \u001b[32m0.7929\u001b[0m        \u001b[35m1.5012\u001b[0m  1.2504\n","     14        \u001b[36m1.4185\u001b[0m       \u001b[32m0.8196\u001b[0m        \u001b[35m1.3658\u001b[0m  1.2537\n","     15        \u001b[36m1.2893\u001b[0m       \u001b[32m0.8608\u001b[0m        \u001b[35m1.2444\u001b[0m  1.2337\n","     16        \u001b[36m1.1726\u001b[0m       \u001b[32m0.8918\u001b[0m        \u001b[35m1.1340\u001b[0m  1.2302\n","     17        \u001b[36m1.0656\u001b[0m       \u001b[32m0.9138\u001b[0m        \u001b[35m1.0324\u001b[0m  1.2417\n","     18        \u001b[36m0.9669\u001b[0m       \u001b[32m0.9227\u001b[0m        \u001b[35m0.9389\u001b[0m  1.2338\n","     19        \u001b[36m0.8763\u001b[0m       \u001b[32m0.9325\u001b[0m        \u001b[35m0.8540\u001b[0m  1.2457\n","     20        \u001b[36m0.7948\u001b[0m       \u001b[32m0.9410\u001b[0m        \u001b[35m0.7783\u001b[0m  1.3342\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9886\u001b[0m       \u001b[32m0.0918\u001b[0m        \u001b[35m2.9781\u001b[0m  1.2649\n","      2        \u001b[36m2.9667\u001b[0m       \u001b[32m0.1481\u001b[0m        \u001b[35m2.9540\u001b[0m  1.2594\n","      3        \u001b[36m2.9384\u001b[0m       \u001b[32m0.2198\u001b[0m        \u001b[35m2.9215\u001b[0m  1.2441\n","      4        \u001b[36m2.8997\u001b[0m       \u001b[32m0.3130\u001b[0m        \u001b[35m2.8769\u001b[0m  1.2487\n","      5        \u001b[36m2.8451\u001b[0m       \u001b[32m0.4292\u001b[0m        \u001b[35m2.8123\u001b[0m  1.2513\n","      6        \u001b[36m2.7653\u001b[0m       \u001b[32m0.4831\u001b[0m        \u001b[35m2.7174\u001b[0m  1.2438\n","      7        \u001b[36m2.6508\u001b[0m       \u001b[32m0.5033\u001b[0m        \u001b[35m2.5853\u001b[0m  1.2779\n","      8        \u001b[36m2.5013\u001b[0m       \u001b[32m0.5178\u001b[0m        \u001b[35m2.4245\u001b[0m  1.2321\n","      9        \u001b[36m2.3345\u001b[0m       \u001b[32m0.6439\u001b[0m        \u001b[35m2.2578\u001b[0m  1.2370\n","     10        \u001b[36m2.1678\u001b[0m       \u001b[32m0.7395\u001b[0m        \u001b[35m2.0926\u001b[0m  1.2456\n","     11        \u001b[36m1.9988\u001b[0m       \u001b[32m0.7793\u001b[0m        \u001b[35m1.9215\u001b[0m  1.2309\n","     12        \u001b[36m1.8237\u001b[0m       \u001b[32m0.7868\u001b[0m        \u001b[35m1.7463\u001b[0m  1.2370\n","     13        \u001b[36m1.6484\u001b[0m       \u001b[32m0.8022\u001b[0m        \u001b[35m1.5757\u001b[0m  1.2434\n","     14        \u001b[36m1.4812\u001b[0m       \u001b[32m0.8238\u001b[0m        \u001b[35m1.4176\u001b[0m  1.2286\n","     15        \u001b[36m1.3294\u001b[0m       \u001b[32m0.8365\u001b[0m        \u001b[35m1.2779\u001b[0m  1.2739\n","     16        \u001b[36m1.1964\u001b[0m       \u001b[32m0.8477\u001b[0m        \u001b[35m1.1561\u001b[0m  1.2543\n","     17        \u001b[36m1.0798\u001b[0m       \u001b[32m0.8622\u001b[0m        \u001b[35m1.0492\u001b[0m  1.2365\n","     18        \u001b[36m0.9776\u001b[0m       \u001b[32m0.8739\u001b[0m        \u001b[35m0.9551\u001b[0m  1.2450\n","     19        \u001b[36m0.8877\u001b[0m       \u001b[32m0.8889\u001b[0m        \u001b[35m0.8719\u001b[0m  1.2480\n","     20        \u001b[36m0.8083\u001b[0m       \u001b[32m0.9072\u001b[0m        \u001b[35m0.7976\u001b[0m  1.2606\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9869\u001b[0m       \u001b[32m0.0923\u001b[0m        \u001b[35m2.9743\u001b[0m  1.3135\n","      2        \u001b[36m2.9596\u001b[0m       \u001b[32m0.1954\u001b[0m        \u001b[35m2.9438\u001b[0m  1.3615\n","      3        \u001b[36m2.9233\u001b[0m       \u001b[32m0.2868\u001b[0m        \u001b[35m2.9018\u001b[0m  1.3204\n","      4        \u001b[36m2.8716\u001b[0m       \u001b[32m0.3215\u001b[0m        \u001b[35m2.8390\u001b[0m  1.2975\n","      5        \u001b[36m2.7932\u001b[0m       \u001b[32m0.3843\u001b[0m        \u001b[35m2.7449\u001b[0m  1.2578\n","      6        \u001b[36m2.6817\u001b[0m       \u001b[32m0.5037\u001b[0m        \u001b[35m2.6179\u001b[0m  1.2563\n","      7        \u001b[36m2.5428\u001b[0m       \u001b[32m0.5834\u001b[0m        \u001b[35m2.4708\u001b[0m  1.2546\n","      8        \u001b[36m2.3897\u001b[0m       \u001b[32m0.6532\u001b[0m        \u001b[35m2.3139\u001b[0m  1.2370\n","      9        \u001b[36m2.2301\u001b[0m       \u001b[32m0.6701\u001b[0m        \u001b[35m2.1526\u001b[0m  1.2619\n","     10        \u001b[36m2.0681\u001b[0m       \u001b[32m0.7067\u001b[0m        \u001b[35m1.9878\u001b[0m  1.2821\n","     11        \u001b[36m1.9007\u001b[0m       \u001b[32m0.7413\u001b[0m        \u001b[35m1.8160\u001b[0m  1.2697\n","     12        \u001b[36m1.7281\u001b[0m       \u001b[32m0.7769\u001b[0m        \u001b[35m1.6441\u001b[0m  1.2772\n","     13        \u001b[36m1.5612\u001b[0m       \u001b[32m0.8243\u001b[0m        \u001b[35m1.4843\u001b[0m  1.2603\n","     14        \u001b[36m1.4096\u001b[0m       \u001b[32m0.8482\u001b[0m        \u001b[35m1.3421\u001b[0m  1.3014\n","     15        \u001b[36m1.2758\u001b[0m       \u001b[32m0.8819\u001b[0m        \u001b[35m1.2168\u001b[0m  1.2707\n","     16        \u001b[36m1.1577\u001b[0m       \u001b[32m0.8993\u001b[0m        \u001b[35m1.1056\u001b[0m  1.2436\n","     17        \u001b[36m1.0522\u001b[0m       \u001b[32m0.9016\u001b[0m        \u001b[35m1.0051\u001b[0m  1.2421\n","     18        \u001b[36m0.9559\u001b[0m       \u001b[32m0.9035\u001b[0m        \u001b[35m0.9136\u001b[0m  1.2821\n","     19        \u001b[36m0.8688\u001b[0m       \u001b[32m0.9049\u001b[0m        \u001b[35m0.8312\u001b[0m  1.2471\n","     20        \u001b[36m0.7904\u001b[0m       \u001b[32m0.9105\u001b[0m        \u001b[35m0.7577\u001b[0m  1.2276\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9848\u001b[0m       \u001b[32m0.1514\u001b[0m        \u001b[35m2.9714\u001b[0m  1.2812\n","      2        \u001b[36m2.9556\u001b[0m       \u001b[32m0.1968\u001b[0m        \u001b[35m2.9394\u001b[0m  1.2465\n","      3        \u001b[36m2.9174\u001b[0m       \u001b[32m0.2826\u001b[0m        \u001b[35m2.8952\u001b[0m  1.2188\n","      4        \u001b[36m2.8631\u001b[0m       \u001b[32m0.3327\u001b[0m        \u001b[35m2.8312\u001b[0m  1.2367\n","      5        \u001b[36m2.7854\u001b[0m       \u001b[32m0.3632\u001b[0m        \u001b[35m2.7411\u001b[0m  1.2446\n","      6        \u001b[36m2.6800\u001b[0m       \u001b[32m0.3847\u001b[0m        \u001b[35m2.6230\u001b[0m  1.2581\n","      7        \u001b[36m2.5474\u001b[0m       \u001b[32m0.4747\u001b[0m        \u001b[35m2.4800\u001b[0m  1.2690\n","      8        \u001b[36m2.3930\u001b[0m       \u001b[32m0.6237\u001b[0m        \u001b[35m2.3193\u001b[0m  1.3473\n","      9        \u001b[36m2.2232\u001b[0m       \u001b[32m0.6809\u001b[0m        \u001b[35m2.1440\u001b[0m  1.2717\n","     10        \u001b[36m2.0374\u001b[0m       \u001b[32m0.7193\u001b[0m        \u001b[35m1.9520\u001b[0m  1.2383\n","     11        \u001b[36m1.8396\u001b[0m       \u001b[32m0.7587\u001b[0m        \u001b[35m1.7558\u001b[0m  1.2501\n","     12        \u001b[36m1.6491\u001b[0m       \u001b[32m0.7816\u001b[0m        \u001b[35m1.5763\u001b[0m  1.2424\n","     13        \u001b[36m1.4813\u001b[0m       \u001b[32m0.8074\u001b[0m        \u001b[35m1.4220\u001b[0m  1.2760\n","     14        \u001b[36m1.3378\u001b[0m       \u001b[32m0.8229\u001b[0m        \u001b[35m1.2903\u001b[0m  1.2437\n","     15        \u001b[36m1.2139\u001b[0m       \u001b[32m0.8650\u001b[0m        \u001b[35m1.1753\u001b[0m  1.2307\n","     16        \u001b[36m1.1041\u001b[0m       \u001b[32m0.8791\u001b[0m        \u001b[35m1.0724\u001b[0m  1.2413\n","     17        \u001b[36m1.0048\u001b[0m       \u001b[32m0.8875\u001b[0m        \u001b[35m0.9790\u001b[0m  1.2329\n","     18        \u001b[36m0.9146\u001b[0m       \u001b[32m0.8922\u001b[0m        \u001b[35m0.8945\u001b[0m  1.2385\n","     19        \u001b[36m0.8332\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m0.8185\u001b[0m  1.2302\n","     20        \u001b[36m0.7600\u001b[0m       \u001b[32m0.8960\u001b[0m        \u001b[35m0.7504\u001b[0m  1.2411\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9854\u001b[0m       \u001b[32m0.0914\u001b[0m        \u001b[35m2.9708\u001b[0m  1.2630\n","      2        \u001b[36m2.9551\u001b[0m       \u001b[32m0.2081\u001b[0m        \u001b[35m2.9384\u001b[0m  1.2353\n","      3        \u001b[36m2.9169\u001b[0m       \u001b[32m0.3013\u001b[0m        \u001b[35m2.8945\u001b[0m  1.2493\n","      4        \u001b[36m2.8644\u001b[0m       \u001b[32m0.3861\u001b[0m        \u001b[35m2.8336\u001b[0m  1.2402\n","      5        \u001b[36m2.7912\u001b[0m       \u001b[32m0.4428\u001b[0m        \u001b[35m2.7488\u001b[0m  1.2336\n","      6        \u001b[36m2.6914\u001b[0m       \u001b[32m0.4555\u001b[0m        \u001b[35m2.6357\u001b[0m  1.2271\n","      7        \u001b[36m2.5613\u001b[0m       \u001b[32m0.4681\u001b[0m        \u001b[35m2.4911\u001b[0m  1.2354\n","      8        \u001b[36m2.3998\u001b[0m       \u001b[32m0.4813\u001b[0m        \u001b[35m2.3167\u001b[0m  1.2369\n","      9        \u001b[36m2.2130\u001b[0m       \u001b[32m0.5370\u001b[0m        \u001b[35m2.1222\u001b[0m  1.2681\n","     10        \u001b[36m2.0132\u001b[0m       \u001b[32m0.6312\u001b[0m        \u001b[35m1.9223\u001b[0m  1.2382\n","     11        \u001b[36m1.8173\u001b[0m       \u001b[32m0.6781\u001b[0m        \u001b[35m1.7344\u001b[0m  1.2300\n","     12        \u001b[36m1.6377\u001b[0m       \u001b[32m0.7062\u001b[0m        \u001b[35m1.5648\u001b[0m  1.2940\n","     13        \u001b[36m1.4757\u001b[0m       \u001b[32m0.7329\u001b[0m        \u001b[35m1.4134\u001b[0m  1.3072\n","     14        \u001b[36m1.3303\u001b[0m       \u001b[32m0.7507\u001b[0m        \u001b[35m1.2784\u001b[0m  1.2277\n","     15        \u001b[36m1.2004\u001b[0m       \u001b[32m0.7680\u001b[0m        \u001b[35m1.1581\u001b[0m  1.2492\n","     16        \u001b[36m1.0844\u001b[0m       \u001b[32m0.7849\u001b[0m        \u001b[35m1.0511\u001b[0m  1.2338\n","     17        \u001b[36m0.9816\u001b[0m       \u001b[32m0.7999\u001b[0m        \u001b[35m0.9563\u001b[0m  1.2623\n","     18        \u001b[36m0.8907\u001b[0m       \u001b[32m0.8261\u001b[0m        \u001b[35m0.8724\u001b[0m  1.2415\n","     19        \u001b[36m0.8102\u001b[0m       \u001b[32m0.8636\u001b[0m        \u001b[35m0.7978\u001b[0m  1.2391\n","     20        \u001b[36m0.7388\u001b[0m       \u001b[32m0.8847\u001b[0m        \u001b[35m0.7313\u001b[0m  1.2472\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9899\u001b[0m       \u001b[32m0.0989\u001b[0m        \u001b[35m2.9799\u001b[0m  1.2509\n","      2        \u001b[36m2.9678\u001b[0m       \u001b[32m0.1251\u001b[0m        \u001b[35m2.9555\u001b[0m  1.2322\n","      3        \u001b[36m2.9387\u001b[0m       \u001b[32m0.1462\u001b[0m        \u001b[35m2.9215\u001b[0m  1.2411\n","      4        \u001b[36m2.8972\u001b[0m       \u001b[32m0.1659\u001b[0m        \u001b[35m2.8716\u001b[0m  1.2629\n","      5        \u001b[36m2.8366\u001b[0m       \u001b[32m0.2427\u001b[0m        \u001b[35m2.7998\u001b[0m  1.2428\n","      6        \u001b[36m2.7507\u001b[0m       \u001b[32m0.2938\u001b[0m        \u001b[35m2.6993\u001b[0m  1.2338\n","      7        \u001b[36m2.6338\u001b[0m       \u001b[32m0.3669\u001b[0m        \u001b[35m2.5673\u001b[0m  1.2419\n","      8        \u001b[36m2.4883\u001b[0m       \u001b[32m0.4672\u001b[0m        \u001b[35m2.4114\u001b[0m  1.2367\n","      9        \u001b[36m2.3256\u001b[0m       \u001b[32m0.6045\u001b[0m        \u001b[35m2.2443\u001b[0m  1.2331\n","     10        \u001b[36m2.1564\u001b[0m       \u001b[32m0.7020\u001b[0m        \u001b[35m2.0722\u001b[0m  1.2253\n","     11        \u001b[36m1.9831\u001b[0m       \u001b[32m0.7376\u001b[0m        \u001b[35m1.8971\u001b[0m  1.2272\n","     12        \u001b[36m1.8079\u001b[0m       \u001b[32m0.7666\u001b[0m        \u001b[35m1.7221\u001b[0m  1.2721\n","     13        \u001b[36m1.6347\u001b[0m       \u001b[32m0.8121\u001b[0m        \u001b[35m1.5527\u001b[0m  1.2418\n","     14        \u001b[36m1.4692\u001b[0m       \u001b[32m0.8327\u001b[0m        \u001b[35m1.3932\u001b[0m  1.2332\n","     15        \u001b[36m1.3168\u001b[0m       \u001b[32m0.8613\u001b[0m        \u001b[35m1.2486\u001b[0m  1.2385\n","     16        \u001b[36m1.1804\u001b[0m       \u001b[32m0.8650\u001b[0m        \u001b[35m1.1200\u001b[0m  1.2213\n","     17        \u001b[36m1.0597\u001b[0m       \u001b[32m0.8674\u001b[0m        \u001b[35m1.0071\u001b[0m  1.2978\n","     18        \u001b[36m0.9537\u001b[0m       \u001b[32m0.8786\u001b[0m        \u001b[35m0.9085\u001b[0m  1.2686\n","     19        \u001b[36m0.8610\u001b[0m       \u001b[32m0.8988\u001b[0m        \u001b[35m0.8225\u001b[0m  1.2305\n","     20        \u001b[36m0.7800\u001b[0m       \u001b[32m0.9110\u001b[0m        \u001b[35m0.7475\u001b[0m  1.2612\n","     21        \u001b[36m0.7092\u001b[0m       \u001b[32m0.9152\u001b[0m        \u001b[35m0.6819\u001b[0m  1.2535\n","     22        \u001b[36m0.6470\u001b[0m       \u001b[32m0.9171\u001b[0m        \u001b[35m0.6244\u001b[0m  1.2344\n","     23        \u001b[36m0.5925\u001b[0m       \u001b[32m0.9203\u001b[0m        \u001b[35m0.5739\u001b[0m  1.2245\n","     24        \u001b[36m0.5447\u001b[0m       \u001b[32m0.9227\u001b[0m        \u001b[35m0.5295\u001b[0m  1.2169\n","     25        \u001b[36m0.5026\u001b[0m       \u001b[32m0.9246\u001b[0m        \u001b[35m0.4905\u001b[0m  1.2272\n","     26        \u001b[36m0.4656\u001b[0m       \u001b[32m0.9255\u001b[0m        \u001b[35m0.4560\u001b[0m  1.2303\n","     27        \u001b[36m0.4330\u001b[0m       \u001b[32m0.9269\u001b[0m        \u001b[35m0.4256\u001b[0m  1.2199\n","     28        \u001b[36m0.4042\u001b[0m       \u001b[32m0.9288\u001b[0m        \u001b[35m0.3988\u001b[0m  1.2492\n","     29        \u001b[36m0.3787\u001b[0m       \u001b[32m0.9297\u001b[0m        \u001b[35m0.3750\u001b[0m  1.2292\n","     30        \u001b[36m0.3560\u001b[0m       \u001b[32m0.9316\u001b[0m        \u001b[35m0.3538\u001b[0m  1.2241\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9870\u001b[0m       \u001b[32m0.2118\u001b[0m        \u001b[35m2.9751\u001b[0m  1.2313\n","      2        \u001b[36m2.9614\u001b[0m       \u001b[32m0.3116\u001b[0m        \u001b[35m2.9482\u001b[0m  1.2245\n","      3        \u001b[36m2.9295\u001b[0m       \u001b[32m0.3744\u001b[0m        \u001b[35m2.9119\u001b[0m  1.2368\n","      4        \u001b[36m2.8846\u001b[0m       0.3674        \u001b[35m2.8581\u001b[0m  1.2392\n","      5        \u001b[36m2.8161\u001b[0m       0.3168        \u001b[35m2.7760\u001b[0m  1.2542\n","      6        \u001b[36m2.7188\u001b[0m       0.3341        \u001b[35m2.6665\u001b[0m  1.2682\n","      7        \u001b[36m2.5969\u001b[0m       \u001b[32m0.4499\u001b[0m        \u001b[35m2.5372\u001b[0m  1.2391\n","      8        \u001b[36m2.4595\u001b[0m       \u001b[32m0.5726\u001b[0m        \u001b[35m2.3962\u001b[0m  1.2382\n","      9        \u001b[36m2.3126\u001b[0m       \u001b[32m0.6359\u001b[0m        \u001b[35m2.2462\u001b[0m  1.2812\n","     10        \u001b[36m2.1558\u001b[0m       \u001b[32m0.6715\u001b[0m        \u001b[35m2.0847\u001b[0m  1.2619\n","     11        \u001b[36m1.9879\u001b[0m       \u001b[32m0.7245\u001b[0m        \u001b[35m1.9134\u001b[0m  1.2711\n","     12        \u001b[36m1.8147\u001b[0m       \u001b[32m0.7638\u001b[0m        \u001b[35m1.7410\u001b[0m  1.3089\n","     13        \u001b[36m1.6454\u001b[0m       \u001b[32m0.8144\u001b[0m        \u001b[35m1.5774\u001b[0m  1.2671\n","     14        \u001b[36m1.4896\u001b[0m       \u001b[32m0.8561\u001b[0m        \u001b[35m1.4302\u001b[0m  1.2964\n","     15        \u001b[36m1.3504\u001b[0m       \u001b[32m0.8702\u001b[0m        \u001b[35m1.2991\u001b[0m  1.2392\n","     16        \u001b[36m1.2259\u001b[0m       \u001b[32m0.8782\u001b[0m        \u001b[35m1.1818\u001b[0m  1.2670\n","     17        \u001b[36m1.1132\u001b[0m       \u001b[32m0.8852\u001b[0m        \u001b[35m1.0756\u001b[0m  1.2484\n","     18        \u001b[36m1.0105\u001b[0m       \u001b[32m0.8889\u001b[0m        \u001b[35m0.9792\u001b[0m  1.2651\n","     19        \u001b[36m0.9172\u001b[0m       \u001b[32m0.8927\u001b[0m        \u001b[35m0.8919\u001b[0m  1.2332\n","     20        \u001b[36m0.8327\u001b[0m       \u001b[32m0.8950\u001b[0m        \u001b[35m0.8133\u001b[0m  1.2400\n","     21        \u001b[36m0.7565\u001b[0m       \u001b[32m0.8988\u001b[0m        \u001b[35m0.7427\u001b[0m  1.2526\n","     22        \u001b[36m0.6883\u001b[0m       \u001b[32m0.9025\u001b[0m        \u001b[35m0.6794\u001b[0m  1.2711\n","     23        \u001b[36m0.6273\u001b[0m       \u001b[32m0.9077\u001b[0m        \u001b[35m0.6231\u001b[0m  1.2761\n","     24        \u001b[36m0.5733\u001b[0m       \u001b[32m0.9128\u001b[0m        \u001b[35m0.5733\u001b[0m  1.2400\n","     25        \u001b[36m0.5256\u001b[0m       \u001b[32m0.9166\u001b[0m        \u001b[35m0.5295\u001b[0m  1.2407\n","     26        \u001b[36m0.4838\u001b[0m       \u001b[32m0.9189\u001b[0m        \u001b[35m0.4910\u001b[0m  1.2565\n","     27        \u001b[36m0.4471\u001b[0m       \u001b[32m0.9213\u001b[0m        \u001b[35m0.4571\u001b[0m  1.2470\n","     28        \u001b[36m0.4149\u001b[0m       \u001b[32m0.9297\u001b[0m        \u001b[35m0.4274\u001b[0m  1.2386\n","     29        \u001b[36m0.3866\u001b[0m       \u001b[32m0.9381\u001b[0m        \u001b[35m0.4012\u001b[0m  1.2619\n","     30        \u001b[36m0.3615\u001b[0m       \u001b[32m0.9485\u001b[0m        \u001b[35m0.3780\u001b[0m  1.2567\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9894\u001b[0m       \u001b[32m0.0614\u001b[0m        \u001b[35m2.9754\u001b[0m  1.2448\n","      2        \u001b[36m2.9582\u001b[0m       \u001b[32m0.1495\u001b[0m        \u001b[35m2.9377\u001b[0m  1.2347\n","      3        \u001b[36m2.9106\u001b[0m       \u001b[32m0.2573\u001b[0m        \u001b[35m2.8792\u001b[0m  1.2376\n","      4        \u001b[36m2.8391\u001b[0m       \u001b[32m0.3229\u001b[0m        \u001b[35m2.7945\u001b[0m  1.2393\n","      5        \u001b[36m2.7406\u001b[0m       \u001b[32m0.3786\u001b[0m        \u001b[35m2.6846\u001b[0m  1.2361\n","      6        \u001b[36m2.6208\u001b[0m       \u001b[32m0.4513\u001b[0m        \u001b[35m2.5592\u001b[0m  1.2896\n","      7        \u001b[36m2.4876\u001b[0m       \u001b[32m0.5033\u001b[0m        \u001b[35m2.4224\u001b[0m  1.3093\n","      8        \u001b[36m2.3414\u001b[0m       \u001b[32m0.5595\u001b[0m        \u001b[35m2.2709\u001b[0m  1.2585\n","      9        \u001b[36m2.1802\u001b[0m       \u001b[32m0.5918\u001b[0m        \u001b[35m2.1054\u001b[0m  1.2312\n","     10        \u001b[36m2.0091\u001b[0m       \u001b[32m0.6172\u001b[0m        \u001b[35m1.9334\u001b[0m  1.2342\n","     11        \u001b[36m1.8362\u001b[0m       \u001b[32m0.6439\u001b[0m        \u001b[35m1.7643\u001b[0m  1.2381\n","     12        \u001b[36m1.6714\u001b[0m       \u001b[32m0.6865\u001b[0m        \u001b[35m1.6084\u001b[0m  1.2431\n","     13        \u001b[36m1.5226\u001b[0m       \u001b[32m0.7306\u001b[0m        \u001b[35m1.4713\u001b[0m  1.2217\n","     14        \u001b[36m1.3923\u001b[0m       \u001b[32m0.7671\u001b[0m        \u001b[35m1.3525\u001b[0m  1.2243\n","     15        \u001b[36m1.2784\u001b[0m       \u001b[32m0.8083\u001b[0m        \u001b[35m1.2485\u001b[0m  1.2491\n","     16        \u001b[36m1.1776\u001b[0m       \u001b[32m0.8393\u001b[0m        \u001b[35m1.1554\u001b[0m  1.2414\n","     17        \u001b[36m1.0854\u001b[0m       \u001b[32m0.8543\u001b[0m        \u001b[35m1.0682\u001b[0m  1.2296\n","     18        \u001b[36m0.9974\u001b[0m       \u001b[32m0.8683\u001b[0m        \u001b[35m0.9839\u001b[0m  1.2385\n","     19        \u001b[36m0.9132\u001b[0m       \u001b[32m0.8843\u001b[0m        \u001b[35m0.9041\u001b[0m  1.2464\n","     20        \u001b[36m0.8344\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m0.8296\u001b[0m  1.2178\n","     21        \u001b[36m0.7617\u001b[0m       \u001b[32m0.9128\u001b[0m        \u001b[35m0.7611\u001b[0m  1.2249\n","     22        \u001b[36m0.6954\u001b[0m       \u001b[32m0.9255\u001b[0m        \u001b[35m0.6983\u001b[0m  1.2243\n","     23        \u001b[36m0.6350\u001b[0m       \u001b[32m0.9353\u001b[0m        \u001b[35m0.6410\u001b[0m  1.2684\n","     24        \u001b[36m0.5802\u001b[0m       \u001b[32m0.9424\u001b[0m        \u001b[35m0.5889\u001b[0m  1.2253\n","     25        \u001b[36m0.5308\u001b[0m       \u001b[32m0.9442\u001b[0m        \u001b[35m0.5420\u001b[0m  1.2139\n","     26        \u001b[36m0.4867\u001b[0m       \u001b[32m0.9466\u001b[0m        \u001b[35m0.5002\u001b[0m  1.2324\n","     27        \u001b[36m0.4477\u001b[0m       \u001b[32m0.9499\u001b[0m        \u001b[35m0.4634\u001b[0m  1.2187\n","     28        \u001b[36m0.4134\u001b[0m       \u001b[32m0.9527\u001b[0m        \u001b[35m0.4309\u001b[0m  1.2147\n","     29        \u001b[36m0.3833\u001b[0m       \u001b[32m0.9578\u001b[0m        \u001b[35m0.4025\u001b[0m  1.2396\n","     30        \u001b[36m0.3568\u001b[0m       \u001b[32m0.9588\u001b[0m        \u001b[35m0.3776\u001b[0m  1.2241\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9871\u001b[0m       \u001b[32m0.0675\u001b[0m        \u001b[35m2.9750\u001b[0m  1.3296\n","      2        \u001b[36m2.9600\u001b[0m       \u001b[32m0.1012\u001b[0m        \u001b[35m2.9444\u001b[0m  1.2331\n","      3        \u001b[36m2.9234\u001b[0m       \u001b[32m0.1959\u001b[0m        \u001b[35m2.9017\u001b[0m  1.2230\n","      4        \u001b[36m2.8718\u001b[0m       \u001b[32m0.3219\u001b[0m        \u001b[35m2.8406\u001b[0m  1.2161\n","      5        \u001b[36m2.7978\u001b[0m       \u001b[32m0.5028\u001b[0m        \u001b[35m2.7530\u001b[0m  1.2454\n","      6        \u001b[36m2.6947\u001b[0m       \u001b[32m0.5187\u001b[0m        \u001b[35m2.6353\u001b[0m  1.2330\n","      7        \u001b[36m2.5641\u001b[0m       \u001b[32m0.5258\u001b[0m        \u001b[35m2.4931\u001b[0m  1.2115\n","      8        \u001b[36m2.4098\u001b[0m       \u001b[32m0.5792\u001b[0m        \u001b[35m2.3260\u001b[0m  1.2361\n","      9        \u001b[36m2.2253\u001b[0m       \u001b[32m0.6771\u001b[0m        \u001b[35m2.1256\u001b[0m  1.2655\n","     10        \u001b[36m2.0131\u001b[0m       \u001b[32m0.7498\u001b[0m        \u001b[35m1.9074\u001b[0m  1.2223\n","     11        \u001b[36m1.7986\u001b[0m       \u001b[32m0.7854\u001b[0m        \u001b[35m1.7012\u001b[0m  1.2206\n","     12        \u001b[36m1.6080\u001b[0m       \u001b[32m0.7971\u001b[0m        \u001b[35m1.5269\u001b[0m  1.2125\n","     13        \u001b[36m1.4509\u001b[0m       \u001b[32m0.8327\u001b[0m        \u001b[35m1.3847\u001b[0m  1.2322\n","     14        \u001b[36m1.3222\u001b[0m       \u001b[32m0.8814\u001b[0m        \u001b[35m1.2663\u001b[0m  1.2135\n","     15        \u001b[36m1.2127\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m1.1631\u001b[0m  1.2292\n","     16        \u001b[36m1.1147\u001b[0m       \u001b[32m0.8993\u001b[0m        \u001b[35m1.0689\u001b[0m  1.2378\n","     17        \u001b[36m1.0236\u001b[0m       \u001b[32m0.9002\u001b[0m        \u001b[35m0.9806\u001b[0m  1.2380\n","     18        \u001b[36m0.9378\u001b[0m       \u001b[32m0.9039\u001b[0m        \u001b[35m0.8979\u001b[0m  1.2156\n","     19        \u001b[36m0.8577\u001b[0m       \u001b[32m0.9053\u001b[0m        \u001b[35m0.8217\u001b[0m  1.2171\n","     20        \u001b[36m0.7842\u001b[0m       \u001b[32m0.9077\u001b[0m        \u001b[35m0.7524\u001b[0m  1.2246\n","     21        \u001b[36m0.7173\u001b[0m       \u001b[32m0.9119\u001b[0m        \u001b[35m0.6896\u001b[0m  1.2350\n","     22        \u001b[36m0.6568\u001b[0m       \u001b[32m0.9147\u001b[0m        \u001b[35m0.6329\u001b[0m  1.2221\n","     23        \u001b[36m0.6023\u001b[0m       \u001b[32m0.9171\u001b[0m        \u001b[35m0.5819\u001b[0m  1.2357\n","     24        \u001b[36m0.5536\u001b[0m       \u001b[32m0.9208\u001b[0m        \u001b[35m0.5363\u001b[0m  1.2350\n","     25        \u001b[36m0.5102\u001b[0m       \u001b[32m0.9241\u001b[0m        \u001b[35m0.4958\u001b[0m  1.3022\n","     26        \u001b[36m0.4718\u001b[0m       \u001b[32m0.9335\u001b[0m        \u001b[35m0.4599\u001b[0m  1.2539\n","     27        \u001b[36m0.4377\u001b[0m       \u001b[32m0.9447\u001b[0m        \u001b[35m0.4281\u001b[0m  1.3230\n","     28        \u001b[36m0.4076\u001b[0m       \u001b[32m0.9578\u001b[0m        \u001b[35m0.3999\u001b[0m  1.2150\n","     29        \u001b[36m0.3807\u001b[0m       \u001b[32m0.9649\u001b[0m        \u001b[35m0.3748\u001b[0m  1.2469\n","     30        \u001b[36m0.3568\u001b[0m       \u001b[32m0.9672\u001b[0m        \u001b[35m0.3523\u001b[0m  1.2219\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9882\u001b[0m       \u001b[32m0.1139\u001b[0m        \u001b[35m2.9764\u001b[0m  1.2222\n","      2        \u001b[36m2.9635\u001b[0m       \u001b[32m0.2156\u001b[0m        \u001b[35m2.9504\u001b[0m  1.2515\n","      3        \u001b[36m2.9327\u001b[0m       \u001b[32m0.3440\u001b[0m        \u001b[35m2.9154\u001b[0m  1.2153\n","      4        \u001b[36m2.8902\u001b[0m       \u001b[32m0.4260\u001b[0m        \u001b[35m2.8656\u001b[0m  1.2151\n","      5        \u001b[36m2.8285\u001b[0m       \u001b[32m0.4564\u001b[0m        \u001b[35m2.7923\u001b[0m  1.2175\n","      6        \u001b[36m2.7382\u001b[0m       \u001b[32m0.4597\u001b[0m        \u001b[35m2.6855\u001b[0m  1.2053\n","      7        \u001b[36m2.6096\u001b[0m       \u001b[32m0.4822\u001b[0m        \u001b[35m2.5380\u001b[0m  1.2179\n","      8        \u001b[36m2.4411\u001b[0m       \u001b[32m0.5445\u001b[0m        \u001b[35m2.3539\u001b[0m  1.2261\n","      9        \u001b[36m2.2411\u001b[0m       \u001b[32m0.6813\u001b[0m        \u001b[35m2.1463\u001b[0m  1.2132\n","     10        \u001b[36m2.0315\u001b[0m       \u001b[32m0.7404\u001b[0m        \u001b[35m1.9424\u001b[0m  1.2719\n","     11        \u001b[36m1.8365\u001b[0m       \u001b[32m0.7690\u001b[0m        \u001b[35m1.7583\u001b[0m  1.2146\n","     12        \u001b[36m1.6617\u001b[0m       \u001b[32m0.7896\u001b[0m        \u001b[35m1.5925\u001b[0m  1.2162\n","     13        \u001b[36m1.5025\u001b[0m       \u001b[32m0.8308\u001b[0m        \u001b[35m1.4412\u001b[0m  1.2512\n","     14        \u001b[36m1.3563\u001b[0m       \u001b[32m0.8622\u001b[0m        \u001b[35m1.3029\u001b[0m  1.2199\n","     15        \u001b[36m1.2224\u001b[0m       \u001b[32m0.8768\u001b[0m        \u001b[35m1.1770\u001b[0m  1.2129\n","     16        \u001b[36m1.1009\u001b[0m       \u001b[32m0.8857\u001b[0m        \u001b[35m1.0637\u001b[0m  1.2068\n","     17        \u001b[36m0.9923\u001b[0m       \u001b[32m0.8927\u001b[0m        \u001b[35m0.9634\u001b[0m  1.2124\n","     18        \u001b[36m0.8967\u001b[0m       \u001b[32m0.8978\u001b[0m        \u001b[35m0.8754\u001b[0m  1.2140\n","     19        \u001b[36m0.8128\u001b[0m       \u001b[32m0.9016\u001b[0m        \u001b[35m0.7981\u001b[0m  1.2439\n","     20        \u001b[36m0.7390\u001b[0m       \u001b[32m0.9039\u001b[0m        \u001b[35m0.7298\u001b[0m  1.2183\n","     21        \u001b[36m0.6738\u001b[0m       0.9030        \u001b[35m0.6694\u001b[0m  1.2301\n","     22        \u001b[36m0.6161\u001b[0m       \u001b[32m0.9067\u001b[0m        \u001b[35m0.6159\u001b[0m  1.2821\n","     23        \u001b[36m0.5651\u001b[0m       \u001b[32m0.9114\u001b[0m        \u001b[35m0.5686\u001b[0m  1.2363\n","     24        \u001b[36m0.5201\u001b[0m       \u001b[32m0.9138\u001b[0m        \u001b[35m0.5268\u001b[0m  1.2207\n","     25        \u001b[36m0.4805\u001b[0m       \u001b[32m0.9180\u001b[0m        \u001b[35m0.4900\u001b[0m  1.2278\n","     26        \u001b[36m0.4456\u001b[0m       \u001b[32m0.9213\u001b[0m        \u001b[35m0.4576\u001b[0m  1.2230\n","     27        \u001b[36m0.4149\u001b[0m       \u001b[32m0.9250\u001b[0m        \u001b[35m0.4291\u001b[0m  1.2358\n","     28        \u001b[36m0.3878\u001b[0m       \u001b[32m0.9330\u001b[0m        \u001b[35m0.4038\u001b[0m  1.2434\n","     29        \u001b[36m0.3637\u001b[0m       \u001b[32m0.9485\u001b[0m        \u001b[35m0.3812\u001b[0m  1.2588\n","     30        \u001b[36m0.3422\u001b[0m       \u001b[32m0.9592\u001b[0m        \u001b[35m0.3610\u001b[0m  1.2352\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9928\u001b[0m       \u001b[32m0.0815\u001b[0m        \u001b[35m2.9850\u001b[0m  1.2099\n","      2        \u001b[36m2.9771\u001b[0m       \u001b[32m0.1195\u001b[0m        \u001b[35m2.9668\u001b[0m  1.2334\n","      3        \u001b[36m2.9554\u001b[0m       \u001b[32m0.1542\u001b[0m        \u001b[35m2.9414\u001b[0m  1.2192\n","      4        \u001b[36m2.9244\u001b[0m       \u001b[32m0.2840\u001b[0m        \u001b[35m2.9041\u001b[0m  1.2519\n","      5        \u001b[36m2.8780\u001b[0m       \u001b[32m0.4185\u001b[0m        \u001b[35m2.8469\u001b[0m  1.2429\n","      6        \u001b[36m2.8081\u001b[0m       \u001b[32m0.4649\u001b[0m        \u001b[35m2.7635\u001b[0m  1.2279\n","      7        \u001b[36m2.7097\u001b[0m       \u001b[32m0.4827\u001b[0m        \u001b[35m2.6518\u001b[0m  1.2399\n","      8        \u001b[36m2.5864\u001b[0m       \u001b[32m0.5501\u001b[0m        \u001b[35m2.5214\u001b[0m  1.2273\n","      9        \u001b[36m2.4456\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m2.3739\u001b[0m  1.2104\n","     10        \u001b[36m2.2830\u001b[0m       \u001b[32m0.6687\u001b[0m        \u001b[35m2.2019\u001b[0m  1.2156\n","     11        \u001b[36m2.0983\u001b[0m       \u001b[32m0.7095\u001b[0m        \u001b[35m2.0126\u001b[0m  1.2325\n","     12        \u001b[36m1.9053\u001b[0m       \u001b[32m0.7385\u001b[0m        \u001b[35m1.8226\u001b[0m  1.2453\n","     13        \u001b[36m1.7194\u001b[0m       \u001b[32m0.7573\u001b[0m        \u001b[35m1.6457\u001b[0m  1.2309\n","     14        \u001b[36m1.5489\u001b[0m       \u001b[32m0.7746\u001b[0m        \u001b[35m1.4848\u001b[0m  1.2150\n","     15        \u001b[36m1.3942\u001b[0m       \u001b[32m0.7802\u001b[0m        \u001b[35m1.3403\u001b[0m  1.2135\n","     16        \u001b[36m1.2565\u001b[0m       \u001b[32m0.7816\u001b[0m        \u001b[35m1.2131\u001b[0m  1.2364\n","     17        \u001b[36m1.1350\u001b[0m       \u001b[32m0.7905\u001b[0m        \u001b[35m1.1008\u001b[0m  1.2982\n","     18        \u001b[36m1.0276\u001b[0m       \u001b[32m0.7999\u001b[0m        \u001b[35m1.0017\u001b[0m  1.2223\n","     19        \u001b[36m0.9333\u001b[0m       \u001b[32m0.8140\u001b[0m        \u001b[35m0.9148\u001b[0m  1.2235\n","     20        \u001b[36m0.8510\u001b[0m       \u001b[32m0.8449\u001b[0m        \u001b[35m0.8388\u001b[0m  1.2469\n","     21        \u001b[36m0.7792\u001b[0m       \u001b[32m0.8791\u001b[0m        \u001b[35m0.7720\u001b[0m  1.2294\n","     22        \u001b[36m0.7161\u001b[0m       \u001b[32m0.8885\u001b[0m        \u001b[35m0.7128\u001b[0m  1.2301\n","     23        \u001b[36m0.6601\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m0.6597\u001b[0m  1.2269\n","     24        \u001b[36m0.6099\u001b[0m       \u001b[32m0.9011\u001b[0m        \u001b[35m0.6119\u001b[0m  1.2126\n","     25        \u001b[36m0.5648\u001b[0m       \u001b[32m0.9077\u001b[0m        \u001b[35m0.5688\u001b[0m  1.2667\n","     26        \u001b[36m0.5241\u001b[0m       \u001b[32m0.9082\u001b[0m        \u001b[35m0.5299\u001b[0m  1.2101\n","     27        \u001b[36m0.4874\u001b[0m       \u001b[32m0.9128\u001b[0m        \u001b[35m0.4947\u001b[0m  1.2264\n","     28        \u001b[36m0.4542\u001b[0m       \u001b[32m0.9180\u001b[0m        \u001b[35m0.4630\u001b[0m  1.2681\n","     29        \u001b[36m0.4243\u001b[0m       \u001b[32m0.9246\u001b[0m        \u001b[35m0.4343\u001b[0m  1.2236\n","     30        \u001b[36m0.3973\u001b[0m       \u001b[32m0.9321\u001b[0m        \u001b[35m0.4085\u001b[0m  1.2272\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9901\u001b[0m       \u001b[32m0.1635\u001b[0m        \u001b[35m2.9767\u001b[0m  1.2441\n","      2        \u001b[36m2.9603\u001b[0m       \u001b[32m0.2409\u001b[0m        \u001b[35m2.9412\u001b[0m  1.2236\n","      3        \u001b[36m2.9173\u001b[0m       \u001b[32m0.2545\u001b[0m        \u001b[35m2.8891\u001b[0m  1.2368\n","      4        \u001b[36m2.8538\u001b[0m       \u001b[32m0.2816\u001b[0m        \u001b[35m2.8123\u001b[0m  1.2428\n","      5        \u001b[36m2.7638\u001b[0m       \u001b[32m0.3243\u001b[0m        \u001b[35m2.7088\u001b[0m  1.2232\n","      6        \u001b[36m2.6524\u001b[0m       \u001b[32m0.4466\u001b[0m        \u001b[35m2.5915\u001b[0m  1.2509\n","      7        \u001b[36m2.5323\u001b[0m       \u001b[32m0.5825\u001b[0m        \u001b[35m2.4678\u001b[0m  1.2669\n","      8        \u001b[36m2.4016\u001b[0m       \u001b[32m0.6710\u001b[0m        \u001b[35m2.3296\u001b[0m  1.2402\n","      9        \u001b[36m2.2530\u001b[0m       \u001b[32m0.7226\u001b[0m        \u001b[35m2.1714\u001b[0m  1.2417\n","     10        \u001b[36m2.0838\u001b[0m       \u001b[32m0.7502\u001b[0m        \u001b[35m1.9926\u001b[0m  1.2456\n","     11        \u001b[36m1.8988\u001b[0m       \u001b[32m0.7680\u001b[0m        \u001b[35m1.8033\u001b[0m  1.3183\n","     12        \u001b[36m1.7102\u001b[0m       \u001b[32m0.7938\u001b[0m        \u001b[35m1.6179\u001b[0m  1.2817\n","     13        \u001b[36m1.5313\u001b[0m       \u001b[32m0.8168\u001b[0m        \u001b[35m1.4470\u001b[0m  1.2447\n","     14        \u001b[36m1.3690\u001b[0m       \u001b[32m0.8322\u001b[0m        \u001b[35m1.2937\u001b[0m  1.2603\n","     15        \u001b[36m1.2243\u001b[0m       \u001b[32m0.8397\u001b[0m        \u001b[35m1.1570\u001b[0m  1.2417\n","     16        \u001b[36m1.0952\u001b[0m       \u001b[32m0.8491\u001b[0m        \u001b[35m1.0353\u001b[0m  1.2405\n","     17        \u001b[36m0.9815\u001b[0m       \u001b[32m0.8538\u001b[0m        \u001b[35m0.9299\u001b[0m  1.2480\n","     18        \u001b[36m0.8837\u001b[0m       \u001b[32m0.8585\u001b[0m        \u001b[35m0.8399\u001b[0m  1.2400\n","     19        \u001b[36m0.8002\u001b[0m       \u001b[32m0.8627\u001b[0m        \u001b[35m0.7632\u001b[0m  1.2354\n","     20        \u001b[36m0.7288\u001b[0m       \u001b[32m0.8702\u001b[0m        \u001b[35m0.6975\u001b[0m  1.2381\n","     21        \u001b[36m0.6674\u001b[0m       \u001b[32m0.8861\u001b[0m        \u001b[35m0.6408\u001b[0m  1.2526\n","     22        \u001b[36m0.6142\u001b[0m       \u001b[32m0.9231\u001b[0m        \u001b[35m0.5914\u001b[0m  1.2395\n","     23        \u001b[36m0.5676\u001b[0m       \u001b[32m0.9400\u001b[0m        \u001b[35m0.5480\u001b[0m  1.2341\n","     24        \u001b[36m0.5264\u001b[0m       0.9391        \u001b[35m0.5094\u001b[0m  1.2400\n","     25        \u001b[36m0.4898\u001b[0m       0.9339        \u001b[35m0.4749\u001b[0m  1.2386\n","     26        \u001b[36m0.4569\u001b[0m       0.9330        \u001b[35m0.4439\u001b[0m  1.2192\n","     27        \u001b[36m0.4274\u001b[0m       0.9335        \u001b[35m0.4160\u001b[0m  1.2251\n","     28        \u001b[36m0.4008\u001b[0m       0.9344        \u001b[35m0.3909\u001b[0m  1.2314\n","     29        \u001b[36m0.3767\u001b[0m       0.9381        \u001b[35m0.3682\u001b[0m  1.2243\n","     30        \u001b[36m0.3550\u001b[0m       \u001b[32m0.9410\u001b[0m        \u001b[35m0.3477\u001b[0m  1.2508\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9874\u001b[0m       \u001b[32m0.1275\u001b[0m        \u001b[35m2.9766\u001b[0m  1.2230\n","      2        \u001b[36m2.9633\u001b[0m       \u001b[32m0.1500\u001b[0m        \u001b[35m2.9491\u001b[0m  1.2314\n","      3        \u001b[36m2.9301\u001b[0m       \u001b[32m0.1912\u001b[0m        \u001b[35m2.9113\u001b[0m  1.2283\n","      4        \u001b[36m2.8846\u001b[0m       \u001b[32m0.2521\u001b[0m        \u001b[35m2.8588\u001b[0m  1.2476\n","      5        \u001b[36m2.8209\u001b[0m       \u001b[32m0.3163\u001b[0m        \u001b[35m2.7845\u001b[0m  1.2131\n","      6        \u001b[36m2.7321\u001b[0m       \u001b[32m0.4072\u001b[0m        \u001b[35m2.6829\u001b[0m  1.3203\n","      7        \u001b[36m2.6160\u001b[0m       \u001b[32m0.4995\u001b[0m        \u001b[35m2.5554\u001b[0m  1.2765\n","      8        \u001b[36m2.4768\u001b[0m       \u001b[32m0.6251\u001b[0m        \u001b[35m2.4075\u001b[0m  1.2244\n","      9        \u001b[36m2.3168\u001b[0m       \u001b[32m0.6963\u001b[0m        \u001b[35m2.2380\u001b[0m  1.2346\n","     10        \u001b[36m2.1336\u001b[0m       \u001b[32m0.7296\u001b[0m        \u001b[35m2.0457\u001b[0m  1.2322\n","     11        \u001b[36m1.9306\u001b[0m       \u001b[32m0.7877\u001b[0m        \u001b[35m1.8392\u001b[0m  1.2234\n","     12        \u001b[36m1.7223\u001b[0m       \u001b[32m0.8257\u001b[0m        \u001b[35m1.6374\u001b[0m  1.2260\n","     13        \u001b[36m1.5301\u001b[0m       \u001b[32m0.8608\u001b[0m        \u001b[35m1.4607\u001b[0m  1.2313\n","     14        \u001b[36m1.3682\u001b[0m       \u001b[32m0.8768\u001b[0m        \u001b[35m1.3155\u001b[0m  1.2284\n","     15        \u001b[36m1.2354\u001b[0m       \u001b[32m0.8833\u001b[0m        \u001b[35m1.1949\u001b[0m  1.2710\n","     16        \u001b[36m1.1221\u001b[0m       \u001b[32m0.8880\u001b[0m        \u001b[35m1.0893\u001b[0m  1.2315\n","     17        \u001b[36m1.0207\u001b[0m       \u001b[32m0.8908\u001b[0m        \u001b[35m0.9935\u001b[0m  1.2249\n","     18        \u001b[36m0.9278\u001b[0m       \u001b[32m0.8969\u001b[0m        \u001b[35m0.9058\u001b[0m  1.2416\n","     19        \u001b[36m0.8431\u001b[0m       \u001b[32m0.8988\u001b[0m        \u001b[35m0.8265\u001b[0m  1.2331\n","     20        \u001b[36m0.7667\u001b[0m       \u001b[32m0.9011\u001b[0m        \u001b[35m0.7554\u001b[0m  1.2259\n","     21        \u001b[36m0.6984\u001b[0m       \u001b[32m0.9039\u001b[0m        \u001b[35m0.6918\u001b[0m  1.2280\n","     22        \u001b[36m0.6373\u001b[0m       \u001b[32m0.9072\u001b[0m        \u001b[35m0.6350\u001b[0m  1.2138\n","     23        \u001b[36m0.5828\u001b[0m       \u001b[32m0.9082\u001b[0m        \u001b[35m0.5844\u001b[0m  1.2596\n","     24        \u001b[36m0.5344\u001b[0m       \u001b[32m0.9114\u001b[0m        \u001b[35m0.5396\u001b[0m  1.2262\n","     25        \u001b[36m0.4916\u001b[0m       \u001b[32m0.9133\u001b[0m        \u001b[35m0.4999\u001b[0m  1.2257\n","     26        \u001b[36m0.4540\u001b[0m       \u001b[32m0.9171\u001b[0m        \u001b[35m0.4652\u001b[0m  1.2166\n","     27        \u001b[36m0.4210\u001b[0m       \u001b[32m0.9189\u001b[0m        \u001b[35m0.4347\u001b[0m  1.2200\n","     28        \u001b[36m0.3922\u001b[0m       \u001b[32m0.9217\u001b[0m        \u001b[35m0.4079\u001b[0m  1.2418\n","     29        \u001b[36m0.3668\u001b[0m       \u001b[32m0.9264\u001b[0m        \u001b[35m0.3843\u001b[0m  1.2259\n","     30        \u001b[36m0.3443\u001b[0m       \u001b[32m0.9330\u001b[0m        \u001b[35m0.3633\u001b[0m  1.2335\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.9874\u001b[0m       \u001b[32m0.0834\u001b[0m        \u001b[35m2.9738\u001b[0m  1.3341\n","      2        \u001b[36m2.9576\u001b[0m       \u001b[32m0.2067\u001b[0m        \u001b[35m2.9387\u001b[0m  1.2659\n","      3        \u001b[36m2.9156\u001b[0m       \u001b[32m0.3102\u001b[0m        \u001b[35m2.8894\u001b[0m  1.2385\n","      4        \u001b[36m2.8564\u001b[0m       \u001b[32m0.4025\u001b[0m        \u001b[35m2.8191\u001b[0m  1.2512\n","      5        \u001b[36m2.7718\u001b[0m       \u001b[32m0.4574\u001b[0m        \u001b[35m2.7201\u001b[0m  1.2437\n","      6        \u001b[36m2.6581\u001b[0m       \u001b[32m0.5019\u001b[0m        \u001b[35m2.5949\u001b[0m  1.2308\n","      7        \u001b[36m2.5200\u001b[0m       \u001b[32m0.5305\u001b[0m        \u001b[35m2.4480\u001b[0m  1.2299\n","      8        \u001b[36m2.3601\u001b[0m       \u001b[32m0.6012\u001b[0m        \u001b[35m2.2804\u001b[0m  1.2263\n","      9        \u001b[36m2.1831\u001b[0m       0.6007        \u001b[35m2.1017\u001b[0m  1.2531\n","     10        \u001b[36m2.0060\u001b[0m       \u001b[32m0.6172\u001b[0m        \u001b[35m1.9310\u001b[0m  1.2304\n","     11        \u001b[36m1.8414\u001b[0m       \u001b[32m0.6471\u001b[0m        \u001b[35m1.7730\u001b[0m  1.2515\n","     12        \u001b[36m1.6879\u001b[0m       \u001b[32m0.6748\u001b[0m        \u001b[35m1.6245\u001b[0m  1.2387\n","     13        \u001b[36m1.5421\u001b[0m       \u001b[32m0.7048\u001b[0m        \u001b[35m1.4839\u001b[0m  1.2325\n","     14        \u001b[36m1.4038\u001b[0m       \u001b[32m0.7512\u001b[0m        \u001b[35m1.3522\u001b[0m  1.2387\n","     15        \u001b[36m1.2747\u001b[0m       \u001b[32m0.7849\u001b[0m        \u001b[35m1.2309\u001b[0m  1.2421\n","     16        \u001b[36m1.1563\u001b[0m       \u001b[32m0.8121\u001b[0m        \u001b[35m1.1208\u001b[0m  1.2393\n","     17        \u001b[36m1.0497\u001b[0m       \u001b[32m0.8355\u001b[0m        \u001b[35m1.0224\u001b[0m  1.2568\n","     18        \u001b[36m0.9549\u001b[0m       \u001b[32m0.8510\u001b[0m        \u001b[35m0.9351\u001b[0m  1.2275\n","     19        \u001b[36m0.8710\u001b[0m       \u001b[32m0.8650\u001b[0m        \u001b[35m0.8575\u001b[0m  1.2286\n","     20        \u001b[36m0.7959\u001b[0m       \u001b[32m0.8744\u001b[0m        \u001b[35m0.7876\u001b[0m  1.2554\n","     21        \u001b[36m0.7288\u001b[0m       \u001b[32m0.8880\u001b[0m        \u001b[35m0.7251\u001b[0m  1.2302\n","     22        \u001b[36m0.6689\u001b[0m       \u001b[32m0.8955\u001b[0m        \u001b[35m0.6689\u001b[0m  1.2281\n","     23        \u001b[36m0.6152\u001b[0m       \u001b[32m0.8993\u001b[0m        \u001b[35m0.6183\u001b[0m  1.2192\n","     24        \u001b[36m0.5671\u001b[0m       \u001b[32m0.9044\u001b[0m        \u001b[35m0.5730\u001b[0m  1.2226\n","     25        \u001b[36m0.5240\u001b[0m       \u001b[32m0.9086\u001b[0m        \u001b[35m0.5323\u001b[0m  1.2522\n","     26        \u001b[36m0.4855\u001b[0m       \u001b[32m0.9138\u001b[0m        \u001b[35m0.4959\u001b[0m  1.2776\n","     27        \u001b[36m0.4511\u001b[0m       \u001b[32m0.9236\u001b[0m        \u001b[35m0.4632\u001b[0m  1.2476\n","     28        \u001b[36m0.4203\u001b[0m       \u001b[32m0.9339\u001b[0m        \u001b[35m0.4339\u001b[0m  1.2265\n","     29        \u001b[36m0.3927\u001b[0m       \u001b[32m0.9456\u001b[0m        \u001b[35m0.4075\u001b[0m  1.2257\n","     30        \u001b[36m0.3677\u001b[0m       \u001b[32m0.9522\u001b[0m        \u001b[35m0.3837\u001b[0m  1.2159\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6940\u001b[0m       \u001b[32m0.5455\u001b[0m        \u001b[35m2.0323\u001b[0m  1.2374\n","      2        \u001b[36m1.3249\u001b[0m       \u001b[32m0.7146\u001b[0m        \u001b[35m0.8899\u001b[0m  1.2618\n","      3        \u001b[36m0.5957\u001b[0m       \u001b[32m0.9058\u001b[0m        \u001b[35m0.4323\u001b[0m  1.2126\n","      4        \u001b[36m0.3093\u001b[0m       \u001b[32m0.9503\u001b[0m        \u001b[35m0.2492\u001b[0m  1.2341\n","      5        \u001b[36m0.2002\u001b[0m       \u001b[32m0.9667\u001b[0m        \u001b[35m0.1787\u001b[0m  1.2222\n","      6        \u001b[36m0.1446\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1385\u001b[0m  1.2359\n","      7        \u001b[36m0.1124\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.1178\u001b[0m  1.2521\n","      8        \u001b[36m0.0922\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.1064\u001b[0m  1.2782\n","      9        \u001b[36m0.0780\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.0989\u001b[0m  1.2567\n","     10        \u001b[36m0.0671\u001b[0m       \u001b[32m0.9813\u001b[0m        \u001b[35m0.0935\u001b[0m  1.2648\n","     11        \u001b[36m0.0583\u001b[0m       \u001b[32m0.9827\u001b[0m        \u001b[35m0.0893\u001b[0m  1.2095\n","     12        \u001b[36m0.0509\u001b[0m       0.9817        \u001b[35m0.0861\u001b[0m  1.2209\n","     13        \u001b[36m0.0446\u001b[0m       0.9822        \u001b[35m0.0835\u001b[0m  1.2208\n","     14        \u001b[36m0.0392\u001b[0m       0.9822        \u001b[35m0.0814\u001b[0m  1.2226\n","     15        \u001b[36m0.0346\u001b[0m       0.9827        \u001b[35m0.0797\u001b[0m  1.2305\n","     16        \u001b[36m0.0306\u001b[0m       \u001b[32m0.9836\u001b[0m        \u001b[35m0.0784\u001b[0m  1.2099\n","     17        \u001b[36m0.0272\u001b[0m       \u001b[32m0.9841\u001b[0m        \u001b[35m0.0774\u001b[0m  1.2160\n","     18        \u001b[36m0.0243\u001b[0m       0.9836        \u001b[35m0.0766\u001b[0m  1.2405\n","     19        \u001b[36m0.0217\u001b[0m       0.9836        \u001b[35m0.0760\u001b[0m  1.2395\n","     20        \u001b[36m0.0196\u001b[0m       0.9836        \u001b[35m0.0756\u001b[0m  1.2228\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6479\u001b[0m       \u001b[32m0.5722\u001b[0m        \u001b[35m2.0316\u001b[0m  1.2965\n","      2        \u001b[36m1.3495\u001b[0m       \u001b[32m0.7577\u001b[0m        \u001b[35m0.8579\u001b[0m  1.2204\n","      3        \u001b[36m0.5896\u001b[0m       \u001b[32m0.9400\u001b[0m        \u001b[35m0.4011\u001b[0m  1.2283\n","      4        \u001b[36m0.2887\u001b[0m       \u001b[32m0.9705\u001b[0m        \u001b[35m0.2341\u001b[0m  1.2333\n","      5        \u001b[36m0.1767\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1742\u001b[0m  1.2185\n","      6        \u001b[36m0.1284\u001b[0m       \u001b[32m0.9752\u001b[0m        \u001b[35m0.1459\u001b[0m  1.2581\n","      7        \u001b[36m0.1021\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1304\u001b[0m  1.2282\n","      8        \u001b[36m0.0847\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.1209\u001b[0m  1.2221\n","      9        \u001b[36m0.0720\u001b[0m       0.9775        \u001b[35m0.1148\u001b[0m  1.2427\n","     10        \u001b[36m0.0621\u001b[0m       0.9770        \u001b[35m0.1106\u001b[0m  1.2201\n","     11        \u001b[36m0.0539\u001b[0m       0.9780        \u001b[35m0.1077\u001b[0m  1.2218\n","     12        \u001b[36m0.0472\u001b[0m       0.9775        \u001b[35m0.1055\u001b[0m  1.2273\n","     13        \u001b[36m0.0416\u001b[0m       0.9780        \u001b[35m0.1039\u001b[0m  1.2394\n","     14        \u001b[36m0.0367\u001b[0m       0.9780        \u001b[35m0.1027\u001b[0m  1.2662\n","     15        \u001b[36m0.0326\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.1019\u001b[0m  1.2166\n","     16        \u001b[36m0.0291\u001b[0m       0.9799        \u001b[35m0.1013\u001b[0m  1.2317\n","     17        \u001b[36m0.0260\u001b[0m       0.9799        \u001b[35m0.1010\u001b[0m  1.2305\n","     18        \u001b[36m0.0234\u001b[0m       0.9799        \u001b[35m0.1008\u001b[0m  1.2594\n","     19        \u001b[36m0.0211\u001b[0m       0.9794        0.1009  1.2631\n","     20        \u001b[36m0.0190\u001b[0m       0.9784        0.1010  1.2743\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6801\u001b[0m       \u001b[32m0.4316\u001b[0m        \u001b[35m2.1561\u001b[0m  1.2653\n","      2        \u001b[36m1.3559\u001b[0m       \u001b[32m0.7559\u001b[0m        \u001b[35m0.9418\u001b[0m  1.2391\n","      3        \u001b[36m0.5612\u001b[0m       \u001b[32m0.8636\u001b[0m        \u001b[35m0.4440\u001b[0m  1.2374\n","      4        \u001b[36m0.2956\u001b[0m       \u001b[32m0.9541\u001b[0m        \u001b[35m0.2676\u001b[0m  1.2355\n","      5        \u001b[36m0.1907\u001b[0m       \u001b[32m0.9644\u001b[0m        \u001b[35m0.1939\u001b[0m  1.3002\n","      6        \u001b[36m0.1396\u001b[0m       \u001b[32m0.9691\u001b[0m        \u001b[35m0.1584\u001b[0m  1.3048\n","      7        \u001b[36m0.1101\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1386\u001b[0m  1.2367\n","      8        \u001b[36m0.0909\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1263\u001b[0m  1.2454\n","      9        \u001b[36m0.0772\u001b[0m       \u001b[32m0.9747\u001b[0m        \u001b[35m0.1183\u001b[0m  1.3010\n","     10        \u001b[36m0.0666\u001b[0m       0.9747        \u001b[35m0.1127\u001b[0m  1.2614\n","     11        \u001b[36m0.0581\u001b[0m       \u001b[32m0.9752\u001b[0m        \u001b[35m0.1087\u001b[0m  1.2454\n","     12        \u001b[36m0.0510\u001b[0m       0.9742        \u001b[35m0.1057\u001b[0m  1.2480\n","     13        \u001b[36m0.0452\u001b[0m       0.9742        \u001b[35m0.1036\u001b[0m  1.2435\n","     14        \u001b[36m0.0402\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1020\u001b[0m  1.2652\n","     15        \u001b[36m0.0359\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1008\u001b[0m  1.2387\n","     16        \u001b[36m0.0322\u001b[0m       0.9770        \u001b[35m0.1000\u001b[0m  1.2574\n","     17        \u001b[36m0.0290\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.0994\u001b[0m  1.2727\n","     18        \u001b[36m0.0262\u001b[0m       0.9770        \u001b[35m0.0989\u001b[0m  1.2804\n","     19        \u001b[36m0.0237\u001b[0m       0.9770        \u001b[35m0.0986\u001b[0m  1.2373\n","     20        \u001b[36m0.0216\u001b[0m       0.9770        \u001b[35m0.0985\u001b[0m  1.2493\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6632\u001b[0m       \u001b[32m0.4963\u001b[0m        \u001b[35m2.0725\u001b[0m  1.2437\n","      2        \u001b[36m1.3750\u001b[0m       \u001b[32m0.7591\u001b[0m        \u001b[35m0.8692\u001b[0m  1.2420\n","      3        \u001b[36m0.5569\u001b[0m       \u001b[32m0.9278\u001b[0m        \u001b[35m0.3723\u001b[0m  1.2275\n","      4        \u001b[36m0.2922\u001b[0m       \u001b[32m0.9653\u001b[0m        \u001b[35m0.2241\u001b[0m  1.2246\n","      5        \u001b[36m0.1895\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1571\u001b[0m  1.2680\n","      6        \u001b[36m0.1349\u001b[0m       \u001b[32m0.9808\u001b[0m        \u001b[35m0.1245\u001b[0m  1.2685\n","      7        \u001b[36m0.1050\u001b[0m       0.9808        \u001b[35m0.1076\u001b[0m  1.2321\n","      8        \u001b[36m0.0861\u001b[0m       \u001b[32m0.9817\u001b[0m        \u001b[35m0.0974\u001b[0m  1.2266\n","      9        \u001b[36m0.0727\u001b[0m       \u001b[32m0.9822\u001b[0m        \u001b[35m0.0905\u001b[0m  1.2388\n","     10        \u001b[36m0.0623\u001b[0m       \u001b[32m0.9827\u001b[0m        \u001b[35m0.0856\u001b[0m  1.3049\n","     11        \u001b[36m0.0540\u001b[0m       0.9827        \u001b[35m0.0819\u001b[0m  1.2535\n","     12        \u001b[36m0.0471\u001b[0m       0.9827        \u001b[35m0.0791\u001b[0m  1.2429\n","     13        \u001b[36m0.0413\u001b[0m       \u001b[32m0.9831\u001b[0m        \u001b[35m0.0768\u001b[0m  1.2642\n","     14        \u001b[36m0.0364\u001b[0m       0.9831        \u001b[35m0.0750\u001b[0m  1.2330\n","     15        \u001b[36m0.0321\u001b[0m       \u001b[32m0.9836\u001b[0m        \u001b[35m0.0735\u001b[0m  1.2344\n","     16        \u001b[36m0.0285\u001b[0m       0.9836        \u001b[35m0.0723\u001b[0m  1.2361\n","     17        \u001b[36m0.0254\u001b[0m       \u001b[32m0.9841\u001b[0m        \u001b[35m0.0713\u001b[0m  1.2468\n","     18        \u001b[36m0.0227\u001b[0m       0.9841        \u001b[35m0.0705\u001b[0m  1.2648\n","     19        \u001b[36m0.0203\u001b[0m       0.9841        \u001b[35m0.0699\u001b[0m  1.2342\n","     20        \u001b[36m0.0183\u001b[0m       0.9841        \u001b[35m0.0694\u001b[0m  1.2396\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7181\u001b[0m       \u001b[32m0.5586\u001b[0m        \u001b[35m2.1535\u001b[0m  1.2483\n","      2        \u001b[36m1.3856\u001b[0m       \u001b[32m0.7179\u001b[0m        \u001b[35m0.9145\u001b[0m  1.2394\n","      3        \u001b[36m0.6030\u001b[0m       \u001b[32m0.9096\u001b[0m        \u001b[35m0.4008\u001b[0m  1.2432\n","      4        \u001b[36m0.3113\u001b[0m       \u001b[32m0.9691\u001b[0m        \u001b[35m0.2532\u001b[0m  1.2454\n","      5        \u001b[36m0.1979\u001b[0m       \u001b[32m0.9747\u001b[0m        \u001b[35m0.1875\u001b[0m  1.2227\n","      6        \u001b[36m0.1399\u001b[0m       \u001b[32m0.9756\u001b[0m        \u001b[35m0.1533\u001b[0m  1.2405\n","      7        \u001b[36m0.1085\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.1347\u001b[0m  1.2494\n","      8        \u001b[36m0.0888\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.1236\u001b[0m  1.2630\n","      9        \u001b[36m0.0750\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.1164\u001b[0m  1.2389\n","     10        \u001b[36m0.0644\u001b[0m       0.9789        \u001b[35m0.1114\u001b[0m  1.2352\n","     11        \u001b[36m0.0560\u001b[0m       0.9789        \u001b[35m0.1077\u001b[0m  1.2271\n","     12        \u001b[36m0.0492\u001b[0m       0.9789        \u001b[35m0.1048\u001b[0m  1.2318\n","     13        \u001b[36m0.0435\u001b[0m       0.9794        \u001b[35m0.1026\u001b[0m  1.2422\n","     14        \u001b[36m0.0386\u001b[0m       0.9789        \u001b[35m0.1009\u001b[0m  1.2667\n","     15        \u001b[36m0.0345\u001b[0m       0.9784        \u001b[35m0.0996\u001b[0m  1.3002\n","     16        \u001b[36m0.0310\u001b[0m       0.9784        \u001b[35m0.0986\u001b[0m  1.2938\n","     17        \u001b[36m0.0279\u001b[0m       0.9780        \u001b[35m0.0980\u001b[0m  1.2300\n","     18        \u001b[36m0.0252\u001b[0m       0.9780        \u001b[35m0.0977\u001b[0m  1.2282\n","     19        \u001b[36m0.0228\u001b[0m       0.9780        \u001b[35m0.0975\u001b[0m  1.2551\n","     20        \u001b[36m0.0207\u001b[0m       0.9770        \u001b[35m0.0975\u001b[0m  1.2387\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7190\u001b[0m       \u001b[32m0.4531\u001b[0m        \u001b[35m2.1342\u001b[0m  1.2457\n","      2        \u001b[36m1.3764\u001b[0m       \u001b[32m0.7249\u001b[0m        \u001b[35m0.9878\u001b[0m  1.2141\n","      3        \u001b[36m0.5859\u001b[0m       \u001b[32m0.8777\u001b[0m        \u001b[35m0.4646\u001b[0m  1.2606\n","      4        \u001b[36m0.3057\u001b[0m       \u001b[32m0.9560\u001b[0m        \u001b[35m0.2709\u001b[0m  1.2462\n","      5        \u001b[36m0.1915\u001b[0m       \u001b[32m0.9700\u001b[0m        \u001b[35m0.1930\u001b[0m  1.2471\n","      6        \u001b[36m0.1377\u001b[0m       \u001b[32m0.9705\u001b[0m        \u001b[35m0.1585\u001b[0m  1.2401\n","      7        \u001b[36m0.1083\u001b[0m       \u001b[32m0.9733\u001b[0m        \u001b[35m0.1399\u001b[0m  1.2275\n","      8        \u001b[36m0.0892\u001b[0m       0.9733        \u001b[35m0.1274\u001b[0m  1.2189\n","      9        \u001b[36m0.0753\u001b[0m       \u001b[32m0.9756\u001b[0m        \u001b[35m0.1185\u001b[0m  1.2574\n","     10        \u001b[36m0.0647\u001b[0m       0.9756        \u001b[35m0.1119\u001b[0m  1.2232\n","     11        \u001b[36m0.0561\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1072\u001b[0m  1.2539\n","     12        \u001b[36m0.0491\u001b[0m       0.9761        \u001b[35m0.1039\u001b[0m  1.2279\n","     13        \u001b[36m0.0431\u001b[0m       0.9766        \u001b[35m0.1015\u001b[0m  1.2352\n","     14        \u001b[36m0.0381\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.0999\u001b[0m  1.2223\n","     15        \u001b[36m0.0337\u001b[0m       0.9766        \u001b[35m0.0988\u001b[0m  1.2205\n","     16        \u001b[36m0.0299\u001b[0m       0.9766        \u001b[35m0.0981\u001b[0m  1.2378\n","     17        \u001b[36m0.0267\u001b[0m       0.9766        \u001b[35m0.0976\u001b[0m  1.2267\n","     18        \u001b[36m0.0239\u001b[0m       0.9770        \u001b[35m0.0974\u001b[0m  1.2442\n","     19        \u001b[36m0.0215\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.0974\u001b[0m  1.3241\n","     20        \u001b[36m0.0194\u001b[0m       0.9775        0.0974  1.2917\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6838\u001b[0m       \u001b[32m0.5023\u001b[0m        \u001b[35m2.0488\u001b[0m  1.2561\n","      2        \u001b[36m1.3617\u001b[0m       \u001b[32m0.7118\u001b[0m        \u001b[35m0.8882\u001b[0m  1.2477\n","      3        \u001b[36m0.6089\u001b[0m       \u001b[32m0.9021\u001b[0m        \u001b[35m0.4210\u001b[0m  1.2426\n","      4        \u001b[36m0.3246\u001b[0m       \u001b[32m0.9686\u001b[0m        \u001b[35m0.2522\u001b[0m  1.2428\n","      5        \u001b[36m0.2114\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1810\u001b[0m  1.2377\n","      6        \u001b[36m0.1488\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1404\u001b[0m  1.2451\n","      7        \u001b[36m0.1134\u001b[0m       0.9766        \u001b[35m0.1188\u001b[0m  1.2661\n","      8        \u001b[36m0.0917\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.1066\u001b[0m  1.2339\n","      9        \u001b[36m0.0767\u001b[0m       0.9784        \u001b[35m0.0991\u001b[0m  1.2443\n","     10        \u001b[36m0.0653\u001b[0m       0.9784        \u001b[35m0.0939\u001b[0m  1.2425\n","     11        \u001b[36m0.0562\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.0901\u001b[0m  1.2278\n","     12        \u001b[36m0.0488\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.0871\u001b[0m  1.2599\n","     13        \u001b[36m0.0426\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.0847\u001b[0m  1.2186\n","     14        \u001b[36m0.0374\u001b[0m       0.9803        \u001b[35m0.0828\u001b[0m  1.2227\n","     15        \u001b[36m0.0329\u001b[0m       0.9799        \u001b[35m0.0813\u001b[0m  1.2582\n","     16        \u001b[36m0.0291\u001b[0m       0.9803        \u001b[35m0.0800\u001b[0m  1.2292\n","     17        \u001b[36m0.0258\u001b[0m       0.9794        \u001b[35m0.0790\u001b[0m  1.2281\n","     18        \u001b[36m0.0230\u001b[0m       0.9794        \u001b[35m0.0782\u001b[0m  1.2282\n","     19        \u001b[36m0.0206\u001b[0m       0.9799        \u001b[35m0.0775\u001b[0m  1.2357\n","     20        \u001b[36m0.0185\u001b[0m       0.9803        \u001b[35m0.0770\u001b[0m  1.2157\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7097\u001b[0m       \u001b[32m0.5253\u001b[0m        \u001b[35m2.0800\u001b[0m  1.2234\n","      2        \u001b[36m1.3401\u001b[0m       \u001b[32m0.7395\u001b[0m        \u001b[35m0.8655\u001b[0m  1.2702\n","      3        \u001b[36m0.6030\u001b[0m       \u001b[32m0.9264\u001b[0m        \u001b[35m0.4043\u001b[0m  1.2961\n","      4        \u001b[36m0.3110\u001b[0m       \u001b[32m0.9658\u001b[0m        \u001b[35m0.2513\u001b[0m  1.2634\n","      5        \u001b[36m0.1949\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1859\u001b[0m  1.2421\n","      6        \u001b[36m0.1394\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1523\u001b[0m  1.2279\n","      7        \u001b[36m0.1085\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1336\u001b[0m  1.2577\n","      8        \u001b[36m0.0887\u001b[0m       0.9761        \u001b[35m0.1223\u001b[0m  1.2686\n","      9        \u001b[36m0.0745\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1151\u001b[0m  1.2415\n","     10        \u001b[36m0.0637\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.1101\u001b[0m  1.2536\n","     11        \u001b[36m0.0551\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.1064\u001b[0m  1.2389\n","     12        \u001b[36m0.0481\u001b[0m       0.9780        \u001b[35m0.1035\u001b[0m  1.2639\n","     13        \u001b[36m0.0423\u001b[0m       0.9784        \u001b[35m0.1011\u001b[0m  1.2443\n","     14        \u001b[36m0.0374\u001b[0m       0.9775        \u001b[35m0.0990\u001b[0m  1.2709\n","     15        \u001b[36m0.0332\u001b[0m       0.9775        \u001b[35m0.0975\u001b[0m  1.2739\n","     16        \u001b[36m0.0296\u001b[0m       0.9770        \u001b[35m0.0963\u001b[0m  1.2662\n","     17        \u001b[36m0.0265\u001b[0m       0.9770        \u001b[35m0.0954\u001b[0m  1.2446\n","     18        \u001b[36m0.0238\u001b[0m       0.9775        \u001b[35m0.0948\u001b[0m  1.3121\n","     19        \u001b[36m0.0214\u001b[0m       0.9775        \u001b[35m0.0944\u001b[0m  1.2893\n","     20        \u001b[36m0.0193\u001b[0m       0.9770        \u001b[35m0.0942\u001b[0m  1.2690\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6350\u001b[0m       \u001b[32m0.4480\u001b[0m        \u001b[35m2.0007\u001b[0m  1.2576\n","      2        \u001b[36m1.2867\u001b[0m       \u001b[32m0.7376\u001b[0m        \u001b[35m0.9557\u001b[0m  1.2420\n","      3        \u001b[36m0.5882\u001b[0m       \u001b[32m0.8449\u001b[0m        \u001b[35m0.4736\u001b[0m  1.2453\n","      4        \u001b[36m0.3227\u001b[0m       \u001b[32m0.9583\u001b[0m        \u001b[35m0.2879\u001b[0m  1.2219\n","      5        \u001b[36m0.2136\u001b[0m       \u001b[32m0.9630\u001b[0m        \u001b[35m0.2096\u001b[0m  1.2441\n","      6        \u001b[36m0.1520\u001b[0m       \u001b[32m0.9709\u001b[0m        \u001b[35m0.1652\u001b[0m  1.2529\n","      7        \u001b[36m0.1166\u001b[0m       \u001b[32m0.9714\u001b[0m        \u001b[35m0.1436\u001b[0m  1.2464\n","      8        \u001b[36m0.0943\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1300\u001b[0m  1.3000\n","      9        \u001b[36m0.0789\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1206\u001b[0m  1.2605\n","     10        \u001b[36m0.0673\u001b[0m       \u001b[32m0.9756\u001b[0m        \u001b[35m0.1134\u001b[0m  1.2285\n","     11        \u001b[36m0.0583\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1079\u001b[0m  1.2456\n","     12        \u001b[36m0.0510\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.1035\u001b[0m  1.2427\n","     13        \u001b[36m0.0448\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.1002\u001b[0m  1.2489\n","     14        \u001b[36m0.0397\u001b[0m       0.9784        \u001b[35m0.0977\u001b[0m  1.2457\n","     15        \u001b[36m0.0352\u001b[0m       0.9784        \u001b[35m0.0959\u001b[0m  1.2513\n","     16        \u001b[36m0.0314\u001b[0m       0.9780        \u001b[35m0.0945\u001b[0m  1.2283\n","     17        \u001b[36m0.0281\u001b[0m       0.9780        \u001b[35m0.0935\u001b[0m  1.2291\n","     18        \u001b[36m0.0252\u001b[0m       0.9784        \u001b[35m0.0929\u001b[0m  1.2414\n","     19        \u001b[36m0.0227\u001b[0m       0.9780        \u001b[35m0.0924\u001b[0m  1.2415\n","     20        \u001b[36m0.0205\u001b[0m       0.9784        \u001b[35m0.0922\u001b[0m  1.2416\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6780\u001b[0m       \u001b[32m0.5464\u001b[0m        \u001b[35m1.9951\u001b[0m  1.2601\n","      2        \u001b[36m1.3129\u001b[0m       \u001b[32m0.7971\u001b[0m        \u001b[35m0.8500\u001b[0m  1.2231\n","      3        \u001b[36m0.5490\u001b[0m       \u001b[32m0.9231\u001b[0m        \u001b[35m0.3928\u001b[0m  1.2308\n","      4        \u001b[36m0.2771\u001b[0m       \u001b[32m0.9733\u001b[0m        \u001b[35m0.2169\u001b[0m  1.2228\n","      5        \u001b[36m0.1757\u001b[0m       \u001b[32m0.9756\u001b[0m        \u001b[35m0.1534\u001b[0m  1.2266\n","      6        \u001b[36m0.1272\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1249\u001b[0m  1.2239\n","      7        \u001b[36m0.1007\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.1094\u001b[0m  1.2124\n","      8        \u001b[36m0.0838\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.0999\u001b[0m  1.2255\n","      9        \u001b[36m0.0714\u001b[0m       \u001b[32m0.9808\u001b[0m        \u001b[35m0.0936\u001b[0m  1.2551\n","     10        \u001b[36m0.0616\u001b[0m       \u001b[32m0.9813\u001b[0m        \u001b[35m0.0890\u001b[0m  1.2270\n","     11        \u001b[36m0.0535\u001b[0m       \u001b[32m0.9822\u001b[0m        \u001b[35m0.0855\u001b[0m  1.2195\n","     12        \u001b[36m0.0467\u001b[0m       \u001b[32m0.9827\u001b[0m        \u001b[35m0.0826\u001b[0m  1.2697\n","     13        \u001b[36m0.0408\u001b[0m       \u001b[32m0.9831\u001b[0m        \u001b[35m0.0803\u001b[0m  1.2680\n","     14        \u001b[36m0.0359\u001b[0m       \u001b[32m0.9836\u001b[0m        \u001b[35m0.0784\u001b[0m  1.2223\n","     15        \u001b[36m0.0316\u001b[0m       \u001b[32m0.9841\u001b[0m        \u001b[35m0.0769\u001b[0m  1.2157\n","     16        \u001b[36m0.0279\u001b[0m       \u001b[32m0.9845\u001b[0m        \u001b[35m0.0758\u001b[0m  1.2328\n","     17        \u001b[36m0.0248\u001b[0m       0.9836        \u001b[35m0.0749\u001b[0m  1.2569\n","     18        \u001b[36m0.0221\u001b[0m       0.9836        \u001b[35m0.0742\u001b[0m  1.2275\n","     19        \u001b[36m0.0197\u001b[0m       0.9841        \u001b[35m0.0737\u001b[0m  1.2117\n","     20        \u001b[36m0.0177\u001b[0m       0.9836        \u001b[35m0.0733\u001b[0m  1.2415\n","     21        \u001b[36m0.0160\u001b[0m       0.9836        \u001b[35m0.0730\u001b[0m  1.2367\n","     22        \u001b[36m0.0145\u001b[0m       0.9836        \u001b[35m0.0728\u001b[0m  1.2368\n","     23        \u001b[36m0.0132\u001b[0m       0.9836        \u001b[35m0.0727\u001b[0m  1.2393\n","     24        \u001b[36m0.0121\u001b[0m       0.9836        \u001b[35m0.0726\u001b[0m  1.2358\n","     25        \u001b[36m0.0112\u001b[0m       0.9836        \u001b[35m0.0725\u001b[0m  1.2528\n","     26        \u001b[36m0.0103\u001b[0m       0.9841        \u001b[35m0.0725\u001b[0m  1.2405\n","     27        \u001b[36m0.0096\u001b[0m       0.9845        0.0725  1.2229\n","     28        \u001b[36m0.0089\u001b[0m       0.9841        0.0726  1.2227\n","     29        \u001b[36m0.0084\u001b[0m       0.9841        0.0726  1.2274\n","     30        \u001b[36m0.0079\u001b[0m       0.9841        0.0727  1.2238\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6711\u001b[0m       \u001b[32m0.6167\u001b[0m        \u001b[35m1.9795\u001b[0m  1.2271\n","      2        \u001b[36m1.2850\u001b[0m       \u001b[32m0.7765\u001b[0m        \u001b[35m0.8377\u001b[0m  1.2277\n","      3        \u001b[36m0.5710\u001b[0m       \u001b[32m0.9086\u001b[0m        \u001b[35m0.3880\u001b[0m  1.2599\n","      4        \u001b[36m0.2992\u001b[0m       \u001b[32m0.9691\u001b[0m        \u001b[35m0.2536\u001b[0m  1.2428\n","      5        \u001b[36m0.1974\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1900\u001b[0m  1.2219\n","      6        \u001b[36m0.1427\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1546\u001b[0m  1.2306\n","      7        \u001b[36m0.1105\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1343\u001b[0m  1.2799\n","      8        \u001b[36m0.0897\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.1219\u001b[0m  1.2521\n","      9        \u001b[36m0.0748\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.1136\u001b[0m  1.2367\n","     10        \u001b[36m0.0636\u001b[0m       0.9780        \u001b[35m0.1078\u001b[0m  1.2388\n","     11        \u001b[36m0.0549\u001b[0m       0.9780        \u001b[35m0.1034\u001b[0m  1.2645\n","     12        \u001b[36m0.0479\u001b[0m       0.9770        \u001b[35m0.0998\u001b[0m  1.2377\n","     13        \u001b[36m0.0421\u001b[0m       0.9770        \u001b[35m0.0969\u001b[0m  1.2359\n","     14        \u001b[36m0.0372\u001b[0m       0.9766        \u001b[35m0.0945\u001b[0m  1.2307\n","     15        \u001b[36m0.0331\u001b[0m       0.9766        \u001b[35m0.0927\u001b[0m  1.2410\n","     16        \u001b[36m0.0295\u001b[0m       0.9770        \u001b[35m0.0912\u001b[0m  1.2249\n","     17        \u001b[36m0.0265\u001b[0m       0.9775        \u001b[35m0.0902\u001b[0m  1.2234\n","     18        \u001b[36m0.0238\u001b[0m       0.9775        \u001b[35m0.0894\u001b[0m  1.2265\n","     19        \u001b[36m0.0215\u001b[0m       0.9780        \u001b[35m0.0888\u001b[0m  1.2515\n","     20        \u001b[36m0.0195\u001b[0m       0.9780        \u001b[35m0.0885\u001b[0m  1.2339\n","     21        \u001b[36m0.0177\u001b[0m       0.9784        \u001b[35m0.0883\u001b[0m  1.2263\n","     22        \u001b[36m0.0162\u001b[0m       0.9789        \u001b[35m0.0881\u001b[0m  1.2277\n","     23        \u001b[36m0.0149\u001b[0m       0.9789        \u001b[35m0.0880\u001b[0m  1.2091\n","     24        \u001b[36m0.0137\u001b[0m       0.9789        \u001b[35m0.0879\u001b[0m  1.2160\n","     25        \u001b[36m0.0127\u001b[0m       0.9789        0.0880  1.2192\n","     26        \u001b[36m0.0118\u001b[0m       0.9794        0.0880  1.2251\n","     27        \u001b[36m0.0110\u001b[0m       0.9794        0.0881  1.2626\n","     28        \u001b[36m0.0104\u001b[0m       0.9789        0.0882  1.2262\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6704\u001b[0m       \u001b[32m0.4700\u001b[0m        \u001b[35m2.0567\u001b[0m  1.2471\n","      2        \u001b[36m1.2887\u001b[0m       \u001b[32m0.7132\u001b[0m        \u001b[35m0.9711\u001b[0m  1.2453\n","      3        \u001b[36m0.5668\u001b[0m       \u001b[32m0.8819\u001b[0m        \u001b[35m0.4448\u001b[0m  1.3297\n","      4        \u001b[36m0.3003\u001b[0m       \u001b[32m0.9517\u001b[0m        \u001b[35m0.2688\u001b[0m  1.2556\n","      5        \u001b[36m0.1975\u001b[0m       \u001b[32m0.9620\u001b[0m        \u001b[35m0.1999\u001b[0m  1.2596\n","      6        \u001b[36m0.1410\u001b[0m       \u001b[32m0.9691\u001b[0m        \u001b[35m0.1612\u001b[0m  1.2168\n","      7        \u001b[36m0.1100\u001b[0m       \u001b[32m0.9733\u001b[0m        \u001b[35m0.1416\u001b[0m  1.2200\n","      8        \u001b[36m0.0901\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1284\u001b[0m  1.2285\n","      9        \u001b[36m0.0758\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.1193\u001b[0m  1.2172\n","     10        \u001b[36m0.0649\u001b[0m       0.9780        \u001b[35m0.1128\u001b[0m  1.2137\n","     11        \u001b[36m0.0562\u001b[0m       0.9770        \u001b[35m0.1082\u001b[0m  1.2402\n","     12        \u001b[36m0.0491\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.1049\u001b[0m  1.2404\n","     13        \u001b[36m0.0432\u001b[0m       0.9784        \u001b[35m0.1025\u001b[0m  1.2769\n","     14        \u001b[36m0.0381\u001b[0m       0.9789        \u001b[35m0.1008\u001b[0m  1.2405\n","     15        \u001b[36m0.0338\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.0993\u001b[0m  1.2481\n","     16        \u001b[36m0.0301\u001b[0m       0.9789        \u001b[35m0.0982\u001b[0m  1.2281\n","     17        \u001b[36m0.0268\u001b[0m       0.9789        \u001b[35m0.0974\u001b[0m  1.2324\n","     18        \u001b[36m0.0240\u001b[0m       0.9784        \u001b[35m0.0967\u001b[0m  1.2083\n","     19        \u001b[36m0.0216\u001b[0m       0.9784        \u001b[35m0.0962\u001b[0m  1.2382\n","     20        \u001b[36m0.0196\u001b[0m       0.9784        \u001b[35m0.0958\u001b[0m  1.2335\n","     21        \u001b[36m0.0178\u001b[0m       0.9789        \u001b[35m0.0956\u001b[0m  1.2589\n","     22        \u001b[36m0.0163\u001b[0m       0.9794        \u001b[35m0.0954\u001b[0m  1.2330\n","     23        \u001b[36m0.0150\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.0954\u001b[0m  1.2536\n","     24        \u001b[36m0.0138\u001b[0m       0.9799        0.0954  1.2345\n","     25        \u001b[36m0.0128\u001b[0m       0.9799        0.0954  1.2173\n","     26        \u001b[36m0.0119\u001b[0m       0.9799        0.0955  1.2244\n","     27        \u001b[36m0.0111\u001b[0m       0.9794        0.0956  1.2093\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6767\u001b[0m       \u001b[32m0.6167\u001b[0m        \u001b[35m1.9715\u001b[0m  1.2803\n","      2        \u001b[36m1.2713\u001b[0m       \u001b[32m0.7324\u001b[0m        \u001b[35m0.8602\u001b[0m  1.2343\n","      3        \u001b[36m0.5440\u001b[0m       \u001b[32m0.8880\u001b[0m        \u001b[35m0.3932\u001b[0m  1.2218\n","      4        \u001b[36m0.2817\u001b[0m       \u001b[32m0.9705\u001b[0m        \u001b[35m0.2233\u001b[0m  1.2180\n","      5        \u001b[36m0.1821\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1569\u001b[0m  1.2199\n","      6        \u001b[36m0.1312\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.1266\u001b[0m  1.2124\n","      7        \u001b[36m0.1035\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.1096\u001b[0m  1.2247\n","      8        \u001b[36m0.0858\u001b[0m       \u001b[32m0.9817\u001b[0m        \u001b[35m0.0989\u001b[0m  1.2082\n","      9        \u001b[36m0.0730\u001b[0m       \u001b[32m0.9822\u001b[0m        \u001b[35m0.0914\u001b[0m  1.2384\n","     10        \u001b[36m0.0630\u001b[0m       \u001b[32m0.9831\u001b[0m        \u001b[35m0.0860\u001b[0m  1.2173\n","     11        \u001b[36m0.0549\u001b[0m       0.9831        \u001b[35m0.0820\u001b[0m  1.2049\n","     12        \u001b[36m0.0482\u001b[0m       \u001b[32m0.9841\u001b[0m        \u001b[35m0.0790\u001b[0m  1.2296\n","     13        \u001b[36m0.0424\u001b[0m       0.9841        \u001b[35m0.0766\u001b[0m  1.2135\n","     14        \u001b[36m0.0375\u001b[0m       \u001b[32m0.9845\u001b[0m        \u001b[35m0.0747\u001b[0m  1.2173\n","     15        \u001b[36m0.0332\u001b[0m       \u001b[32m0.9850\u001b[0m        \u001b[35m0.0733\u001b[0m  1.2216\n","     16        \u001b[36m0.0295\u001b[0m       0.9850        \u001b[35m0.0721\u001b[0m  1.2175\n","     17        \u001b[36m0.0263\u001b[0m       0.9845        \u001b[35m0.0712\u001b[0m  1.2520\n","     18        \u001b[36m0.0235\u001b[0m       0.9845        \u001b[35m0.0704\u001b[0m  1.2374\n","     19        \u001b[36m0.0211\u001b[0m       0.9845        \u001b[35m0.0698\u001b[0m  1.2374\n","     20        \u001b[36m0.0189\u001b[0m       0.9845        \u001b[35m0.0694\u001b[0m  1.2255\n","     21        \u001b[36m0.0171\u001b[0m       0.9845        \u001b[35m0.0691\u001b[0m  1.2124\n","     22        \u001b[36m0.0154\u001b[0m       0.9850        \u001b[35m0.0688\u001b[0m  1.2294\n","     23        \u001b[36m0.0140\u001b[0m       0.9850        \u001b[35m0.0686\u001b[0m  1.2391\n","     24        \u001b[36m0.0128\u001b[0m       \u001b[32m0.9855\u001b[0m        \u001b[35m0.0685\u001b[0m  1.2201\n","     25        \u001b[36m0.0117\u001b[0m       0.9855        \u001b[35m0.0684\u001b[0m  1.2517\n","     26        \u001b[36m0.0108\u001b[0m       0.9855        \u001b[35m0.0683\u001b[0m  1.3077\n","     27        \u001b[36m0.0100\u001b[0m       0.9855        \u001b[35m0.0683\u001b[0m  1.2203\n","     28        \u001b[36m0.0093\u001b[0m       0.9855        \u001b[35m0.0683\u001b[0m  1.2175\n","     29        \u001b[36m0.0086\u001b[0m       0.9855        0.0683  1.2169\n","     30        \u001b[36m0.0081\u001b[0m       0.9855        0.0683  1.2226\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7626\u001b[0m       \u001b[32m0.5487\u001b[0m        \u001b[35m2.2703\u001b[0m  1.2288\n","      2        \u001b[36m1.4601\u001b[0m       \u001b[32m0.7273\u001b[0m        \u001b[35m0.9189\u001b[0m  1.2195\n","      3        \u001b[36m0.6318\u001b[0m       \u001b[32m0.8918\u001b[0m        \u001b[35m0.4459\u001b[0m  1.2511\n","      4        \u001b[36m0.3200\u001b[0m       \u001b[32m0.9606\u001b[0m        \u001b[35m0.2580\u001b[0m  1.2237\n","      5        \u001b[36m0.1980\u001b[0m       \u001b[32m0.9672\u001b[0m        \u001b[35m0.1867\u001b[0m  1.2302\n","      6        \u001b[36m0.1419\u001b[0m       \u001b[32m0.9714\u001b[0m        \u001b[35m0.1511\u001b[0m  1.2185\n","      7        \u001b[36m0.1107\u001b[0m       \u001b[32m0.9724\u001b[0m        \u001b[35m0.1313\u001b[0m  1.2282\n","      8        \u001b[36m0.0907\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1192\u001b[0m  1.2288\n","      9        \u001b[36m0.0763\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1112\u001b[0m  1.2266\n","     10        \u001b[36m0.0655\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1056\u001b[0m  1.2241\n","     11        \u001b[36m0.0568\u001b[0m       0.9770        \u001b[35m0.1015\u001b[0m  1.2549\n","     12        \u001b[36m0.0498\u001b[0m       0.9761        \u001b[35m0.0983\u001b[0m  1.2470\n","     13        \u001b[36m0.0439\u001b[0m       0.9770        \u001b[35m0.0959\u001b[0m  1.2141\n","     14        \u001b[36m0.0389\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.0941\u001b[0m  1.2261\n","     15        \u001b[36m0.0347\u001b[0m       0.9775        \u001b[35m0.0926\u001b[0m  1.2254\n","     16        \u001b[36m0.0311\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.0916\u001b[0m  1.2302\n","     17        \u001b[36m0.0280\u001b[0m       0.9780        \u001b[35m0.0908\u001b[0m  1.2305\n","     18        \u001b[36m0.0252\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.0903\u001b[0m  1.2266\n","     19        \u001b[36m0.0228\u001b[0m       0.9784        \u001b[35m0.0900\u001b[0m  1.2643\n","     20        \u001b[36m0.0207\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.0899\u001b[0m  1.2234\n","     21        \u001b[36m0.0188\u001b[0m       0.9789        0.0900  1.3176\n","     22        \u001b[36m0.0171\u001b[0m       0.9789        0.0901  1.2477\n","     23        \u001b[36m0.0157\u001b[0m       0.9789        0.0903  1.2402\n","     24        \u001b[36m0.0144\u001b[0m       0.9789        0.0906  1.2101\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7296\u001b[0m       \u001b[32m0.4142\u001b[0m        \u001b[35m2.2741\u001b[0m  1.2461\n","      2        \u001b[36m1.4556\u001b[0m       \u001b[32m0.7451\u001b[0m        \u001b[35m0.9907\u001b[0m  1.2327\n","      3        \u001b[36m0.6016\u001b[0m       \u001b[32m0.8946\u001b[0m        \u001b[35m0.4648\u001b[0m  1.2171\n","      4        \u001b[36m0.3032\u001b[0m       \u001b[32m0.9560\u001b[0m        \u001b[35m0.2626\u001b[0m  1.2279\n","      5        \u001b[36m0.1876\u001b[0m       \u001b[32m0.9677\u001b[0m        \u001b[35m0.1872\u001b[0m  1.2196\n","      6        \u001b[36m0.1358\u001b[0m       \u001b[32m0.9728\u001b[0m        \u001b[35m0.1521\u001b[0m  1.2067\n","      7        \u001b[36m0.1068\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1334\u001b[0m  1.2168\n","      8        \u001b[36m0.0878\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.1216\u001b[0m  1.2374\n","      9        \u001b[36m0.0740\u001b[0m       0.9770        \u001b[35m0.1134\u001b[0m  1.2705\n","     10        \u001b[36m0.0633\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.1076\u001b[0m  1.2235\n","     11        \u001b[36m0.0547\u001b[0m       \u001b[32m0.9794\u001b[0m        \u001b[35m0.1033\u001b[0m  1.2249\n","     12        \u001b[36m0.0476\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.1002\u001b[0m  1.2345\n","     13        \u001b[36m0.0416\u001b[0m       \u001b[32m0.9803\u001b[0m        \u001b[35m0.0979\u001b[0m  1.2212\n","     14        \u001b[36m0.0366\u001b[0m       0.9799        \u001b[35m0.0963\u001b[0m  1.2290\n","     15        \u001b[36m0.0323\u001b[0m       0.9803        \u001b[35m0.0950\u001b[0m  1.2290\n","     16        \u001b[36m0.0287\u001b[0m       0.9803        \u001b[35m0.0942\u001b[0m  1.2189\n","     17        \u001b[36m0.0256\u001b[0m       0.9799        \u001b[35m0.0935\u001b[0m  1.2449\n","     18        \u001b[36m0.0229\u001b[0m       0.9799        \u001b[35m0.0931\u001b[0m  1.2424\n","     19        \u001b[36m0.0206\u001b[0m       0.9803        \u001b[35m0.0929\u001b[0m  1.2177\n","     20        \u001b[36m0.0187\u001b[0m       0.9794        \u001b[35m0.0928\u001b[0m  1.2188\n","     21        \u001b[36m0.0170\u001b[0m       0.9789        0.0928  1.3196\n","     22        \u001b[36m0.0155\u001b[0m       0.9794        0.0930  1.2148\n","     23        \u001b[36m0.0143\u001b[0m       0.9789        0.0932  1.2259\n","     24        \u001b[36m0.0132\u001b[0m       0.9789        0.0934  1.2092\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.7258\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m2.1296\u001b[0m  1.2333\n","      2        \u001b[36m1.3768\u001b[0m       \u001b[32m0.7113\u001b[0m        \u001b[35m0.8676\u001b[0m  1.2159\n","      3        \u001b[36m0.5825\u001b[0m       \u001b[32m0.9002\u001b[0m        \u001b[35m0.4272\u001b[0m  1.2350\n","      4        \u001b[36m0.3081\u001b[0m       \u001b[32m0.9695\u001b[0m        \u001b[35m0.2381\u001b[0m  1.2324\n","      5        \u001b[36m0.1959\u001b[0m       \u001b[32m0.9738\u001b[0m        \u001b[35m0.1638\u001b[0m  1.2287\n","      6        \u001b[36m0.1379\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.1288\u001b[0m  1.2250\n","      7        \u001b[36m0.1058\u001b[0m       \u001b[32m0.9799\u001b[0m        \u001b[35m0.1104\u001b[0m  1.2301\n","      8        \u001b[36m0.0863\u001b[0m       \u001b[32m0.9813\u001b[0m        \u001b[35m0.0998\u001b[0m  1.2815\n","      9        \u001b[36m0.0727\u001b[0m       \u001b[32m0.9817\u001b[0m        \u001b[35m0.0929\u001b[0m  1.2388\n","     10        \u001b[36m0.0624\u001b[0m       \u001b[32m0.9822\u001b[0m        \u001b[35m0.0882\u001b[0m  1.2426\n","     11        \u001b[36m0.0540\u001b[0m       \u001b[32m0.9831\u001b[0m        \u001b[35m0.0847\u001b[0m  1.2403\n","     12        \u001b[36m0.0472\u001b[0m       0.9831        \u001b[35m0.0820\u001b[0m  1.2351\n","     13        \u001b[36m0.0414\u001b[0m       \u001b[32m0.9845\u001b[0m        \u001b[35m0.0799\u001b[0m  1.2223\n","     14        \u001b[36m0.0364\u001b[0m       \u001b[32m0.9850\u001b[0m        \u001b[35m0.0782\u001b[0m  1.2260\n","     15        \u001b[36m0.0322\u001b[0m       0.9845        \u001b[35m0.0767\u001b[0m  1.2273\n","     16        \u001b[36m0.0285\u001b[0m       0.9845        \u001b[35m0.0756\u001b[0m  1.2648\n","     17        \u001b[36m0.0253\u001b[0m       0.9845        \u001b[35m0.0746\u001b[0m  1.2420\n","     18        \u001b[36m0.0226\u001b[0m       0.9845        \u001b[35m0.0738\u001b[0m  1.2294\n","     19        \u001b[36m0.0202\u001b[0m       0.9841        \u001b[35m0.0731\u001b[0m  1.2307\n","     20        \u001b[36m0.0181\u001b[0m       0.9836        \u001b[35m0.0726\u001b[0m  1.2770\n","     21        \u001b[36m0.0163\u001b[0m       0.9836        \u001b[35m0.0722\u001b[0m  1.2608\n","     22        \u001b[36m0.0147\u001b[0m       0.9836        \u001b[35m0.0719\u001b[0m  1.2331\n","     23        \u001b[36m0.0134\u001b[0m       0.9836        \u001b[35m0.0716\u001b[0m  1.2244\n","     24        \u001b[36m0.0122\u001b[0m       0.9841        \u001b[35m0.0714\u001b[0m  1.2786\n","     25        \u001b[36m0.0112\u001b[0m       0.9836        \u001b[35m0.0713\u001b[0m  1.2186\n","     26        \u001b[36m0.0103\u001b[0m       0.9841        \u001b[35m0.0713\u001b[0m  1.2456\n","     27        \u001b[36m0.0095\u001b[0m       0.9841        \u001b[35m0.0712\u001b[0m  1.2270\n","     28        \u001b[36m0.0089\u001b[0m       0.9841        0.0712  1.2328\n","     29        \u001b[36m0.0083\u001b[0m       0.9841        0.0713  1.2203\n","     30        \u001b[36m0.0078\u001b[0m       0.9841        0.0713  1.2257\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6180\u001b[0m       \u001b[32m0.4522\u001b[0m        \u001b[35m1.9712\u001b[0m  1.2808\n","      2        \u001b[36m1.3509\u001b[0m       \u001b[32m0.7709\u001b[0m        \u001b[35m0.8939\u001b[0m  1.2353\n","      3        \u001b[36m0.5987\u001b[0m       \u001b[32m0.9157\u001b[0m        \u001b[35m0.4088\u001b[0m  1.2257\n","      4        \u001b[36m0.3046\u001b[0m       \u001b[32m0.9667\u001b[0m        \u001b[35m0.2486\u001b[0m  1.2339\n","      5        \u001b[36m0.1950\u001b[0m       \u001b[32m0.9724\u001b[0m        \u001b[35m0.1834\u001b[0m  1.2290\n","      6        \u001b[36m0.1406\u001b[0m       \u001b[32m0.9752\u001b[0m        \u001b[35m0.1508\u001b[0m  1.2225\n","      7        \u001b[36m0.1096\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.1329\u001b[0m  1.2263\n","      8        \u001b[36m0.0898\u001b[0m       0.9775        \u001b[35m0.1221\u001b[0m  1.2403\n","      9        \u001b[36m0.0758\u001b[0m       0.9770        \u001b[35m0.1150\u001b[0m  1.2492\n","     10        \u001b[36m0.0652\u001b[0m       0.9766        \u001b[35m0.1100\u001b[0m  1.2602\n","     11        \u001b[36m0.0568\u001b[0m       0.9775        \u001b[35m0.1064\u001b[0m  1.2494\n","     12        \u001b[36m0.0499\u001b[0m       0.9775        \u001b[35m0.1037\u001b[0m  1.2427\n","     13        \u001b[36m0.0441\u001b[0m       0.9766        \u001b[35m0.1015\u001b[0m  1.2300\n","     14        \u001b[36m0.0391\u001b[0m       0.9766        \u001b[35m0.0998\u001b[0m  1.2230\n","     15        \u001b[36m0.0348\u001b[0m       0.9766        \u001b[35m0.0984\u001b[0m  1.3262\n","     16        \u001b[36m0.0310\u001b[0m       0.9766        \u001b[35m0.0973\u001b[0m  1.2425\n","     17        \u001b[36m0.0278\u001b[0m       0.9766        \u001b[35m0.0964\u001b[0m  1.2273\n","     18        \u001b[36m0.0249\u001b[0m       0.9766        \u001b[35m0.0958\u001b[0m  1.2588\n","     19        \u001b[36m0.0224\u001b[0m       0.9766        \u001b[35m0.0953\u001b[0m  1.2453\n","     20        \u001b[36m0.0203\u001b[0m       0.9770        \u001b[35m0.0950\u001b[0m  1.2351\n","     21        \u001b[36m0.0184\u001b[0m       0.9770        \u001b[35m0.0949\u001b[0m  1.2411\n","     22        \u001b[36m0.0167\u001b[0m       0.9775        \u001b[35m0.0949\u001b[0m  1.2340\n","     23        \u001b[36m0.0153\u001b[0m       0.9775        0.0950  1.2297\n","     24        \u001b[36m0.0141\u001b[0m       0.9770        0.0951  1.2333\n","     25        \u001b[36m0.0130\u001b[0m       0.9766        0.0953  1.2617\n","     26        \u001b[36m0.0121\u001b[0m       0.9766        0.0956  1.2521\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.6154\u001b[0m       \u001b[32m0.4705\u001b[0m        \u001b[35m1.9969\u001b[0m  1.2403\n","      2        \u001b[36m1.2677\u001b[0m       \u001b[32m0.7530\u001b[0m        \u001b[35m0.9302\u001b[0m  1.2287\n","      3        \u001b[36m0.5545\u001b[0m       \u001b[32m0.8833\u001b[0m        \u001b[35m0.4426\u001b[0m  1.2317\n","      4        \u001b[36m0.2896\u001b[0m       \u001b[32m0.9634\u001b[0m        \u001b[35m0.2517\u001b[0m  1.2369\n","      5        \u001b[36m0.1789\u001b[0m       \u001b[32m0.9700\u001b[0m        \u001b[35m0.1788\u001b[0m  1.2308\n","      6        \u001b[36m0.1281\u001b[0m       \u001b[32m0.9742\u001b[0m        \u001b[35m0.1468\u001b[0m  1.2439\n","      7        \u001b[36m0.1004\u001b[0m       \u001b[32m0.9756\u001b[0m        \u001b[35m0.1301\u001b[0m  1.2316\n","      8        \u001b[36m0.0827\u001b[0m       \u001b[32m0.9761\u001b[0m        \u001b[35m0.1198\u001b[0m  1.2121\n","      9        \u001b[36m0.0700\u001b[0m       0.9761        \u001b[35m0.1129\u001b[0m  1.2248\n","     10        \u001b[36m0.0603\u001b[0m       \u001b[32m0.9766\u001b[0m        \u001b[35m0.1079\u001b[0m  1.2216\n","     11        \u001b[36m0.0526\u001b[0m       0.9766        \u001b[35m0.1041\u001b[0m  1.2330\n","     12        \u001b[36m0.0461\u001b[0m       \u001b[32m0.9770\u001b[0m        \u001b[35m0.1013\u001b[0m  1.2546\n","     13        \u001b[36m0.0407\u001b[0m       \u001b[32m0.9780\u001b[0m        \u001b[35m0.0992\u001b[0m  1.2917\n","     14        \u001b[36m0.0361\u001b[0m       \u001b[32m0.9784\u001b[0m        \u001b[35m0.0977\u001b[0m  1.2925\n","     15        \u001b[36m0.0321\u001b[0m       0.9784        \u001b[35m0.0966\u001b[0m  1.2349\n","     16        \u001b[36m0.0286\u001b[0m       0.9784        \u001b[35m0.0958\u001b[0m  1.2325\n","     17        \u001b[36m0.0256\u001b[0m       0.9780        \u001b[35m0.0953\u001b[0m  1.2358\n","     18        \u001b[36m0.0231\u001b[0m       0.9784        \u001b[35m0.0950\u001b[0m  1.2266\n","     19        \u001b[36m0.0209\u001b[0m       0.9784        \u001b[35m0.0949\u001b[0m  1.2167\n","     20        \u001b[36m0.0190\u001b[0m       \u001b[32m0.9789\u001b[0m        \u001b[35m0.0948\u001b[0m  1.2316\n","     21        \u001b[36m0.0173\u001b[0m       0.9789        0.0949  1.2280\n","     22        \u001b[36m0.0159\u001b[0m       \u001b[32m0.9794\u001b[0m        0.0950  1.2664\n","     23        \u001b[36m0.0147\u001b[0m       0.9794        0.0952  1.2182\n","     24        \u001b[36m0.0136\u001b[0m       0.9794        0.0954  1.2306\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","  epoch    train_loss    valid_acc    valid_loss     dur\n","-------  ------------  -----------  ------------  ------\n","      1        \u001b[36m2.2857\u001b[0m       \u001b[32m0.6894\u001b[0m        \u001b[35m1.2004\u001b[0m  1.8220\n","      2        \u001b[36m0.6975\u001b[0m       \u001b[32m0.9116\u001b[0m        \u001b[35m0.3800\u001b[0m  1.8266\n","      3        \u001b[36m0.2765\u001b[0m       \u001b[32m0.9722\u001b[0m        \u001b[35m0.2067\u001b[0m  1.8697\n","      4        \u001b[36m0.1580\u001b[0m       \u001b[32m0.9750\u001b[0m        \u001b[35m0.1451\u001b[0m  1.8329\n","      5        \u001b[36m0.1114\u001b[0m       \u001b[32m0.9753\u001b[0m        \u001b[35m0.1208\u001b[0m  1.8325\n","      6        \u001b[36m0.0884\u001b[0m       \u001b[32m0.9775\u001b[0m        \u001b[35m0.1089\u001b[0m  1.8464\n","      7        \u001b[36m0.0737\u001b[0m       \u001b[32m0.9781\u001b[0m        \u001b[35m0.1019\u001b[0m  1.8639\n","      8        \u001b[36m0.0630\u001b[0m       \u001b[32m0.9788\u001b[0m        \u001b[35m0.0972\u001b[0m  1.8350\n","      9        \u001b[36m0.0545\u001b[0m       \u001b[32m0.9791\u001b[0m        \u001b[35m0.0938\u001b[0m  1.9739\n","     10        \u001b[36m0.0476\u001b[0m       0.9791        \u001b[35m0.0913\u001b[0m  1.8369\n","     11        \u001b[36m0.0418\u001b[0m       \u001b[32m0.9797\u001b[0m        \u001b[35m0.0897\u001b[0m  1.8209\n","     12        \u001b[36m0.0369\u001b[0m       \u001b[32m0.9806\u001b[0m        \u001b[35m0.0885\u001b[0m  1.8218\n","     13        \u001b[36m0.0327\u001b[0m       0.9803        \u001b[35m0.0877\u001b[0m  1.8189\n","     14        \u001b[36m0.0291\u001b[0m       0.9803        \u001b[35m0.0871\u001b[0m  1.8487\n","     15        \u001b[36m0.0259\u001b[0m       0.9803        \u001b[35m0.0867\u001b[0m  1.8541\n","     16        \u001b[36m0.0232\u001b[0m       0.9797        \u001b[35m0.0864\u001b[0m  1.8251\n","     17        \u001b[36m0.0208\u001b[0m       0.9794        \u001b[35m0.0863\u001b[0m  1.8205\n","     18        \u001b[36m0.0188\u001b[0m       0.9794        \u001b[35m0.0862\u001b[0m  1.8236\n","     19        \u001b[36m0.0169\u001b[0m       0.9803        0.0862  1.8659\n","     20        \u001b[36m0.0154\u001b[0m       0.9806        0.0863  1.8394\n","     21        \u001b[36m0.0140\u001b[0m       0.9803        0.0864  1.8293\n","     22        \u001b[36m0.0128\u001b[0m       0.9803        0.0866  1.8383\n","Stopping since valid_loss has not improved in the last 5 epochs.\n","Best parameters: {'callbacks': [('EarlyStopping', <skorch.callbacks.training.EarlyStopping object at 0x7de66474ba00>)], 'lr': 0.1, 'max_epochs': 30, 'module__input_size': 5000, 'module__nonlin': <function relu at 0x7de68ccf08b0>, 'module__num_units': 200}\n","Best score: 0.9802501248379017\n"]}],"source":["# Create GridSearchCV object\n","\n","gs = GridSearchCV(net_cv_gs, param_grid, refit=True, cv=3, scoring='accuracy')\n","\n","\n","\n","# Fit the GridSearchCV object\n","\n","gs.fit(X_cv_updated, y_cv_updted)\n","\n","\n","\n","# Print the best parameters and score\n","\n","print(\"Best parameters:\", gs.best_params_)\n","\n","print(\"Best score:\", gs.best_score_)\n","\n","\n","\n","# You can now use the best estimator to make predictions\n","\n","best_model = gs.best_estimator_"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T23:54:42.705494Z","iopub.status.busy":"2024-10-11T23:54:42.705039Z","iopub.status.idle":"2024-10-11T23:54:43.275211Z","shell.execute_reply":"2024-10-11T23:54:43.274148Z","shell.execute_reply.started":"2024-10-11T23:54:42.705443Z"},"id":"fH_fhHu82Fxh","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy with Best Model: 0.98\n"]}],"source":["# Predict and get accuracy using best model\n","\n","\n","\n","y_pred_best = best_model.predict(X_cv_updated_test)\n","\n","test_accuracy_best = np.mean(y_pred_best == y_test_np)\n","\n","print(f\"Test Accuracy with Best Model: {test_accuracy_best}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Best Parameters: \n","\n","* Learning Rate: 0.1\n","\n","* Max Epochs: 30\n","\n","* Module Input Size: 5000\n","\n","* Activation: ReLU\n","\n","* Number of Units: 200\n","\n","* Early Stopping: 5 Epochs\n","\n","\n","\n","### The effect of hyperparametrs is significant on the training. Some of the observations are as follows:\n","\n","* Changing learning rate from 0.01 to 0.1 results in a massive improvement in the validation accuracy. This is seen when the validation accuracy improves from ~30% to nearly 97.1%. This may be because increasing the learning rate resulted in \"escaping\" the local minima and converge faster.\n","\n","* Having more units in a layer does not necessarily mean a better accuracy. When the number of units increased from 200 to 300, the accuracy dropped instead of increasing. However, it increased when the number was changed from 100 to 200. This probably implies that 200 units provide sufficient complexity for the model to perform at its best and 300 just leads to overfitting. \n","\n","* ReLU outperforms the other activation functions like tanh (not shown on grid search but tested independently). This may be because ReLU does not suffer from the vanishing gradient problem. \n","\n","* Increasing max_features for the vectorizer (from 100 to 5000) leads to noticeable increase in accuracy from ~75% to ~97%. More features allowed our models to pick up on even more patterns in the text to make accurate predictions. \n","\n","* Early stopping did kick in during training when accuracy stalled for 5 epochs. This allowed the grid search to run at an accelerated pace and find the hyperparameters faster. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lr48VZEGcXAb"},"source":["\n","\n","---\n","\n","\n","\n","📝❓ Write your lab report here addressing all questions in the notebook"]},{"cell_type":"markdown","metadata":{"id":"QFJ8eObYcyss"},"source":["# Lab Report\n","\n","\n","\n","## Introduction\n","\n","\n","\n","In this lab, we explored the use of neural networks for language classification using the `skorch` library. We experimented with different vectorizers and hyperparameters to improve the model's performance. The dataset consisted of text data in various languages, and the goal was to classify the text into one of the 20 languages.\n","\n","\n","\n","## Data Preparation\n","\n","\n","\n","We started by preparing the dataset, which involved:\n","\n","- Downloading the dataset.\n","\n","- Combining the training and testing data into dataframes.\n","\n","- Filtering the data to include only the 20 selected languages.\n","\n","- Splitting the data into training and testing sets.\n","\n","- Reorganising the training and test datasets to 80:20 split.\n","\n","- Encoding the labels using `LabelEncoder`.\n","\n","\n","\n","## Feature Extraction\n","\n","\n","\n","We experimented with different feature extraction techniques:\n","\n","- **Count Vectorizer**: Extracted character-level bigrams with a maximum of 100 and 5000 features.\n","\n","- **TF-IDF Vectorizer**: Extracted character-level bigrams with a maximum of 5000 features.\n","\n","\n","\n","## Neural Network Architecture\n","\n","\n","\n","We did not try to improve the vanilla neural network provided in the code template (other than altering the number of units in the hidden layer). This showed how a simple MLP is capable of outperforming ML techniques introduced in part 1 of the assignment. \n","\n","\n","\n","## Experiments and Results\n","\n","\n","\n","### Initial Experiments\n","\n","\n","\n","1. **Count Vectorizer with 100 Features**:\n","\n","    - Achieved a test accuracy of 68.2%.\n","\n","\n","\n","2. **Count Vectorizer with 5000 Features**:\n","\n","    - Improved test accuracy to ~98.1%.\n","\n","\n","\n","3. **TF-IDF Vectorizer**:\n","\n","    - Achieved a test accuracy of ~95.78%.\n","\n","\n","\n","### Why choose `Count Vectorizer` over `TF-IDF Vectoizer` for our grid search?\n","\n","- For our language classification task, the Count Vectorizer showed a slightly higher accuracy (98%) compared to TF-IDF (95.78%). This aligns with expectations for language identification, where the mere presence and frequency of specific character patterns or words are often more indicative of the language than their relative importance across documents.\n","\n","\n","### Hyperparameter Tuning\n","\n","\n","\n","We used `GridSearchCV` to find the best hyperparameters. The best parameters were:\n","\n","- Learning Rate: 0.1\n","\n","- Max Epochs: 30\n","\n","- Module Input Size: 5000\n","\n","- Activation: ReLU\n","\n","- Number of Units: 200\n","\n","\n","\n","The best model achieved a test accuracy of ~98%.\n","\n","\n","\n","### Observations Summary\n","\n","\n","\n","- **Learning Rate**: Increasing the learning rate from 0.01 to 0.1 resulted in a significant improvement in validation accuracy.\n","\n","- **Number of Units**: 200 units provided the best performance, while increasing to 300 units led to overfitting.\n","\n","- **Activation Function**: ReLU was chosen as initial testing done with tanh weren't promising.\n","\n","- **Vectorizer Features**: Increasing the maximum features for the vectorizer from 100 to 5000 led to a noticeable increase in accuracy.\n","\n","- **Early Stopping**: Did kick in during grid search where stalled accuracy led to training being stopped and a new combination of hyperparameters being tested.\n","\n","\n","\n","## Conclusion\n","\n","\n","\n","The experiments demonstrated the importance of hyperparameter tuning and feature extraction in improving the performance of neural networks for language classification. The best model achieved a test accuracy of ~98%, highlighting the effectiveness of the chosen hyperparameters and vectorizer settings.\n","\n","\n","\n","---\n","\n","\n","\n","## Questions\n","\n","\n","\n","### What is the effect of your modifications on validation performance? Discuss potential reasons.\n","\n","\n","* Changing learning rate from 0.01 to 0.1 results in a massive improvement in the validation accuracy. This is seen when the validation accuracy improves from ~30% to nearly 97.1%. This may be because increasing the learning rate resulted in \"escaping\" the local minima and converge faster.\n","\n","* Having more units in a layer does not necessarily mean a better accuracy. When the number of units increased from 200 to 300, the accuracy dropped instead of increasing. However, it increased when the number was changed from 100 to 200. This probably implies that 200 units provide sufficient complexity for the model to perform at its best and 300 just leads to overfitting. \n","\n","* ReLU outperforms the other activation functions like tanh (not shown on grid search but tested independently). This may be because ReLU does not suffer from the vanishing gradient problem. \n","\n","* Increasing max_features for the vectorizer (from 100 to 5000) leads to noticeable increase in accuracy from ~75% to ~97%. More features allowed our models to pick up on even more patterns in the text to make accurate predictions. \n","\n","* Early stopping did kick in during training when accuracy stalled for 5 epochs. This allowed the grid search to run at an accelerated pace and find the hyperparameters faster. "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"vscode":{"interpreter":{"hash":"bd97b8bffa4d3737e84826bc3d37be3046061822757ce35137ab82ad4c5a2016"}}},"nbformat":4,"nbformat_minor":4}
