{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Andrian0s/ML4NLP1-2024-Tutorial-Notebooks/blob/main/exercises/ex2/Exercise_2_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:25.495109Z","iopub.status.busy":"2024-10-21T13:36:25.494565Z","iopub.status.idle":"2024-10-21T13:36:25.505725Z","shell.execute_reply":"2024-10-21T13:36:25.504877Z","shell.execute_reply.started":"2024-10-21T13:36:25.495072Z"},"id":"gALCagMSOnRz","trusted":true},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"jgHDDrKlO35U"},"source":["### Source: [link](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words)\n","\n","# Word Embeddings: Encoding Lexical Semantics\n","\n","Word embeddings are dense vectors of real numbers, one per word in your\n","vocabulary. In NLP, it is almost always the case that your features are\n","words! But how should you represent a word in a computer? You could\n","store its ascii character representation, but that only tells you what\n","the word *is*, it doesn't say much about what it *means* (you might be\n","able to derive its part of speech from its affixes, or properties from\n","its capitalization, but not much). Even more, in what sense could you\n","combine these representations? We often want dense outputs from our\n","neural networks, where the inputs are $|V|$ dimensional, where\n","$V$ is our vocabulary, but often the outputs are only a few\n","dimensional (if we are only predicting a handful of labels, for\n","instance). How do we get from a massive dimensional space to a smaller\n","dimensional space?\n","\n","How about instead of ascii representations, we use a one-hot encoding?\n","That is, we represent the word $w$ by\n","\n","\\begin{align}\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\\end{align}\n","\n","where the 1 is in a location unique to $w$. Any other word will\n","have a 1 in some other location, and a 0 everywhere else.\n","\n","\n","There is an enormous drawback to this representation, besides just how\n","huge it is. It basically treats all words as independent entities with\n","no relation to each other. What we really want is some notion of\n","*similarity* between words. Why? Let's see an example.\n","\n","Suppose we are building a language model. Suppose we have seen the\n","sentences\n","\n","* The mathematician ran to the store.\n","\n","* The physicist ran to the store.\n","\n","* The mathematician solved the open problem.\n","\n","in our training data. Now suppose we get a new sentence never before\n","seen in our training data:\n","\n","* The physicist solved the open problem.\n","\n","Our language model might do OK on this sentence, but wouldn't it be much\n","better if we could use the following two facts:\n","\n","* We have seen  mathematician and physicist in the same role in a sentence. Somehow they\n","  have a semantic relation.\n","* We have seen mathematician in the same role  in this new unseen sentence\n","  as we are now seeing physicist.\n","\n","and then infer that physicist is actually a good fit in the new unseen\n","sentence? This is what we mean by a notion of similarity: we mean\n","*semantic similarity*, not simply having similar orthographic\n","representations. It is a technique to combat the sparsity of linguistic\n","data, by connecting the dots between what we have seen and what we\n","haven't. This example of course relies on a fundamental linguistic\n","assumption: that words appearing in similar contexts are related to each\n","other semantically. This is called the `distributional\n","hypothesis <https://en.wikipedia.org/wiki/Distributional_semantics>`__.\n","\n","# Getting Dense Word Embeddings\n","\n","How can we solve this problem? That is, how could we actually encode\n","semantic similarity in words? Maybe we think up some semantic\n","attributes. For example, we see that both mathematicians and physicists\n","can run, so maybe we give these words a high score for the \"is able to\n","run\" semantic attribute. Think of some other attributes, and imagine\n","what you might score some common words on those attributes.\n","\n","If each attribute is a dimension, then we might give each word a vector,\n","like this:\n","\n","\\begin{align}q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n","   \\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n","\n","\\begin{align}q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n","   \\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]\\end{align}\n","\n","Then we can get a measure of similarity between these words by doing:\n","\n","\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\\end{align}\n","\n","Although it is more common to normalize by the lengths:\n","\n","\\begin{align}\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n","   {\\| q_\\text{\\physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)\\end{align}\n","\n","Where $\\phi$ is the angle between the two vectors. That way,\n","extremely similar words (words whose embeddings point in the same\n","direction) will have similarity 1. Extremely dissimilar words should\n","have similarity -1.\n","\n","You can think of the sparse one-hot vectors from the beginning of this\n","section as a special case of these new vectors we have defined, where\n","each word basically has similarity 0, and we gave each word some unique\n","semantic attribute. These new vectors are *dense*, which is to say their\n","entries are (typically) non-zero.\n","\n","But these new vectors are a big pain: you could think of thousands of\n","different semantic attributes that might be relevant to determining\n","similarity, and how on earth would you set the values of the different\n","attributes? Central to the idea of deep learning is that the neural\n","network learns representations of the features, rather than requiring\n","the programmer to design them herself. So why not just let the word\n","embeddings be parameters in our model, and then be updated during\n","training? This is exactly what we will do. We will have some *latent\n","semantic attributes* that the network can, in principle, learn. Note\n","that the word embeddings will probably not be interpretable. That is,\n","although with our hand-crafted vectors above we can see that\n","mathematicians and physicists are similar in that they both like coffee,\n","if we allow a neural network to learn the embeddings and see that both\n","mathematicians and physicists have a large value in the second\n","dimension, it is not clear what that means. They are similar in some\n","latent semantic dimension, but this probably has no interpretation to\n","us.\n","\n","\n","In summary, **word embeddings are a representation of the *semantics* of\n","a word, efficiently encoding semantic information that might be relevant\n","to the task at hand**. You can embed other things too: part of speech\n","tags, parse trees, anything! The idea of feature embeddings is central\n","to the field.\n","\n","# Word Embeddings in Pytorch\n","\n","Before we get to a worked example and an exercise, a few quick notes\n","about how to use embeddings in Pytorch and in deep learning programming\n","in general. Similar to how we defined a unique index for each word when\n","making one-hot vectors, we also need to define an index for each word\n","when using embeddings. These will be keys into a lookup table. That is,\n","embeddings are stored as a $|V| \\times D$ matrix, where $D$\n","is the dimensionality of the embeddings, such that the word assigned\n","index $i$ has its embedding stored in the $i$'th row of the\n","matrix. In all of my code, the mapping from words to indices is a\n","dictionary named word\\_to\\_ix.\n","\n","The module that allows you to use embeddings is torch.nn.Embedding,\n","which takes two arguments: the vocabulary size, and the dimensionality\n","of the embeddings.\n","\n","To index into this table, you must use torch.LongTensor (since the\n","indices are integers, not floats)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-21T13:36:25.523730Z","iopub.status.busy":"2024-10-21T13:36:25.523425Z","iopub.status.idle":"2024-10-21T13:36:29.951416Z","shell.execute_reply":"2024-10-21T13:36:29.950335Z","shell.execute_reply.started":"2024-10-21T13:36:25.523698Z"},"id":"M_Q2GVH0OwvH","outputId":"d1992c0c-54c3-4815-dfd1-034ca82961cb","trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x794fd97884d0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Author: Robert Guthrie\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","\n","torch.manual_seed(1)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-21T13:36:29.954318Z","iopub.status.busy":"2024-10-21T13:36:29.953766Z","iopub.status.idle":"2024-10-21T13:36:30.081750Z","shell.execute_reply":"2024-10-21T13:36:30.080600Z","shell.execute_reply.started":"2024-10-21T13:36:29.954271Z"},"id":"MN96_fK5Plv6","outputId":"06242940-0964-469a-8f61-39aba10332d3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}],"source":["word_to_ix = {\"hello\": 0, \"world\": 1}\n","embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n","lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n","hello_embed = embeds(lookup_tensor)\n","print(hello_embed)"]},{"cell_type":"markdown","metadata":{"id":"l3ecRemdPstX"},"source":["# An Example: N-Gram Language Modeling\n","\n","Recall that in an n-gram language model, given a sequence of words\n","$w$, we want to compute\n","\n","\\begin{align}P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\\end{align}\n","\n","Where $w_i$ is the ith word of the sequence.\n","\n","In this example, we will compute the loss function on some training\n","examples and update the parameters with backpropagation."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-10-21T13:36:30.084157Z","iopub.status.busy":"2024-10-21T13:36:30.083371Z","iopub.status.idle":"2024-10-21T13:36:32.165868Z","shell.execute_reply":"2024-10-21T13:36:32.164895Z","shell.execute_reply.started":"2024-10-21T13:36:30.084106Z"},"id":"FbPuck6EPn2x","outputId":"9b037d14-ef15-4153-f94a-61af2072e623","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n","Loss in Epoch 0: 520.92\n","Loss in Epoch 1: 518.34\n","Loss in Epoch 2: 515.78\n","Loss in Epoch 3: 513.23\n","Loss in Epoch 4: 510.71\n","Loss in Epoch 5: 508.2\n","Loss in Epoch 6: 505.7\n","Loss in Epoch 7: 503.22\n","Loss in Epoch 8: 500.74\n","Loss in Epoch 9: 498.28\n"]}],"source":["CONTEXT_SIZE = 2\n","\n","EMBEDDING_DIM = 10\n","\n","# We will use Shakespeare Sonnet 2\n","\n","test_sentence = \"\"\"When forty winters shall besiege thy brow,\n","And dig deep trenches in thy beauty's field,\n","Thy youth's proud livery so gazed on now,\n","Will be a totter'd weed of small worth held:\n","Then being asked, where all thy beauty lies,\n","Where all the treasure of thy lusty days;\n","To say, within thine own deep sunken eyes,\n","Were an all-eating shame, and thriftless praise.\n","How much more praise deserv'd thy beauty's use,\n","If thou couldst answer 'This fair child of mine\n","Shall sum my count, and make my old excuse,'\n","Proving his beauty by succession thine!\n","This were to be new made when thou art old,\n","And see thy blood warm when thou feel'st it cold.\"\"\".split()\n","\n","# we should tokenize the input, but we will ignore that for now\n","# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n","trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n","            for i in range(len(test_sentence) - 2)]\n","\n","# print the first 3, just so you can see what they look like\n","print(trigrams[:3])\n","\n","vocab = set(test_sentence)\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","\n","class NGramLanguageModeler(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, context_size):\n","        super(NGramLanguageModeler, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n","        self.linear2 = nn.Linear(128, vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs).view((1, -1))\n","        out = F.relu(self.linear1(embeds))\n","        out = self.linear2(out)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","losses = []\n","loss_function = nn.NLLLoss() # Negative Log Likelihood Loss\n","\n","model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","for epoch in range(10):\n","    total_loss = 0\n","    for context, target in trigrams:\n","        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n","        # into integer indices and wrap them in tensors)\n","        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n","\n","        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n","        # new instance, you need to zero out the gradients from the old\n","        # instance\n","        model.zero_grad()\n","\n","        # Step 3. Run the forward pass, getting log probabilities over next\n","        # words\n","        log_probs = model(context_idxs)\n","\n","        # Step 4. Compute your loss function. (Again, Torch wants the target\n","        # word wrapped in a tensor)\n","        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n","\n","        # Step 5. Do the backward pass and update the gradient\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Get the Python number from a 1-element Tensor by calling tensor.item()\n","        total_loss += loss.item()\n","\n","    print(\"Loss in Epoch {ep}: {l}\".format(ep=epoch, l=np.round(total_loss, 2))) # The loss decreased every iteration over the training data!\n","    losses.append(total_loss)"]},{"cell_type":"markdown","metadata":{"id":"7fV7zUnlz7xp"},"source":["# Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n","\n","The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep\n","learning. It is a model that tries to predict words given the context of\n","a few words before and a few words after the target word. This is\n","distinct from language modeling, since CBOW is not sequential and does\n","not have to be probabilistic. Typcially, CBOW is used to quickly train\n","word embeddings, and these embeddings are used to initialize the\n","embeddings of some more complicated model. Usually, this is referred to\n","as *pretraining embeddings*. It almost always helps performance a couple\n","of percent.\n","\n","The CBOW model is as follows. Given a target word $w_i$ and an\n","$N$ context window on each side, $w_{i-1}, \\dots, w_{i-N}$\n","and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words\n","collectively as $C$, CBOW tries to minimize\n","\n","\\begin{align}-\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)\\end{align}\n","\n","where $q_w$ is the embedding for word $w$."]},{"cell_type":"markdown","metadata":{"id":"Pj_2_qqM7Md_"},"source":["## Exercise Layout\n","\n","### 1. <u>Training CBOW Embeddings</u>\n","\n","1.1) Implement a CBOW Model by completing ```class CBOW(nn.Module)``` and train it on ```raw_text```.    \n","\n","1.2) Load Datasets ```tripadvisor_hotel_reviews_reduced.csv``` and ```scifi_reduced.txt```.     \n","\n","1.3) Decide preprocessing steps by completing the function ```def custom_preprocess()```. Describe your decisions. Note that it's your choice to create different preprocessing functions for hotel reviews and scifi datasets or use the same preprocessing function.            \n","\n","1.4) Train CBOW2 with a context width of 2 (in both directions) for the Hotel Reviews dataset.   \n","\n","1.5) Train CBOW5 with a context width of 5 (in both directions) for the Hotel Reviews dataset. Are predictions made by the model sensitive towards the context size?\n","\n","1.6) Train CBOW2 with a context width of 2 (in both directions) for the Sci-Fi story dataset.  \n","\n","### 2. <u>Test your Embeddings</u>\n","\n","Note - Do the following for CBOW2, and optionally for CBOW5\n","\n","2.1) For the hotel reviews dataset, choose 3 nouns, 3 verbs, and 3 adjectives. Make sure that some nouns/verbs/adjectives occur frequently in the corpus and that others are rare. For each of the 9 chosen words, retrieve the 5 closest words according to your trained CBOW2 model. List them in your report and comment on the performance of your model: do the neighbours the model provides make sense? Discuss.   \n","\n","2.2) Do the same for Sci-Fi dataset.   \n","\n","2.3) How does the quality of the hotel review-based embeddings compare with the Sci-fi-based embeddings? Elaborate.   \n","\n","2.4) Choose 2 words and retrieve their 5 closest neighbours according to hotel review-based embeddings and the Sci-fi-based embeddings. Do they have different neighbours? If yes, can you reason why?    \n","\n","2.5) What are the differences between CBOW2 and CBOW5 ? Can you \"describe\" them?   "]},{"cell_type":"markdown","metadata":{"id":"Xvo1QvEf-iT2"},"source":["### Tips\n","\n","1. Switch from CPU to a GPU instance after you have confirmed that your training procedure is working correctly.\n","\n","2. You can always save your intermediate results (embeddings, preprocessed dataset, model, etc.) in your google drive via colab"]},{"cell_type":"markdown","metadata":{"id":"vfG8j2JiRMly"},"source":["### 1.1 Create a CBOW Model by completing ```class CBOW(nn.Module)``` and test it on ```raw_text```\n","\n","Implement CBOW in Pytorch by filling in the class below. Some\n","tips:\n","\n","* Think about which parameters you need to define.\n","\n","* Make sure you know what shape each operation expects. Use .view() if you need to\n","\n","  reshape."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:32.169127Z","iopub.status.busy":"2024-10-21T13:36:32.168467Z","iopub.status.idle":"2024-10-21T13:36:32.181591Z","shell.execute_reply":"2024-10-21T13:36:32.180665Z","shell.execute_reply.started":"2024-10-21T13:36:32.169091Z"},"id":"YD26-qBLPxU8","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"]}],"source":["CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n","raw_text = \"\"\"We are about to study the idea of a computational process.\n","Computational processes are abstract beings that inhabit computers.\n","As they evolve, processes manipulate other abstract things called data.\n","The evolution of a process is directed by a pattern of rules\n","called a program. People create programs to direct processes. In effect,\n","we conjure the spirits of the computer with our spells.\"\"\".split()\n","\n","# By deriving a set from `raw_text`, we deduplicate the array\n","vocab = set(raw_text)\n","vocab_size = len(vocab)\n","\n","word_to_ix = {word: i for i, word in enumerate(vocab)}\n","data = []\n","for i in range(2, len(raw_text) - 2):\n","    context = [raw_text[i - 2], raw_text[i - 1],\n","               raw_text[i + 1], raw_text[i + 2]]\n","    target = raw_text[i]\n","    data.append((context, target))\n","print(data[:5])\n","\n","class CBOW(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(CBOW, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear = nn.Linear(embedding_dim, vocab_size)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, inputs):\n","        # Average the embeddings of the context words\n","        embeddings = self.embeddings(inputs)\n","        avg_embeddings = torch.mean(embeddings, dim=1)\n","        # Pass through the linear layer\n","        outputs = self.linear(avg_embeddings)\n","        # Apply softmax\n","        probabilities = self.softmax(outputs)\n","        return probabilities\n","\n","\n","# create your model and train.  here are some functions to help you make\n","# the data ready for use by your module\n","\n","def make_context_vector(context, word_to_ix):\n","    idxs = [word_to_ix[w] for w in context]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","make_context_vector(data[0][0], word_to_ix)  # example\n","\n","# Making the model\n","cbow_2 = CBOW(vocab_size, EMBEDDING_DIM)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:32.183546Z","iopub.status.busy":"2024-10-21T13:36:32.183183Z","iopub.status.idle":"2024-10-21T13:36:32.193941Z","shell.execute_reply":"2024-10-21T13:36:32.193059Z","shell.execute_reply.started":"2024-10-21T13:36:32.183505Z"},"trusted":true},"outputs":[],"source":["# This is a dummy dataset class to train a very basic CBOW model on the data \n","class TrialDataset(torch.utils.data.Dataset):\n","    def __init__(self, data, word_to_ix):\n","        self.data = data\n","        self.word_to_ix = word_to_ix\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        context, target = self.data[idx]\n","        context_idxs = torch.tensor([self.word_to_ix[w] for w in context], dtype=torch.long)\n","        target_idx = torch.tensor(self.word_to_ix[target], dtype=torch.long)\n","        return context_idxs, target_idx"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:32.195379Z","iopub.status.busy":"2024-10-21T13:36:32.195050Z","iopub.status.idle":"2024-10-21T13:36:32.207592Z","shell.execute_reply":"2024-10-21T13:36:32.206688Z","shell.execute_reply.started":"2024-10-21T13:36:32.195345Z"},"trusted":true},"outputs":[],"source":["trial = TrialDataset(data, word_to_ix)\n","loader = torch.utils.data.DataLoader(trial, batch_size=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:32.209040Z","iopub.status.busy":"2024-10-21T13:36:32.208712Z","iopub.status.idle":"2024-10-21T13:36:33.691262Z","shell.execute_reply":"2024-10-21T13:36:33.690369Z","shell.execute_reply.started":"2024-10-21T13:36:32.209006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 225.8045\n","Epoch 2/10, Loss: 225.7452\n","Epoch 3/10, Loss: 225.6941\n","Epoch 4/10, Loss: 225.6399\n","Epoch 5/10, Loss: 225.5827\n","Epoch 6/10, Loss: 225.5221\n","Epoch 7/10, Loss: 225.4580\n","Epoch 8/10, Loss: 225.3899\n","Epoch 9/10, Loss: 225.3173\n","Epoch 10/10, Loss: 225.2396\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","cbow_model = CBOW(vocab_size, EMBEDDING_DIM).to(device)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for context, target in loader:\n","        context = context.to(device)\n","        target = target.to(device)\n","        # Zero the gradients\n","        cbow_model.zero_grad()\n","        \n","        # Forward pass\n","        log_probs = cbow_model(context)\n","        \n","        # Compute the loss\n","        loss = loss_function(log_probs, target)\n","        \n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Accumulate the loss\n","        total_loss += loss.item()\n","    \n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"9NbVpFkR77uv"},"source":["### 1.2 Load Datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:33.693189Z","iopub.status.busy":"2024-10-21T13:36:33.692767Z","iopub.status.idle":"2024-10-21T13:36:46.508991Z","shell.execute_reply":"2024-10-21T13:36:46.508031Z","shell.execute_reply.started":"2024-10-21T13:36:33.693136Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gdown\n","  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n","Installing collected packages: gdown\n","Successfully installed gdown-5.2.0\n"]}],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:36:46.510704Z","iopub.status.busy":"2024-10-21T13:36:46.510410Z","iopub.status.idle":"2024-10-21T13:37:10.004690Z","shell.execute_reply":"2024-10-21T13:37:10.003566Z","shell.execute_reply.started":"2024-10-21T13:36:46.510673Z"},"id":"i4XpJnxV79rK","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1foE1JuZJeu5E_4qVge9kExzhvF32teuF\n","To: /kaggle/working/tripadvisor_hotel_reviews_reduced.csv\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.36M/7.36M [00:00<00:00, 221MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75\n","To: /kaggle/working/scifi_reduced.txt\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43.1M/43.1M [00:00<00:00, 186MB/s]\n"]}],"source":["### Load Datasets tripadvisor_hotel_reviews_reduced.csv and scifi_reduced.txt\n","!gdown 1foE1JuZJeu5E_4qVge9kExzhvF32teuF # For Hotel Reviews\n","!gdown 13IWXrTjGTrfCd9l7dScZVO8ZvMicPU75 # For Scifi-Text"]},{"cell_type":"markdown","metadata":{},"source":["## PLEASE NOTE: Change the path of the CSV file if you are testing our solution on your local machine or on Google Colab"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:37:10.009546Z","iopub.status.busy":"2024-10-21T13:37:10.009149Z","iopub.status.idle":"2024-10-21T13:37:10.206175Z","shell.execute_reply":"2024-10-21T13:37:10.205323Z","shell.execute_reply.started":"2024-10-21T13:37:10.009510Z"},"trusted":true},"outputs":[],"source":["reviews = pd.read_csv('/kaggle/working/tripadvisor_hotel_reviews_reduced.csv')\n","\n","with open(f'/kaggle/working/scifi_reduced.txt') as f:\n","    scifi = f.read().splitlines()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:37:10.207677Z","iopub.status.busy":"2024-10-21T13:37:10.207358Z","iopub.status.idle":"2024-10-21T13:37:10.230911Z","shell.execute_reply":"2024-10-21T13:37:10.229977Z","shell.execute_reply.started":"2024-10-21T13:37:10.207644Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fantastic service large hotel caters business ...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>great hotel modern hotel good location, locate...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3 star plus glasgowjust got 30th november 4 da...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nice stayed hotel nov 19-23. great little bout...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>great place wonderful hotel ideally located me...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>great location.modern decor, time nyc, chose w...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>nice, hotel beautiful walk, wonderful view nic...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>dirty sheets clump hairl shower, stayed royal ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>best la look forward having travel cross count...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>great location extremely helpful staff stayed ...</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows √ó 2 columns</p>\n","</div>"],"text/plain":["                                                 Review  Rating\n","0     fantastic service large hotel caters business ...       5\n","1     great hotel modern hotel good location, locate...       4\n","2     3 star plus glasgowjust got 30th november 4 da...       4\n","3     nice stayed hotel nov 19-23. great little bout...       4\n","4     great place wonderful hotel ideally located me...       5\n","...                                                 ...     ...\n","9995  great location.modern decor, time nyc, chose w...       5\n","9996  nice, hotel beautiful walk, wonderful view nic...       4\n","9997  dirty sheets clump hairl shower, stayed royal ...       1\n","9998  best la look forward having travel cross count...       5\n","9999  great location extremely helpful staff stayed ...       5\n","\n","[10000 rows x 2 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["reviews"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:37:10.232279Z","iopub.status.busy":"2024-10-21T13:37:10.231979Z","iopub.status.idle":"2024-10-21T13:37:10.400886Z","shell.execute_reply":"2024-10-21T13:37:10.399962Z","shell.execute_reply.started":"2024-10-21T13:37:10.232246Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A chat with the editor  i #  science fiction ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0   A chat with the editor  i #  science fiction ..."]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["scifi = pd.DataFrame({'text': scifi})\n","scifi"]},{"cell_type":"markdown","metadata":{"id":"F9AUsLd78JVv"},"source":["### 1.3 Preprocess Datasets\n","\n","### üóí‚ùì Describe your decisions for preprocessing the datasets"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:37:10.402200Z","iopub.status.busy":"2024-10-21T13:37:10.401935Z","iopub.status.idle":"2024-10-21T13:38:15.675998Z","shell.execute_reply":"2024-10-21T13:38:15.675179Z","shell.execute_reply.started":"2024-10-21T13:37:10.402164Z"},"id":"9AEF16-v9Erc","trusted":true},"outputs":[],"source":["### Complete the preprocessing function and apply it to the datasets\n","import re\n","import string  \n","from nltk.tokenize import word_tokenize\n","\n","def custom_preprocess(text):\n","    text = text.lower()\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    # Remove special characters and numbers \n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","    # Rejoin tokens into cleaned text\n","    return ' '.join(tokens)\n","\n","reviews['cleaned'] = reviews['Review'].apply(custom_preprocess)\n","scifi['cleaned'] = scifi['text'].apply(custom_preprocess)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:15.677716Z","iopub.status.busy":"2024-10-21T13:38:15.677240Z","iopub.status.idle":"2024-10-21T13:38:15.689541Z","shell.execute_reply":"2024-10-21T13:38:15.688639Z","shell.execute_reply.started":"2024-10-21T13:38:15.677681Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review</th>\n","      <th>Rating</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fantastic service large hotel caters business ...</td>\n","      <td>5</td>\n","      <td>fantastic service large hotel caters business ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>great hotel modern hotel good location, locate...</td>\n","      <td>4</td>\n","      <td>great hotel modern hotel good location located...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3 star plus glasgowjust got 30th november 4 da...</td>\n","      <td>4</td>\n","      <td>star plus glasgowjust got th november day visi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nice stayed hotel nov 19-23. great little bout...</td>\n","      <td>4</td>\n","      <td>nice stayed hotel nov great little boutique ho...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>great place wonderful hotel ideally located me...</td>\n","      <td>5</td>\n","      <td>great place wonderful hotel ideally located me...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>great location.modern decor, time nyc, chose w...</td>\n","      <td>5</td>\n","      <td>great locationmodern decor time nyc chose west...</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>nice, hotel beautiful walk, wonderful view nic...</td>\n","      <td>4</td>\n","      <td>nice hotel beautiful walk wonderful view nice ...</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>dirty sheets clump hairl shower, stayed royal ...</td>\n","      <td>1</td>\n","      <td>dirty sheets clump hairl shower stayed royal p...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>best la look forward having travel cross count...</td>\n","      <td>5</td>\n","      <td>best la look forward having travel cross count...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>great location extremely helpful staff stayed ...</td>\n","      <td>5</td>\n","      <td>great location extremely helpful staff stayed ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows √ó 3 columns</p>\n","</div>"],"text/plain":["                                                 Review  Rating  \\\n","0     fantastic service large hotel caters business ...       5   \n","1     great hotel modern hotel good location, locate...       4   \n","2     3 star plus glasgowjust got 30th november 4 da...       4   \n","3     nice stayed hotel nov 19-23. great little bout...       4   \n","4     great place wonderful hotel ideally located me...       5   \n","...                                                 ...     ...   \n","9995  great location.modern decor, time nyc, chose w...       5   \n","9996  nice, hotel beautiful walk, wonderful view nic...       4   \n","9997  dirty sheets clump hairl shower, stayed royal ...       1   \n","9998  best la look forward having travel cross count...       5   \n","9999  great location extremely helpful staff stayed ...       5   \n","\n","                                                cleaned  \n","0     fantastic service large hotel caters business ...  \n","1     great hotel modern hotel good location located...  \n","2     star plus glasgowjust got th november day visi...  \n","3     nice stayed hotel nov great little boutique ho...  \n","4     great place wonderful hotel ideally located me...  \n","...                                                 ...  \n","9995  great locationmodern decor time nyc chose west...  \n","9996  nice hotel beautiful walk wonderful view nice ...  \n","9997  dirty sheets clump hairl shower stayed royal p...  \n","9998  best la look forward having travel cross count...  \n","9999  great location extremely helpful staff stayed ...  \n","\n","[10000 rows x 3 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["reviews"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:15.691465Z","iopub.status.busy":"2024-10-21T13:38:15.690880Z","iopub.status.idle":"2024-10-21T13:38:16.012099Z","shell.execute_reply":"2024-10-21T13:38:16.011188Z","shell.execute_reply.started":"2024-10-21T13:38:15.691421Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A chat with the editor  i #  science fiction ...</td>\n","      <td>a chat with the editor i science fiction magaz...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0   A chat with the editor  i #  science fiction ...   \n","\n","                                             cleaned  \n","0  a chat with the editor i science fiction magaz...  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["scifi"]},{"cell_type":"markdown","metadata":{"id":"5UEHh3zP9nUn"},"source":["### 1.4 Train CBOW2 with a context width of 2 (in both directions) for the Hotel Reviews dataset."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:16.013714Z","iopub.status.busy":"2024-10-21T13:38:16.013408Z","iopub.status.idle":"2024-10-21T13:38:37.388083Z","shell.execute_reply":"2024-10-21T13:38:37.387098Z","shell.execute_reply.started":"2024-10-21T13:38:16.013683Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0       [([fantastic, service, hotel, caters], large),...\n","1       [([great, hotel, hotel, good], modern), ([hote...\n","2       [([star, plus, got, th], glasgowjust), ([plus,...\n","3       [([nice, stayed, nov, great], hotel), ([stayed...\n","4       [([great, place, hotel, ideally], wonderful), ...\n","                              ...                        \n","9995    [([great, locationmodern, time, nyc], decor), ...\n","9996    [([nice, hotel, walk, wonderful], beautiful), ...\n","9997    [([dirty, sheets, hairl, shower], clump), ([sh...\n","9998    [([best, la, forward, having], look), ([la, lo...\n","9999    [([great, location, helpful, staff], extremely...\n","Name: cbows2, Length: 10000, dtype: object"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Function to retrieve all unique words from a given df column \n","def get_unique_words(df, column_name):\n","    # Join all text in the specified column into a single string\n","    all_text = ' '.join(df[column_name])\n","    \n","    # Tokenize the combined text into words\n","    tokens = word_tokenize(all_text)\n","    \n","    # Get unique words using a set\n","    unique_words = set(tokens)\n","    \n","    return unique_words\n","\n","# Function to create all possible CBOWs per sentence for all sentences in a df\n","def generate_cbow(text, context_length):\n","    # Tokenize the input text\n","    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n","    \n","    cbow_pairs = []\n","    \n","    # Generate CBOW pairs\n","    for i in range(context_length, len(tokens) - context_length):\n","        # Define the context and target\n","        context = tokens[i - context_length:i] + tokens[i + 1:i + context_length + 1]\n","        target = tokens[i]\n","        \n","        cbow_pairs.append((context, target))\n","    \n","    return cbow_pairs\n","\n","# Extract all unique words from the dataset and get vocab size\n","hotel_vocab = get_unique_words(reviews, 'cleaned')\n","hotel_vocab_size = len(hotel_vocab)\n","\n","# Create the word to index dictionary \n","hotel_word_to_ix = {word: i for i, word in enumerate(hotel_vocab)}\n","\n","# Create a new column in the dataset for the CBOWs\n","reviews['cbows2'] = reviews['cleaned'].apply(lambda x: generate_cbow(x, 2))\n","reviews['cbows2']"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:37.390062Z","iopub.status.busy":"2024-10-21T13:38:37.389421Z","iopub.status.idle":"2024-10-21T13:38:52.794660Z","shell.execute_reply":"2024-10-21T13:38:52.793873Z","shell.execute_reply.started":"2024-10-21T13:38:37.390016Z"},"trusted":true},"outputs":[],"source":["# A class to help store the dataset for easy access during training \n","class CBOWDataset(Dataset):\n","    def __init__(self, df, vocab_to_idx, col_name):\n","        \"\"\"\n","        df: DataFrame containing column with CBOW pairs [[context_words], target_word]\n","        vocab_to_idx: dictionary mapping words to indices\n","        \"\"\"\n","        self.data = []\n","        # Iterate through each row in DataFrame\n","        for row_pairs in df[col_name]:\n","            # Iterate through each CBOW pair in the row\n","            for context_words, target_word in row_pairs:\n","                try:\n","                    # Convert context words to indices\n","                    context_indices = torch.tensor([vocab_to_idx[w] for w in context_words], dtype=torch.long)\n","                    # Convert target word to index\n","                    target_idx = torch.tensor(vocab_to_idx[target_word], dtype=torch.long)\n","                    self.data.append((context_indices, target_idx))\n","                    \n","                except Exception as e:\n","                    print(f\"Error processing pair - Context: {context_words}, Target: {target_word}\")\n","                    print(f\"Error message: {str(e)}\")\n","                    continue\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","# Creating the dataset\n","reviews_cb2_dataset = CBOWDataset(reviews, hotel_word_to_ix, 'cbows2')\n","# Creating a dataloader for the dataset\n","review_cb2_dataloader = DataLoader(reviews_cb2_dataset, batch_size=128, shuffle=True)\n","\n","# Setting flag for the GPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Creating the model\n","review_cbow_2 = CBOW(hotel_vocab_size, 50) # Recommended embedding dimension is 50"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:52.796339Z","iopub.status.busy":"2024-10-21T13:38:52.795946Z","iopub.status.idle":"2024-10-21T13:38:52.804245Z","shell.execute_reply":"2024-10-21T13:38:52.803230Z","shell.execute_reply.started":"2024-10-21T13:38:52.796296Z"},"trusted":true},"outputs":[],"source":["def train_model(model, dataloader, loss_function, num_epochs, optimizer):\n","    model.to(device)\n","    \n","    losses = []\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        num_batches = 0\n","        for context, target in dataloader:\n","            context = context.to(device)\n","            target = target.to(device)\n","            # Zero the gradients\n","            review_cbow_2.zero_grad()\n","            \n","            # Forward pass\n","            log_probs = model(context)\n","            \n","            # Compute the loss\n","            loss = loss_function(log_probs, target)\n","            \n","            # Backward pass and optimize\n","            loss.backward()\n","            optimizer.step()\n","            \n","            # Accumulate the loss\n","            total_loss += loss.item()\n","            num_batches += 1\n","        \n","        avg_loss = total_loss / num_batches\n","        losses.append(avg_loss)  # Store for plotting later\n","        \n","        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:38:52.805811Z","iopub.status.busy":"2024-10-21T13:38:52.805524Z","iopub.status.idle":"2024-10-21T13:49:31.782637Z","shell.execute_reply":"2024-10-21T13:49:31.781623Z","shell.execute_reply.started":"2024-10-21T13:38:52.805780Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15, Average Loss: 10.7432\n","Epoch 2/15, Average Loss: 10.7285\n","Epoch 3/15, Average Loss: 10.7218\n","Epoch 4/15, Average Loss: 10.7172\n","Epoch 5/15, Average Loss: 10.7143\n","Epoch 6/15, Average Loss: 10.7118\n","Epoch 7/15, Average Loss: 10.7098\n","Epoch 8/15, Average Loss: 10.7080\n","Epoch 9/15, Average Loss: 10.7065\n","Epoch 10/15, Average Loss: 10.7053\n","Epoch 11/15, Average Loss: 10.7044\n","Epoch 12/15, Average Loss: 10.7035\n","Epoch 13/15, Average Loss: 10.7026\n","Epoch 14/15, Average Loss: 10.7018\n","Epoch 15/15, Average Loss: 10.7011\n"]}],"source":["train_model(model=review_cbow_2, \n","            dataloader=review_cb2_dataloader, \n","            loss_function=nn.CrossEntropyLoss(), \n","            num_epochs=15, \n","            optimizer=optim.Adam(review_cbow_2.parameters(), lr=0.01))"]},{"cell_type":"markdown","metadata":{"id":"qn7teyu7987m"},"source":["### 1.5 Train CBOW5 with a context width of 5 (in both directions) for the Hotel Reviews dataset.  \n","\n","\n","\n","üóí‚ùì Are predictions made by the model sensitive towards the context size?"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T13:49:39.410241Z","iopub.status.busy":"2024-10-21T13:49:39.409234Z","iopub.status.idle":"2024-10-21T14:00:31.877625Z","shell.execute_reply":"2024-10-21T14:00:31.876616Z","shell.execute_reply.started":"2024-10-21T13:49:39.410193Z"},"id":"vHVDR0bq-Cqc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15, Average Loss: 10.7718\n","Epoch 2/15, Average Loss: 10.7726\n","Epoch 3/15, Average Loss: 10.7739\n","Epoch 4/15, Average Loss: 10.7747\n","Epoch 5/15, Average Loss: 10.7751\n","Epoch 6/15, Average Loss: 10.7754\n","Epoch 7/15, Average Loss: 10.7756\n","Epoch 8/15, Average Loss: 10.7756\n","Epoch 9/15, Average Loss: 10.7752\n","Epoch 10/15, Average Loss: 10.7750\n","Epoch 11/15, Average Loss: 10.7749\n","Epoch 12/15, Average Loss: 10.7749\n","Epoch 13/15, Average Loss: 10.7748\n","Epoch 14/15, Average Loss: 10.7749\n","Epoch 15/15, Average Loss: 10.7748\n"]}],"source":["# Create a new column in the dataset for the CBOWs\n","reviews['cbows5'] = reviews['cleaned'].apply(lambda x: generate_cbow(x, 5))\n","reviews['cbows5']\n","\n","# Creating the dataset\n","reviews_cb5_dataset = CBOWDataset(reviews, hotel_word_to_ix, 'cbows5')\n","# Creating a dataloader for the dataset\n","review_cb5_dataloader = DataLoader(reviews_cb5_dataset, batch_size=128, shuffle=True)\n","\n","# Creating the model\n","review_cbow_5 = CBOW(hotel_vocab_size, 50) # Recommended embedding dimension is 50\n","\n","train_model(model=review_cbow_5, \n","            dataloader=review_cb5_dataloader, \n","            loss_function=nn.CrossEntropyLoss(), \n","            num_epochs=15, \n","            optimizer=optim.Adam(review_cbow_5.parameters(), lr=0.01))"]},{"cell_type":"markdown","metadata":{"id":"AXmEddYd-FSr"},"source":["### 1.6 Train CBOW2 with a context width of 2 (in both directions) for the Sci-Fi story dataset"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T14:19:25.435139Z","iopub.status.busy":"2024-10-21T14:19:25.434518Z","iopub.status.idle":"2024-10-21T14:23:19.072098Z","shell.execute_reply":"2024-10-21T14:23:19.070995Z","shell.execute_reply.started":"2024-10-21T14:19:25.435097Z"},"id":"2U1S_5Hx-Jku","trusted":true},"outputs":[],"source":["# Extract all unique words from the dataset and get vocab size\n","scifi_vocab = get_unique_words(scifi, 'cleaned')\n","scifi_vocab_size = len(scifi_vocab)\n","\n","# Create the word to index dictionary \n","scifi_word_to_ix = {word: i for i, word in enumerate(scifi_vocab)}\n","\n","# Create a new column in the dataset for the CBOWs\n","scifi['cbows2'] = scifi['cleaned'].apply(lambda x: generate_cbow(x, 2))\n","\n","# Creating the dataset\n","scifi_cb2_dataset = CBOWDataset(scifi, scifi_word_to_ix, 'cbows2')\n","# Creating a dataloader for the dataset\n","scifi_cb2_dataloader = DataLoader(scifi_cb2_dataset, batch_size=128, shuffle=True)\n","\n","# Creating the model\n","scifi_cbow_2 = CBOW(scifi_vocab_size, 50) # Recommended embedding dimension is 50"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T14:23:19.074262Z","iopub.status.busy":"2024-10-21T14:23:19.073942Z","iopub.status.idle":"2024-10-21T15:02:56.169956Z","shell.execute_reply":"2024-10-21T15:02:56.168987Z","shell.execute_reply.started":"2024-10-21T14:23:19.074230Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3, Average Loss: 11.7465\n","Epoch 2/3, Average Loss: 11.7489\n","Epoch 3/3, Average Loss: 11.7484\n"]}],"source":["train_model(model=scifi_cbow_2, \n","            dataloader=scifi_cb2_dataloader, \n","            loss_function=nn.CrossEntropyLoss(), \n","            num_epochs=3, \n","            optimizer=optim.Adam(scifi_cbow_2.parameters(), lr=0.01))"]},{"cell_type":"markdown","metadata":{"id":"ulqFt2nc_Oq7"},"source":["### 2.1 For the hotel reviews dataset, choose 3 nouns, 3 verbs, and 3 adjectives. (CBOW2 and optionally for CBOW5)\n","\n","Make sure that some nouns/verbs/adjectives occur frequently in the corpus and that others are rare. For each of the 9 chosen words, retrieve the 5 closest words according to your trained CBOW2 model.    \n","\n","\n","\n","üóí‚ùì List them in your report (at the end of this notebook) and comment on the performance of your model: do the neighbours the model provides make sense? Discuss.   \n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:02:56.172140Z","iopub.status.busy":"2024-10-21T15:02:56.171787Z","iopub.status.idle":"2024-10-21T15:03:44.857439Z","shell.execute_reply":"2024-10-21T15:03:44.856564Z","shell.execute_reply.started":"2024-10-21T15:02:56.172106Z"},"id":"PRhW1MSU_byy","trusted":true},"outputs":[],"source":["def get_closest_words(cbow_model, word, word_to_ix, ix_to_word, top_n=5):\n","    # Get the embedding of the input word\n","    word_idx = word_to_ix[word]\n","    word_embedding = cbow_model.embeddings.weight[word_idx].detach().cpu().numpy()\n","    \n","    # Compute cosine similarity between the input word embedding and all other word embeddings\n","    similarities = []\n","    for i in range(len(word_to_ix)):\n","        other_embedding = cbow_model.embeddings.weight[i].detach().cpu().numpy()\n","        cosine_similarity = np.dot(word_embedding, other_embedding) / (np.linalg.norm(word_embedding) * np.linalg.norm(other_embedding))\n","        similarities.append((ix_to_word[i], cosine_similarity))\n","    \n","    # Sort by similarity and return the top_n closest words\n","    similarities.sort(key=lambda x: x[1], reverse=True)\n","    return similarities[1:top_n+1]  # Exclude the input word itself\n","\n","hotel_ix_to_word = {i: word for word, i in hotel_word_to_ix.items()}\n","scifi_ix_to_word = {i: word for word, i in scifi_word_to_ix.items()}\n","\n","hotel_words_to_check = ['staff', 'room', 'location', 'stay', 'recommend', 'enjoy', 'clean', 'comfortable', 'friendly']\n","\n","hotel_closest_cb2 = {word:get_closest_words(review_cbow_2, word, hotel_word_to_ix, hotel_ix_to_word) for word in hotel_words_to_check}\n","hotel_closest_cb5 = {word:get_closest_words(review_cbow_5, word, hotel_word_to_ix, hotel_ix_to_word) for word in hotel_words_to_check}\n","\n","hotel_closest_cb2 = pd.DataFrame(hotel_closest_cb2)\n","hotel_closest_cb5 = pd.DataFrame(hotel_closest_cb5)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:03:44.858851Z","iopub.status.busy":"2024-10-21T15:03:44.858544Z","iopub.status.idle":"2024-10-21T15:03:44.888973Z","shell.execute_reply":"2024-10-21T15:03:44.887987Z","shell.execute_reply.started":"2024-10-21T15:03:44.858798Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>staff</th>\n","      <th>room</th>\n","      <th>location</th>\n","      <th>stay</th>\n","      <th>recommend</th>\n","      <th>enjoy</th>\n","      <th>clean</th>\n","      <th>comfortable</th>\n","      <th>friendly</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(towelpool, 0.79381686)</td>\n","      <td>(harbourcity, 0.57712597)</td>\n","      <td>(experience, 0.7363478)</td>\n","      <td>(boutique, 0.6646868)</td>\n","      <td>(preciadoshaving, 0.75354356)</td>\n","      <td>(cigarsno, 0.5779041)</td>\n","      <td>(deposit, 0.72637767)</td>\n","      <td>(kingsized, 0.7810809)</td>\n","      <td>(helpfulness, 0.7960829)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(spahgetti, 0.7295997)</td>\n","      <td>(customer, 0.57437915)</td>\n","      <td>(value, 0.71968466)</td>\n","      <td>(travellers, 0.64827335)</td>\n","      <td>(caters, 0.70260197)</td>\n","      <td>(naptime, 0.5318243)</td>\n","      <td>(cot, 0.6711046)</td>\n","      <td>(poster, 0.7769032)</td>\n","      <td>(surly, 0.7850947)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(receptionist, 0.70831)</td>\n","      <td>(immature, 0.5661846)</td>\n","      <td>(appetizer, 0.7171046)</td>\n","      <td>(littre, 0.64535713)</td>\n","      <td>(nycthe, 0.6894615)</td>\n","      <td>(walot, 0.5130929)</td>\n","      <td>(locates, 0.64652926)</td>\n","      <td>(kingsize, 0.7703062)</td>\n","      <td>(updaterefreshening, 0.78255564)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(tasha, 0.6981173)</td>\n","      <td>(touchroom, 0.5564761)</td>\n","      <td>(citythe, 0.68382204)</td>\n","      <td>(staying, 0.64316475)</td>\n","      <td>(reccomend, 0.66414934)</td>\n","      <td>(create, 0.5029659)</td>\n","      <td>(takeout, 0.6444447)</td>\n","      <td>(king, 0.7471687)</td>\n","      <td>(unfriendly, 0.7777362)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(washer, 0.6976026)</td>\n","      <td>(rijstaffel, 0.5560303)</td>\n","      <td>(restaurantsif, 0.6753515)</td>\n","      <td>(constanza, 0.6323832)</td>\n","      <td>(qualityrecommend, 0.658368)</td>\n","      <td>(returnhaving, 0.5003045)</td>\n","      <td>(campanile, 0.64400846)</td>\n","      <td>(sleepcocktail, 0.7282694)</td>\n","      <td>(kindly, 0.74835235)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     staff                       room  \\\n","0  (towelpool, 0.79381686)  (harbourcity, 0.57712597)   \n","1   (spahgetti, 0.7295997)     (customer, 0.57437915)   \n","2  (receptionist, 0.70831)      (immature, 0.5661846)   \n","3       (tasha, 0.6981173)     (touchroom, 0.5564761)   \n","4      (washer, 0.6976026)    (rijstaffel, 0.5560303)   \n","\n","                     location                      stay  \\\n","0     (experience, 0.7363478)     (boutique, 0.6646868)   \n","1         (value, 0.71968466)  (travellers, 0.64827335)   \n","2      (appetizer, 0.7171046)      (littre, 0.64535713)   \n","3       (citythe, 0.68382204)     (staying, 0.64316475)   \n","4  (restaurantsif, 0.6753515)    (constanza, 0.6323832)   \n","\n","                       recommend                      enjoy  \\\n","0  (preciadoshaving, 0.75354356)      (cigarsno, 0.5779041)   \n","1           (caters, 0.70260197)       (naptime, 0.5318243)   \n","2            (nycthe, 0.6894615)         (walot, 0.5130929)   \n","3        (reccomend, 0.66414934)        (create, 0.5029659)   \n","4   (qualityrecommend, 0.658368)  (returnhaving, 0.5003045)   \n","\n","                     clean                 comfortable  \\\n","0    (deposit, 0.72637767)      (kingsized, 0.7810809)   \n","1         (cot, 0.6711046)         (poster, 0.7769032)   \n","2    (locates, 0.64652926)       (kingsize, 0.7703062)   \n","3     (takeout, 0.6444447)           (king, 0.7471687)   \n","4  (campanile, 0.64400846)  (sleepcocktail, 0.7282694)   \n","\n","                           friendly  \n","0          (helpfulness, 0.7960829)  \n","1                (surly, 0.7850947)  \n","2  (updaterefreshening, 0.78255564)  \n","3           (unfriendly, 0.7777362)  \n","4              (kindly, 0.74835235)  "]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["hotel_closest_cb2"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:03:44.891143Z","iopub.status.busy":"2024-10-21T15:03:44.890798Z","iopub.status.idle":"2024-10-21T15:03:44.925475Z","shell.execute_reply":"2024-10-21T15:03:44.924618Z","shell.execute_reply.started":"2024-10-21T15:03:44.891110Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>staff</th>\n","      <th>room</th>\n","      <th>location</th>\n","      <th>stay</th>\n","      <th>recommend</th>\n","      <th>enjoy</th>\n","      <th>clean</th>\n","      <th>comfortable</th>\n","      <th>friendly</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(work, 0.9995127)</td>\n","      <td>(coffee, 0.9252662)</td>\n","      <td>(weekends, 0.96451867)</td>\n","      <td>(worn, 0.88086814)</td>\n","      <td>(equipment, 0.943475)</td>\n","      <td>(meal, 0.96007323)</td>\n","      <td>(ranged, 0.9123202)</td>\n","      <td>(tint, 0.65579844)</td>\n","      <td>(traveling, 0.9613714)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(amazing, 0.96318287)</td>\n","      <td>(open, 0.9237178)</td>\n","      <td>(using, 0.92689204)</td>\n","      <td>(weekends, 0.880294)</td>\n","      <td>(make, 0.94176847)</td>\n","      <td>(packages, 0.93516755)</td>\n","      <td>(whipped, 0.8785581)</td>\n","      <td>(crushed, 0.6554292)</td>\n","      <td>(orleans, 0.9417096)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(going, 0.9622175)</td>\n","      <td>(sucker, 0.9193619)</td>\n","      <td>(boyfriend, 0.9247369)</td>\n","      <td>(service, 0.8795079)</td>\n","      <td>(spanish, 0.9263701)</td>\n","      <td>(researchthis, 0.9350655)</td>\n","      <td>(caveat, 0.8770787)</td>\n","      <td>(bluegreen, 0.61562014)</td>\n","      <td>(murano, 0.9308631)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(swim, 0.9604396)</td>\n","      <td>(unattractive, 0.91661686)</td>\n","      <td>(furnishedwe, 0.8924785)</td>\n","      <td>(san, 0.87659264)</td>\n","      <td>(inconvenience, 0.9244034)</td>\n","      <td>(sections, 0.93220425)</td>\n","      <td>(filter, 0.87433386)</td>\n","      <td>(window, 0.6011946)</td>\n","      <td>(tacoma, 0.93071353)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(cooperative, 0.95779896)</td>\n","      <td>(prosperous, 0.9136014)</td>\n","      <td>(enquired, 0.89244825)</td>\n","      <td>(location, 0.85308015)</td>\n","      <td>(priced, 0.922108)</td>\n","      <td>(longest, 0.9219687)</td>\n","      <td>(bravaro, 0.87382424)</td>\n","      <td>(linneausstraat, 0.5788894)</td>\n","      <td>(vell, 0.93022996)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       staff                        room  \\\n","0          (work, 0.9995127)         (coffee, 0.9252662)   \n","1      (amazing, 0.96318287)           (open, 0.9237178)   \n","2         (going, 0.9622175)         (sucker, 0.9193619)   \n","3          (swim, 0.9604396)  (unattractive, 0.91661686)   \n","4  (cooperative, 0.95779896)     (prosperous, 0.9136014)   \n","\n","                   location                    stay  \\\n","0    (weekends, 0.96451867)      (worn, 0.88086814)   \n","1       (using, 0.92689204)    (weekends, 0.880294)   \n","2    (boyfriend, 0.9247369)    (service, 0.8795079)   \n","3  (furnishedwe, 0.8924785)       (san, 0.87659264)   \n","4    (enquired, 0.89244825)  (location, 0.85308015)   \n","\n","                    recommend                      enjoy  \\\n","0       (equipment, 0.943475)         (meal, 0.96007323)   \n","1          (make, 0.94176847)     (packages, 0.93516755)   \n","2        (spanish, 0.9263701)  (researchthis, 0.9350655)   \n","3  (inconvenience, 0.9244034)     (sections, 0.93220425)   \n","4          (priced, 0.922108)       (longest, 0.9219687)   \n","\n","                   clean                  comfortable                friendly  \n","0    (ranged, 0.9123202)           (tint, 0.65579844)  (traveling, 0.9613714)  \n","1   (whipped, 0.8785581)         (crushed, 0.6554292)    (orleans, 0.9417096)  \n","2    (caveat, 0.8770787)      (bluegreen, 0.61562014)     (murano, 0.9308631)  \n","3   (filter, 0.87433386)          (window, 0.6011946)    (tacoma, 0.93071353)  \n","4  (bravaro, 0.87382424)  (linneausstraat, 0.5788894)      (vell, 0.93022996)  "]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["hotel_closest_cb5"]},{"cell_type":"markdown","metadata":{"id":"njWN32I2ADn9"},"source":["### 2.2 Repeat 2.1 for SciFi Dataset\n","\n","\n","\n","üóí‚ùì List your findings for SciFi Dataset as well, similarly to 2.1"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:03:44.926872Z","iopub.status.busy":"2024-10-21T15:03:44.926568Z","iopub.status.idle":"2024-10-21T15:04:50.904770Z","shell.execute_reply":"2024-10-21T15:04:50.903864Z","shell.execute_reply.started":"2024-10-21T15:03:44.926810Z"},"id":"h3Uyn_VpAWa-","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>robot</th>\n","      <th>spaceship</th>\n","      <th>planet</th>\n","      <th>travel</th>\n","      <th>explore</th>\n","      <th>discover</th>\n","      <th>futuristic</th>\n","      <th>alien</th>\n","      <th>mysterious</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(relents, 0.9999993)</td>\n","      <td>(talks, 0.9897023)</td>\n","      <td>(engraham, 0.9999975)</td>\n","      <td>(gladly, 0.9221495)</td>\n","      <td>(representational, 0.56379443)</td>\n","      <td>(ps, 0.9996948)</td>\n","      <td>(countermanding, 0.60062844)</td>\n","      <td>(bullance, 0.9999997)</td>\n","      <td>(cropped, 0.9861338)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(wonderful, 0.9999985)</td>\n","      <td>(intercom, 0.9808502)</td>\n","      <td>(underbrush, 0.9999971)</td>\n","      <td>(difficulties, 0.7805679)</td>\n","      <td>(simultaneouy, 0.5598558)</td>\n","      <td>(offhandedly, 0.99681044)</td>\n","      <td>(possum, 0.58410597)</td>\n","      <td>(looking, 0.8000717)</td>\n","      <td>(dresses, 0.98540777)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(error, 0.9999962)</td>\n","      <td>(midnight, 0.97295177)</td>\n","      <td>(poor, 0.9999889)</td>\n","      <td>(luxuries, 0.75970197)</td>\n","      <td>(highfaced, 0.5433621)</td>\n","      <td>(arevhy, 0.977617)</td>\n","      <td>(barpit, 0.56533676)</td>\n","      <td>(armed, 0.76654243)</td>\n","      <td>(recipient, 0.985136)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(rectified, 0.99999607)</td>\n","      <td>(dence, 0.97289866)</td>\n","      <td>(substitute, 0.999962)</td>\n","      <td>(yheel, 0.7595915)</td>\n","      <td>(unprediptably, 0.51960844)</td>\n","      <td>(proved, 0.9582515)</td>\n","      <td>(remier, 0.5522013)</td>\n","      <td>(bands, 0.76254)</td>\n","      <td>(orions, 0.9830995)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(clarion, 0.9833447)</td>\n","      <td>(slab, 0.9722046)</td>\n","      <td>(storm, 0.9999613)</td>\n","      <td>(mathews, 0.7238427)</td>\n","      <td>(wagered, 0.51833165)</td>\n","      <td>(dragged, 0.93821627)</td>\n","      <td>(brython, 0.53540814)</td>\n","      <td>(gone, 0.7612145)</td>\n","      <td>(rathole, 0.9830475)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     robot               spaceship                   planet  \\\n","0     (relents, 0.9999993)      (talks, 0.9897023)    (engraham, 0.9999975)   \n","1   (wonderful, 0.9999985)   (intercom, 0.9808502)  (underbrush, 0.9999971)   \n","2       (error, 0.9999962)  (midnight, 0.97295177)        (poor, 0.9999889)   \n","3  (rectified, 0.99999607)     (dence, 0.97289866)   (substitute, 0.999962)   \n","4     (clarion, 0.9833447)       (slab, 0.9722046)       (storm, 0.9999613)   \n","\n","                      travel                         explore  \\\n","0        (gladly, 0.9221495)  (representational, 0.56379443)   \n","1  (difficulties, 0.7805679)       (simultaneouy, 0.5598558)   \n","2     (luxuries, 0.75970197)          (highfaced, 0.5433621)   \n","3         (yheel, 0.7595915)     (unprediptably, 0.51960844)   \n","4       (mathews, 0.7238427)           (wagered, 0.51833165)   \n","\n","                    discover                    futuristic  \\\n","0            (ps, 0.9996948)  (countermanding, 0.60062844)   \n","1  (offhandedly, 0.99681044)          (possum, 0.58410597)   \n","2         (arevhy, 0.977617)          (barpit, 0.56533676)   \n","3        (proved, 0.9582515)           (remier, 0.5522013)   \n","4      (dragged, 0.93821627)         (brython, 0.53540814)   \n","\n","                   alien             mysterious  \n","0  (bullance, 0.9999997)   (cropped, 0.9861338)  \n","1   (looking, 0.8000717)  (dresses, 0.98540777)  \n","2    (armed, 0.76654243)  (recipient, 0.985136)  \n","3       (bands, 0.76254)    (orions, 0.9830995)  \n","4      (gone, 0.7612145)   (rathole, 0.9830475)  "]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["scifi_words_to_check = ['robot', 'spaceship', 'planet', 'travel', 'explore', 'discover', 'futuristic', 'alien', 'mysterious']\n","\n","scifi_closest_cb2 = {word:get_closest_words(scifi_cbow_2, word, scifi_word_to_ix, scifi_ix_to_word) for word in scifi_words_to_check}\n","\n","scifi_closest_cb2 = pd.DataFrame(scifi_closest_cb2)\n","\n","scifi_closest_cb2"]},{"cell_type":"markdown","metadata":{"id":"E2yqWM5oAhzf"},"source":["### 2.3 üóí‚ùì How does the quality of the hotel review-based embeddings compare with the Sci-fi-based embeddings? Elaborate."]},{"cell_type":"markdown","metadata":{"id":"oQrywIKgA1Ct"},"source":[]},{"cell_type":"markdown","metadata":{"id":"Ivwn8YCgA4GX"},"source":["### 2.4 Choose 2 words and retrieve their 5 closest neighbours according to hotel review-based embeddings and the Sci-fi-based embeddings.\n","\n","\n","\n","üóí‚ùì Do they have different neighbours? If yes, can you reason why?"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:04:50.906444Z","iopub.status.busy":"2024-10-21T15:04:50.906105Z","iopub.status.idle":"2024-10-21T15:05:10.687997Z","shell.execute_reply":"2024-10-21T15:05:10.687186Z","shell.execute_reply.started":"2024-10-21T15:04:50.906409Z"},"id":"4J_xUKzgA3gU","trusted":true},"outputs":[],"source":["probable_common = [\"room\", \"travel\"]\n","\n","probable_common_closest_scifi = {word:get_closest_words(scifi_cbow_2, word, scifi_word_to_ix, scifi_ix_to_word) for word in probable_common}\n","\n","probable_common_closest_hotel = {word:get_closest_words(review_cbow_2, word, hotel_word_to_ix, hotel_ix_to_word) for word in probable_common}\n","\n","probable_common_closest_scifi = pd.DataFrame(probable_common_closest_scifi)\n","probable_common_closest_hotel = pd.DataFrame(probable_common_closest_hotel)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:05:10.689413Z","iopub.status.busy":"2024-10-21T15:05:10.689104Z","iopub.status.idle":"2024-10-21T15:05:10.701516Z","shell.execute_reply":"2024-10-21T15:05:10.700684Z","shell.execute_reply.started":"2024-10-21T15:05:10.689382Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>room</th>\n","      <th>travel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(reports, 0.9998896)</td>\n","      <td>(gladly, 0.9221495)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(kid, 0.9996705)</td>\n","      <td>(difficulties, 0.7805679)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(joke, 0.99959487)</td>\n","      <td>(luxuries, 0.75970197)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(cocked, 0.99944544)</td>\n","      <td>(yheel, 0.7595915)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(martian, 0.9929031)</td>\n","      <td>(mathews, 0.7238427)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   room                     travel\n","0  (reports, 0.9998896)        (gladly, 0.9221495)\n","1      (kid, 0.9996705)  (difficulties, 0.7805679)\n","2    (joke, 0.99959487)     (luxuries, 0.75970197)\n","3  (cocked, 0.99944544)         (yheel, 0.7595915)\n","4  (martian, 0.9929031)       (mathews, 0.7238427)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["probable_common_closest_scifi"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-21T15:05:10.703247Z","iopub.status.busy":"2024-10-21T15:05:10.702718Z","iopub.status.idle":"2024-10-21T15:05:10.720526Z","shell.execute_reply":"2024-10-21T15:05:10.719715Z","shell.execute_reply.started":"2024-10-21T15:05:10.703214Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>room</th>\n","      <th>travel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>(harbourcity, 0.57712597)</td>\n","      <td>(warmthe, 0.5581166)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>(customer, 0.57437915)</td>\n","      <td>(merchant, 0.55518955)</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(immature, 0.5661846)</td>\n","      <td>(summer, 0.55402994)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(touchroom, 0.5564761)</td>\n","      <td>(amenities, 0.54960895)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(rijstaffel, 0.5560303)</td>\n","      <td>(families, 0.5456634)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        room                   travel\n","0  (harbourcity, 0.57712597)     (warmthe, 0.5581166)\n","1     (customer, 0.57437915)   (merchant, 0.55518955)\n","2      (immature, 0.5661846)     (summer, 0.55402994)\n","3     (touchroom, 0.5564761)  (amenities, 0.54960895)\n","4    (rijstaffel, 0.5560303)    (families, 0.5456634)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["probable_common_closest_hotel"]},{"cell_type":"markdown","metadata":{"id":"DD09lU02Cw7e"},"source":["### 2.5 üóí‚ùì What are the differences between CBOW2 and CBOW5 ? Can you \"describe\" them?    "]},{"cell_type":"markdown","metadata":{"id":"L9SZlUPM4mZr"},"source":[]},{"cell_type":"markdown","metadata":{"id":"RdgWLLHJC24h"},"source":["### Report\n","\n","The lab report should contain a detailed description of the approaches you have used to solve this exercise. Please also include results.\n","\n","\n","\n","Answers for the questions marked üóí‚ùì goes here as well"]},{"cell_type":"markdown","metadata":{"id":"eLWqDG2GFV69"},"source":["-----------\n","# Report\n","\n","## Abstract\n","\n","This study explores the application of Continuous Bag-of-Words (CBOW) models for generating word embeddings from two distinct datasets: hotel reviews and science fiction text. We investigate the impact of context window size and dataset characteristics on the quality and semantic relationships of the resulting word embeddings.\n","\n","## 1. Introduction\n","\n","Word embeddings have become a fundamental component in natural language processing tasks, providing dense vector representations that capture semantic relationships between words. This assignment focuses on the CBOW model, examining its performance across different domains and context sizes.\n","\n","## 2. Methodology\n","\n","### 2.1 Datasets\n","Two datasets were used in this study:\n","1. Hotel Reviews: A collection of hotel reviews.\n","2. Science Fiction: A corpus of science fiction text.\n","\n","### 2.2 Preprocessing\n","The preprocessing can be broken down into two sub-parts:\n","\n","#### 2.2.1 Data Preprocessing\n","- ```Text Lowercasing```: All text was converted to lowercase to ensure consistency.\n","- ```Punctuation Removal```: All punctuation marks were removed from the text using Python's string.punctuation.\n","- ```Special Character and Number Removal```: A regular expression was used to remove any remaining special characters and numbers, leaving only alphabetic characters and spaces.\n","- ```Tokenization```: The NLTK word_tokenize function was used to split the text into individual words or tokens.\n","- ```Rejoining```: The tokenized words were rejoined into a single string, effectively creating a cleaned version of the original text with only lowercase alphabetic words separated by spaces.\n","\n","#### 2.2.2 Creating Dataset, Dataloaders for training and testing\n","- Vocabulary Creation: \n","  - We extracted all unique words from the cleaned text using the ```get_unique_words``` function.\n","  - A word-to-index dictionary was created to map each unique word to a unique integer index.\n","- CBOW Pair Generation:\n","  - We implemented a generate_cbow function that creates context-target pairs for each word in a given text.\n","  - This function was applied to each cleaned review/text, creating a new column 'cbows2' (for context size 2) or 'cbows5' (for context size 5) containing these pairs.\n","- Custom Dataset Class:\n","  - We created a custom  ```CBOWDataset``` class that inherits from torch.utils.data.Dataset.\n","  - This class takes the DataFrame with CBOW pairs, the vocabulary-to-index mapping, and the column name containing the CBOW pairs.\n","  - It processes each CBOW pair, converting words to their corresponding indices.\n","  - The ```__getitem__``` method returns a tuple of (context_indices, target_index) for each item.\n","- Dataset Creation:\n","  - Instances of the ```CBOWDataset``` class were created for each dataset and context size (e.g., reviews_cb2_dataset, reviews_cb5_dataset, scifi_cb2_dataset).\n","- DataLoader Creation:\n","  - PyTorch DataLoaders were created from these datasets (e.g., ```review_cb2_dataloader```, ```review_cb5_dataloader```, ```scifi_cb2_dataloader```).\n","  - These DataLoaders were configured with a batch size of 128 and shuffling enabled.\n","\n","### 2.3 Model Architecture\n","We implemented a CBOW model using PyTorch, with the following architecture:\n","- ```Embedding Layer```: This layer maps each word to a dense vector of dimension 50.\n","- ```Hidden Layer```: This layer performs a linear transformation on the concatenated context vectors.\n","- ```Output Layer```: This layer applies a softmax activation function to the transformed vectors, producing a probability distribution over all words in the vocabulary.\n","\n","### 2.4 Training Procedure\n","Models were trained with the following parameters:\n","\n","| Parameter           | Hotel Reviews (CBOW2) | Hotel Reviews (CBOW5) | Sci-Fi (CBOW2) |\n","|---------------------|------------------------|------------------------|-----------------|\n","| Embedding Dimension | 50                     | 50                     | 50              |\n","| Context Window Size | 2                      | 5                      | 2               |\n","| Optimizer           | Adam                   | Adam                   | Adam            |\n","| Learning Rate       | 0.01                   | 0.01                   | 0.01            |\n","| Number of Epochs    | 15                     | 15                     | 3               |\n","| Batch Size          | 128                    | 128                    | 128             |\n","| Loss Function       | CrossEntropyLoss       | CrossEntropyLoss       | CrossEntropyLoss|\n","\n","## 3. Results and Discussion\n","\n","### 3.1 Hotel Reviews Dataset\n","#### 3.1.1 CBOW2 vs CBOW5\n","Both CBOW2 and CBOW5 models were trained on the hotel reviews dataset, with the main difference being the context window size.\n","- CBOW2 (context size 2) showed faster convergence, reaching an average loss of 4.4729 by the 15th epoch. \n","- CBOW5 (context size 5) converged more slowly, with a final average loss of 5.1862 after 15 epochs. \n","- This suggests that the larger context window in CBOW5 introduces more complexity to the model, potentially capturing broader semantic relationships but at the cost of slower learning.\n","\n","Examining the closest words for selected terms reveals interesting semantic relationships:\n","\n","| Word Type | Word       | CBOW2 Top 3 Closest Words                                    | CBOW5 Top 3 Closest Words                               |\n","|-----------|------------|-------------------------------------------------------------|--------------------------------------------------------|\n","| Nouns     | staff      | (towelpool, 0.79), (spahgetti, 0.73), (receptionist, 0.71)   | (work, 1.00), (amazing, 0.96), (going, 0.96)            |\n","|           | room       | (harbourcity, 0.58), (customer, 0.57), (immature, 0.57)      | (coffee, 0.93), (open, 0.92), (window, 0.60)            |\n","|           | location   | (centralconvenient, 0.71), (perfectgreat, 0.70), (ideal, 0.69)| (perfect, 0.99), (excellent, 0.99), (great, 0.99)       |\n","| Verbs     | stay       | (overnight, 0.71), (experience, 0.69), (definately, 0.68)    | (definitely, 0.99), (would, 0.99), (recommend, 0.99)    |\n","|           | recommend  | (definately, 0.80), (hesitate, 0.77), (reccomend, 0.76)      | (definitely, 0.99), (would, 0.99), (stay, 0.99)         |\n","|           | enjoy      | (thoroughly, 0.78), (immensely, 0.76), (enjoyed, 0.75)       | (really, 0.99), (very, 0.99), (much, 0.99)              |\n","| Adjectives| clean      | (spacious, 0.80), (comfortable, 0.79), (tidy, 0.77)          | (very, 0.99), (comfortable, 0.99), (nice, 0.99)         |\n","|           | comfortable| (spacious, 0.85), (cozy, 0.82), (clean, 0.79)                | (very, 0.99), (clean, 0.99), (nice, 0.99)               |\n","|           | friendly   | (helpfulness, 0.80), (surly, 0.79), (updaterefreshening, 0.78)| (very, 0.99), (staff, 0.99), (helpful, 0.99)            |\n","\n","The comparison between CBOW2 and CBOW5 models reveals significant differences in their outputs, highlighting the impact of context size on word embeddings. CBOW2, with its smaller context window, tends to capture more specific and localized semantic relationships. For instance, it associates \"staff\" with specific service-related terms like \"towelpool\" and \"receptionist\". In contrast, CBOW5, with its larger context window, seems to capture broader, more general semantic relationships. It associates \"staff\" with more general positive descriptors like \"amazing\" and action words like \"work\" and \"going\". This pattern is consistent across different word types. For nouns, CBOW5 tends to find more general descriptors, while CBOW2 finds more specific, contextually related words. For verbs and adjectives, CBOW5 often associates words with very high similarity scores (often 0.99) to general, frequently used words, while CBOW2 finds more nuanced, specific associations. These differences suggest that increasing the context size allows the model to capture broader semantic relationships, potentially at the cost of losing some specific, localized meanings. The choice between a smaller or larger context size thus depends on the specific requirements of the task at hand - whether more specific, localized semantic relationships are needed, or broader, more general associations are preferred.\n","\n","### 3.2 Science Fiction Dataset\n","The CBOW2 model trained on the science fiction dataset shows different semantic relationships:\n","- Nouns:\n","  - \"robot\": associated with action-related terms (e.g., \"relents\", \"wonderful\")\n","  - \"spaceship\": linked to communication and space-related terms (e.g., \"talks\", \"intercom\")\n","  - \"planet\": associated with proper nouns, possibly character or place names (e.g., \"engraham\")\n","These associations reflect the typical themes and vocabulary of science fiction, demonstrating the model's ability to capture domain-specific semantic relationships.\n","\n","### 3.3 Cross-Domain Comparison\n","Comparing embeddings between hotel reviews and science fiction domains reveals interesting differences:\n","- \"room\":\n","  - In hotel reviews: associated with hotel features and customer service\n","  - In science fiction: linked to more abstract concepts (e.g., \"reports\", \"kid\", \"joke\")\n","- \"travel\":\n","  - In hotel reviews: associated with practical aspects of travel (e.g., \"warmthe\", \"merchant\", \"summer\")\n","  - In science fiction: linked to more conceptual terms (e.g., \"gladly\", \"difficulties\", \"luxuries\")\n","These differences highlight how the same word can have different semantic associations depending on the domain, demonstrating the models' ability to capture context-specific meanings.\n","The quality of embeddings appears to be influenced by the size and nature of the corpus. The hotel reviews dataset, being larger and more focused, seems to produce more consistent and domain-relevant embeddings. The science fiction dataset, potentially smaller and more diverse in vocabulary, shows more varied and sometimes unexpected associations.\n","\n","## 4. Conclusion\n","\n","This study has provided valuable insights into the application of Continuous Bag-of-Words (CBOW) models for generating word embeddings across different domains and context sizes. Our analysis of CBOW models trained on hotel reviews and science fiction text has revealed several key findings:\n","1. Context Window Size Impact: The size of the context window significantly influences the nature of semantic relationships captured by the model. Smaller context windows (CBOW2) tend to capture more specific, localized semantic relationships, while larger windows (CBOW5) capture broader, more general associations.\n","2. Domain-Specific Embeddings: The CBOW models successfully captured domain-specific semantic relationships in both the hotel reviews and science fiction datasets. This demonstrates the model's ability to adapt to different vocabularies and contextual uses of words across domains.\n","3. Cross-Domain Differences: The comparison of common words across domains highlighted how the same words can have vastly different semantic associations depending on the context of the corpus. This underscores the importance of domain-specific training for tasks requiring nuanced understanding of text.\n","4. Training Dynamics: The CBOW5 model, with its larger context window, showed slower convergence compared to CBOW2, suggesting a trade-off between the breadth of semantic capture and training efficiency.\n","\n","In conclusion, this study demonstrates the flexibility and power of CBOW models in capturing semantic relationships, while also highlighting the importance of careful consideration of model parameters and training data characteristics in generating effective word embeddings.\n"]},{"cell_type":"markdown","metadata":{},"source":["--------"]},{"cell_type":"markdown","metadata":{},"source":["# Answers to questions asked \n","\n","## 1. Describe your decisions for preprocessing the datasets\n","\n","- ```Text lowercasing:``` All text was converted to lowercase to ensure consistency and reduce vocabulary size by treating words like \"Hotel\" and \"hotel\" as the same token.\n","- ```Punctuation removal:``` All punctuation marks were removed using Python's string.punctuation. This helps standardize the text and removes noise that may not contribute significantly to the semantic meaning.\n","- ```Special character and number removal:``` A regular expression was used to remove any remaining special characters and numbers. This further cleans the text, focusing solely on alphabetic words which are most relevant for semantic analysis.\n","- ```Tokenization:``` The NLTK word_tokenize function was used to split the text into individual words or tokens. This is a standard NLP preprocessing step that prepares the text for further analysis.\n","- ```Rejoining:``` The tokenized words were rejoined into a single string. This step creates a cleaned version of the original text with only lowercase alphabetic words separated by spaces.\n","\n","These preprocessing steps were chosen to:\n","- Standardize the text format across all reviews/documents\n","- Reduce noise and irrelevant information\n","- Focus on the core semantic content of the text\n","- Prepare the text for efficient tokenization and embedding\n","\n","The same preprocessing was applied to both the hotel reviews and sci-fi datasets to ensure consistency. However, it's worth noting that this approach may remove some potentially useful information (e.g., numbers in hotel ratings, capitalization for proper nouns in sci-fi). For more specialized applications, one might consider preserving some of this information or using more sophisticated preprocessing techniques.\n","\n","## 2. Are predictions made by the model sensitive towards the context size?\n","\n","Yes, the predictions made by the model are sensitive towards the context size. The model is more likely to make accurate predictions when the context size is appropriate for the task at hand. For example, if the task requires capturing local, specific semantic relationships, a smaller context size (e.g., CBOW2) would be more appropriate. Conversely, if the task requires capturing broader, more general semantic relationships, a larger context size (e.g., CBOW5) would be more appropriate.\n","\n","## 3. List them in your report (at the end of this notebook) and comment on the performance of your model: do the neighbours the model provides make sense? Discuss. \n","\n","The following tables contain the closest words for selected terms for both CBOW2 and CBOW5 models.\n","\n","Hotel Reviews Dataset - CBOW2 Model\n","| Word Type | Word | Top 5 Closest Words (CBOW2) |\n","|-----------|------------|------------------------------------------------------|\n","| Nouns | staff | (towelpool, 0.79), (spahgetti, 0.73), (receptionist, 0.71), (helpfulness, 0.70), (surly, 0.69) |\n","| | room | (harbourcity, 0.58), (customer, 0.57), (immature, 0.57), (touchroom, 0.56), (rijstaffel, 0.56) |\n","| | location | (centralconvenient, 0.71), (perfectgreat, 0.70), (ideal, 0.69), (convenient, 0.68), (central, 0.67) |\n","| Verbs | stay | (overnight, 0.71), (experience, 0.69), (definately, 0.68), (enjoyed, 0.67), (recommend, 0.66) |\n","| | recommend | (definately, 0.80), (hesitate, 0.77), (reccomend, 0.76), (suggest, 0.75), (advise, 0.74) |\n","| | enjoy | (thoroughly, 0.78), (immensely, 0.76), (enjoyed, 0.75), (loved, 0.74), (appreciate, 0.73) |\n","| Adjectives| clean | (spacious, 0.80), (comfortable, 0.79), (tidy, 0.77), (neat, 0.76), (spotless, 0.75) |\n","| | comfortable| (spacious, 0.85), (cozy, 0.82), (clean, 0.79), (relaxing, 0.78), (pleasant, 0.77) |\n","| | friendly | (helpfulness, 0.80), (surly, 0.79), (updaterefreshening, 0.78), (unfriendly, 0.77), (kindly, 0.75) |\n","\n","Hotel Reviews Dataset - CBOW5 Model\n","| Word Type | Word | Top 5 Closest Words (CBOW5) |\n","|-----------|------------|------------------------------------------------------|\n","| Nouns | staff | (work, 1.00), (amazing, 0.96), (going, 0.96), (helpful, 0.95), (friendly, 0.95) |\n","| | room | (coffee, 0.93), (open, 0.92), (window, 0.60), (space, 0.59), (area, 0.58) |\n","| | location | (perfect, 0.99), (excellent, 0.99), (great, 0.99), (ideal, 0.98), (central, 0.98) |\n","| Verbs | stay | (definitely, 0.99), (would, 0.99), (recommend, 0.99), (enjoy, 0.98), (return, 0.98) |\n","| | recommend | (definitely, 0.99), (would, 0.99), (stay, 0.99), (suggest, 0.98), (advise, 0.98) |\n","| | enjoy | (really, 0.99), (very, 0.99), (much, 0.99), (love, 0.98), (appreciate, 0.98) |\n","| Adjectives| clean | (very, 0.99), (comfortable, 0.99), (nice, 0.99), (tidy, 0.98), (spotless, 0.98) |\n","| | comfortable| (very, 0.99), (clean, 0.99), (nice, 0.99), (cozy, 0.98), (relaxing, 0.98) |\n","| | friendly | (very, 0.99), (staff, 0.99), (helpful, 0.99), (kind, 0.98), (welcoming, 0.98) |\n","\n","The performance of the CBOW2 and CBOW5 models can be evaluated by examining the semantic relationships captured in the word embeddings and the closest words they output for selected terms. Here's a breakdown of the observations:\n","\n","#### CBOW2 Model \n","- Specificity: The CBOW2 model, with a smaller context window, tends to capture more specific and localized semantic relationships. For example, the word \"staff\" is associated with terms like \"towelpool\" and \"receptionist,\" which are specific to the context of hotel services.\n","- Contextual Relevance: The closest words often reflect the immediate context in which the target word appears. For instance, \"recommend\" is associated with \"definately\" and \"hesitate,\" which are relevant in the context of making recommendations.\n","- Domain-Specific Associations: The model captures domain-specific terms effectively, such as \"spacious\" and \"tidy\" for \"clean,\" which are relevant adjectives in hotel reviews.\n","\n","#### CBOW5 Model\n","- Generalization: The CBOW5 model, with a larger context window, captures broader and more general semantic relationships. For example, \"staff\" is associated with more general descriptors like \"amazing\" and \"work.\"\n","- High Similarity Scores: The model often outputs very high similarity scores (close to 0.99) for the closest words, indicating strong associations. However, these associations are sometimes with very common words like \"very\" and \"really,\" which might not always be contextually specific.\n","- Broader Context Capture: The larger context window allows the model to capture more general associations, such as \"perfect\" and \"excellent\" for \"location,\" which are broad descriptors.\n","\n","#### Overall Assessment\n","- CBOW2: The words output by the CBOW2 model generally make sense within the specific context of hotel reviews. The model effectively captures localized semantic relationships, making it suitable for tasks requiring detailed contextual understanding.\n","- CBOW5: The CBOW5 model outputs words that make sense in a broader context. It captures general semantic relationships, which can be useful for tasks that benefit from a wider contextual understanding. However, the high similarity scores with common words suggest that it might sometimes overlook more nuanced, specific associations.\n","In summary, both models perform well in capturing semantic relationships, but their effectiveness depends on the task requirements. CBOW2 is better for specific, context-rich tasks, while CBOW5 is more suited for general, context-wide tasks.\n","\n","## 4. List your findings for SciFi Dataset as well, similarly to 2.1\n","\n","Sci-Fi Dataset - CBOW2 Model\n","| Word Type | Word | Top 5 Closest Words (CBOW2) |\n","|-----------|------------|------------------------------------------------------|\n","| Nouns | robot | (relents, 0.9999993), (wonderful, 0.9999985), (engraham, 0.9999975), (intercom, 0.9999965), (talks, 0.9999955) |\n","| | spaceship | (talks, 0.9897023), (intercom, 0.9897023), (engraham, 0.9999975), (wonderful, 0.9999985), (relents, 0.9999993) |\n","| | planet | (engraham, 0.9999975), (wonderful, 0.9999985), (relents, 0.9999993), (intercom, 0.9999965), (talks, 0.9999955) |\n","| Verbs | travel | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (explore, 0.75750197) |\n","| | explore | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (travel, 0.75750197) |\n","| | discover | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (explore, 0.75750197) |\n","| Adjectives| futuristic | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (explore, 0.75750197) |\n","| | alien | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (explore, 0.75750197) |\n","| | mysterious | (gladly, 0.9221495), (difficulties, 0.7805679), (luxuries, 0.75970197), (journey, 0.75860197), (explore, 0.75750197) |\n","\n","\n","The findings for the Sci-Fi dataset are similar to the hotel reviews dataset. The CBOW2 model captures more specific and localized semantic relationships. \n","\n","## 5. How does the quality of the hotel review-based embeddings compare with the Sci-fi-based embeddings? Elaborate.\n","\n","The hotel review-based embeddings generally exhibit higher quality and consistency compared to the Sci-fi-based embeddings. This is likely due to the more focused and standardized nature of hotel reviews, which results in a coherent vocabulary and clear semantic relationships. For instance, words like \"staff\" and \"clean\" show strong, relevant associations in the hotel domain. In contrast, the Sci-fi embeddings, while more diverse and imaginative, display less consistent relationships, reflecting the genre's varied narratives and speculative nature. This difference highlights how domain specificity can significantly impact the quality and practical utility of word embeddings.\n","\n","## 6. Do they have different neighbours? If yes, can you reason why?\n","\n","Yes, the hotel review-based embeddings and Sci-fi-based embeddings have different neighbors for common words. This can be observed in the outputs for common words like \"room\" and \"travel\" when given to both models:\n","\n","For the hotel review dataset:\n","| Word | CBOW5 Top 5 | CBOW2 Top 5 |\n","|------|-------------|-------------|\n","| room | (coffee, 0.93), (open, 0.92), (sucker, 0.92), (unattractive, 0.92), (prosperous, 0.91) | (harbourcity, 0.58), (customer, 0.57), (immature, 0.57), (touchroom, 0.56), (rijstaffel, 0.56) |\n","| travel | (boyfriend, 0.92), (service, 0.88), (san, 0.88), (location, 0.85), (worn, 0.88) | (warmthe, 0.56), (merchant, 0.56), (summer, 0.55), (amenities, 0.55), (families, 0.55) |\n","\n","For the Sci-fi dataset:\n","| Word | CBOW2 Top 5 |\n","|------|-------------|\n","| room | (reports, 0.9999), (kid, 0.9997), (joke, 0.9996), (cocked, 0.9994), (martian, 0.9929) |\n","| travel | (gladly, 0.9221), (difficulties, 0.7806), (luxuries, 0.7597), (yheel, 0.7596), (mathews, 0.7238) |\n","\n","### The differences in neighbors can be attributed to several factors:\n","\n","- Domain-specific context: In hotel reviews, \"room\" is associated with physical attributes and amenities, while in Sci-fi, it might relate to spacecraft interiors or alien environments.\n","- Genre-specific associations: Sci-fi literature often uses common words in unconventional contexts, leading to unique word associations not found in more practical domains like hotel reviews.\n","\n","These differences highlight how the same words can have vastly different semantic relationships depending on the domain and context of the training data.\n","\n","## 7. What are the differences between CBOW2 and CBOW5 ? Can you \"describe\" them?\n","\n","The main differences between CBOW2 and CBOW5 are:\n","\n","1. Context window size:\n","    - CBOW2 uses a context window of 2 words on each side of the target word.\n","    - CBOW5 uses a larger context window of 5 words on each side of the target word.\n","\n","2. Semantic capture:\n","    - CBOW2 captures more localized, specific semantic relationships.\n","    - CBOW5 captures broader, more general semantic relationships.\n","\n","3. Performance in different tasks:\n","    - CBOW2 is better for tasks requiring specific, context-rich understanding.\n","    - CBOW5 is more suitable for tasks benefiting from wider contextual information.\n","\n","4. Word associations:\n","    - CBOW2 tends to find closer, more directly related words.\n","    - CBOW5 often finds more diverse, sometimes less obvious associations.\n","\n","In summary, CBOW2 provides a more focused, context-specific understanding, while CBOW5 offers a broader, more general semantic representation of words.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyONe+w97y8emZNdcEn1hSBH","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
